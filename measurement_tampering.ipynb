{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix gradient accumulation to work for batched loss, by computing logits  for \n",
    "# virtual batch without grad, then iterating over mini-batches and replacing the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/nas/ucb/oliveradk/diverse-gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok so the basic setup would be pretraining on the whole thing (already done) \n",
    "# then finetuning on the source and target data using the standard losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from losses.divdis import DivDisLoss \n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.conf import ConfLoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.smooth_top_loss import SmoothTopLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from models.backbone import MultiHeadBackbone\n",
    "from utils.utils import batch_size, to_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "@dataclass \n",
    "class Config: \n",
    "    loss_type: LossType = LossType.TOPK\n",
    "    lr=1e-5#2e-5\n",
    "    weight_decay=1e-2\n",
    "    epochs=5\n",
    "    scheduler=\"cosine\"\n",
    "    frac_warmup=0.05\n",
    "    num_epochs=5\n",
    "    batch_size=2\n",
    "    gradient_accumulation_steps=16 # 2 * 32\n",
    "    seed=42\n",
    "    max_length=1024\n",
    "    dataset_len=64\n",
    "    binary=True\n",
    "    heads=2\n",
    "    source_weight=1.0\n",
    "    aux_weight=1.0\n",
    "    mix_rate_lower_bound=0.1\n",
    "    use_group_labels=False\n",
    "    mixed_precision=False\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    exp_dir=f\"output/mtd/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = \"oliverdk/codegen-350M-mono-measurement_pred\"\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    trust_remote_code=True, \n",
    "    device_map=conf.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True, \n",
    "    padding_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"redwoodresearch/diamonds-seed0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiamondsDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        # tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            item['text'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # convert label to tensor\n",
    "        label = torch.tensor(item['is_correct'])\n",
    "        all_measurements = torch.tensor(all(item['measurements'])).float()\n",
    "        group_labels = torch.stack((label, all_measurements))\n",
    "        encoding = {k:v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding, label, group_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.dataset_len is not None:\n",
    "    for k, subset in dataset.items():\n",
    "        dataset[k] = subset.select(range(conf.dataset_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'is_correct', 'is_clean', 'measurements', 'difficulty'],\n",
       "    num_rows: 64\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source (is clean)\n",
    "val_frac = 0.2\n",
    "source_data = dataset[\"train\"].filter(lambda x: x[\"is_clean\"])\n",
    "splits = source_data.train_test_split(train_size=1-val_frac, test_size=val_frac, seed=conf.seed)\n",
    "dataset[\"source_train\"] = splits['train']\n",
    "dataset[\"source_val\"] = splits['test']\n",
    "\n",
    "# target (is not clean)\n",
    "target_data = dataset[\"train\"].filter(lambda x: not x[\"is_clean\"])\n",
    "dataset[\"target_train\"] = target_data\n",
    "dataset[\"target_val\"] = dataset[\"train_for_val\"]\n",
    "\n",
    "# test (validation)\n",
    "dataset[\"test\"] = dataset[\"validation\"]\n",
    "\n",
    "# remove train and train_for_val\n",
    "dataset.pop(\"train\")\n",
    "dataset.pop(\"train_for_val\")\n",
    "dataset.pop(\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source train: tampering 0.00, fake positive 0.00\n",
      "target train: tampering 0.29, fake positive 0.12\n",
      "source val: tampering 0.00, fake positive 0.00\n",
      "target val: tampering 0.53, fake positive 0.45\n",
      "test: tampering 0.39, fake positive 0.31\n"
     ]
    }
   ],
   "source": [
    "def is_tampering(x):\n",
    "    return not x[\"is_correct\"] and any(x[\"measurements\"])\n",
    "\n",
    "def fake_positive(x):\n",
    "    return not x[\"is_correct\"] and all(x[\"measurements\"])\n",
    "\n",
    "def split_tampering_rate(dataset):\n",
    "    tampering_rate = len(dataset.filter(is_tampering)) / len(dataset)\n",
    "    return tampering_rate\n",
    "def split_fake_positive_rate(dataset):\n",
    "    fake_positive_rate = len(dataset.filter(fake_positive)) / len(dataset)\n",
    "    return fake_positive_rate\n",
    "source_train_tampering_rate = split_tampering_rate(dataset[\"source_train\"])\n",
    "target_train_tampering_rate = split_tampering_rate(dataset[\"target_train\"])\n",
    "source_val_tampering_rate = split_tampering_rate(dataset[\"source_val\"])\n",
    "target_val_tampering_rate = split_tampering_rate(dataset[\"target_val\"])\n",
    "test_tampering_rate = split_tampering_rate(dataset[\"test\"])   \n",
    "\n",
    "source_train_fake_positive_rate = split_fake_positive_rate(dataset[\"source_train\"])\n",
    "target_train_fake_positive_rate = split_fake_positive_rate(dataset[\"target_train\"])\n",
    "source_val_fake_positive_rate = split_fake_positive_rate(dataset[\"source_val\"])\n",
    "target_val_fake_positive_rate = split_fake_positive_rate(dataset[\"target_val\"])\n",
    "test_fake_positive_rate = split_fake_positive_rate(dataset[\"test\"])\n",
    "\n",
    "print(f\"source train: tampering {source_train_tampering_rate:.2f}, fake positive {source_train_fake_positive_rate:.2f}\")\n",
    "print(f\"target train: tampering {target_train_tampering_rate:.2f}, fake positive {target_train_fake_positive_rate:.2f}\")\n",
    "print(f\"source val: tampering {source_val_tampering_rate:.2f}, fake positive {source_val_fake_positive_rate:.2f}\")\n",
    "print(f\"target val: tampering {target_val_tampering_rate:.2f}, fake positive {target_val_fake_positive_rate:.2f}\")\n",
    "print(f\"test: tampering {test_tampering_rate:.2f}, fake positive {test_fake_positive_rate:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_ds = DiamondsDataset(dataset[\"source_train\"], tokenizer, conf.max_length)\n",
    "source_val_ds = DiamondsDataset(dataset[\"source_val\"], tokenizer, conf.max_length)\n",
    "target_train_ds = DiamondsDataset(dataset[\"target_train\"], tokenizer, conf.max_length)\n",
    "target_val_ds = DiamondsDataset(dataset[\"target_val\"], tokenizer, conf.max_length)\n",
    "test_ds = DiamondsDataset(dataset[\"test\"], tokenizer, conf.max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(source_train_ds, batch_size=conf.batch_size)\n",
    "x = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[50256, 50256, 50256,  ...,   685, 42848,   198],\n",
       "          [50256, 50256, 50256,  ...,   685, 42848,   198]]),\n",
       "  'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       " tensor([True, True]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], x[1], x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeasurementPredBackbone(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.pretrained_model.base_model(x['input_ids'], attention_mask=x['attention_mask'])\n",
    "        embd = out.last_hidden_state[:, -1, :]\n",
    "        return embd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_model = MeasurementPredBackbone(pretrained_model).to('cpu')\n",
    "# out = pred_model(x[0])\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "pred_model = MeasurementPredBackbone(pretrained_model).to(conf.device)\n",
    "net = MultiHeadBackbone(pred_model, n_heads=2, feature_dim=1024, classes=1).to(conf.device)\n",
    "\n",
    "source_train_loader = DataLoader(source_train_ds, batch_size=conf.batch_size)\n",
    "target_train_loader = DataLoader(target_train_ds, batch_size=conf.batch_size)\n",
    "source_val_loader = DataLoader(source_val_ds, batch_size=conf.batch_size)\n",
    "target_val_loader = DataLoader(target_val_ds, batch_size=conf.batch_size)\n",
    "target_test_loader = DataLoader(test_ds, batch_size=conf.batch_size)\n",
    "\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "num_training_steps = conf.num_epochs * len(source_train_loader) // conf.gradient_accumulation_steps\n",
    "scheduler = get_scheduler(\n",
    "    name=conf.scheduler,\n",
    "    optimizer=opt,\n",
    "    num_warmup_steps=conf.frac_warmup * num_training_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=2)\n",
    "elif conf.loss_type == LossType.TOPK:\n",
    "    loss_fn = ACELoss(\n",
    "        heads=2, \n",
    "        classes=2, \n",
    "        binary=True, \n",
    "        mode=\"topk\", \n",
    "        mix_rate=conf.mix_rate_lower_bound, \n",
    "        pseudo_label_all_groups=False, \n",
    "        device=conf.device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_src_losses(logits, y, gl, binary, use_group_labels):\n",
    "    logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "    labels = torch.cat([y, y], dim=-1) if not use_group_labels else gl\n",
    "    labels_chunked = torch.chunk(labels, conf.heads, dim=-1)\n",
    "    if binary:\n",
    "        losses = [F.binary_cross_entropy_with_logits(logit.squeeze(), y.squeeze().to(torch.float32)) for logit, y in zip(logits_chunked, labels_chunked)]\n",
    "    else:\n",
    "        losses = [F.cross_entropy(logit.squeeze(), y.squeeze().to(torch.long)) for logit, y in zip(logits_chunked, labels_chunked)]\n",
    "    return losses\n",
    "\n",
    "def compute_corrects(logits: torch.Tensor, head: int, y: torch.Tensor, binary: bool):\n",
    "    if binary:\n",
    "        return ((logits[:, head] > 0) == y.flatten()).sum().item()\n",
    "    else:\n",
    "        logits = logits.view(logits.size(0), conf.heads, -1)\n",
    "        return (logits[:, head].argmax(dim=-1) == y).sum().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute loss over accumulated batch (can I do that? maybe not...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source train: 100%|██████████| 6/6 [00:04<00:00,  1.25it/s]\n",
      "Target val: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s]\n",
      "/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Source val: 100%|██████████| 2/2 [00:00<00:00,  7.53it/s]\n",
      "Target test: 100%|██████████| 32/32 [00:04<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 AUROC 0: 0.5896, Alt: 0.3835\n",
      "Epoch 1 AUROC 1: 0.1682, Alt: 0.0777\n",
      "Epoch 1 Test Accuracies:\n",
      "Target val repulsion loss: nan\n",
      "Target val weighted repulsion loss: nan\n",
      "Source val xent: 4.0897\n",
      "val loss: nan\n",
      "val weighted loss: nan\n",
      "Head 0: 0.5625, Alt: 0.2500\n",
      "Head 1: 0.3594, Alt: 0.1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source train: 100%|██████████| 6/6 [00:04<00:00,  1.38it/s]\n",
      "Target val:  88%|████████▊ | 28/32 [00:03<00:00,  7.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(target_val_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget val\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     70\u001b[0m     x, y, gl \u001b[38;5;241m=\u001b[39m to_device(\u001b[38;5;241m*\u001b[39mbatch, conf\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 71\u001b[0m     logits_val \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     target_loss_val \u001b[38;5;241m=\u001b[39m loss_fn(logits_val)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target_loss_val\u001b[38;5;241m.\u001b[39misnan():\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/diverse-gen/models/backbone.py:19\u001b[0m, in \u001b[0;36mMultiHeadBackbone.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Get features from the shared backbone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     bs \u001b[38;5;241m=\u001b[39m batch_size(x)\n\u001b[0;32m---> 19\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Apply the heads to the features\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads(features)\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m, in \u001b[0;36mMeasurementPredBackbone.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 7\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     embd \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embd\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/transformers/models/codegen/modeling_codegen.py:536\u001b[0m, in \u001b[0;36mCodeGenModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    526\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    527\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m         output_attentions,\n\u001b[1;32m    534\u001b[0m     )\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/transformers/models/codegen/modeling_codegen.py:272\u001b[0m, in \u001b[0;36mCodeGenBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    270\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    271\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 272\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    282\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/transformers/models/codegen/modeling_codegen.py:217\u001b[0m, in \u001b[0;36mCodeGenAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    214\u001b[0m     present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# compute self-attention: V x Softmax(QK^T)\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    220\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(attn_output)\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/od_3_10/lib/python3.10/site-packages/transformers/models/codegen/modeling_codegen.py:131\u001b[0m, in \u001b[0;36mCodeGenAttention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    128\u001b[0m mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights, mask_value)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: change diciotary values to source loss, target loss\n",
    "\n",
    "classes = 2\n",
    "\n",
    "alt_index = 1\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from torch.amp import GradScaler, autocast\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# metrics\n",
    "metrics = defaultdict(list)\n",
    "writer = SummaryWriter(log_dir=conf.exp_dir)\n",
    "\n",
    "\n",
    "# grad_scaler = GradScaler()\n",
    "\n",
    "# For now, I'll just compte all the logits with no grad, then iterate over in mini-batches, \n",
    "# and replace the logits at the index of the batch with the graident version\n",
    "target_iter = iter(target_train_loader)\n",
    "for epoch in range(conf.epochs):\n",
    "    target_logit_ls = []\n",
    "    for batch_idx, (x, y, gl) in tqdm(enumerate(source_train_loader), desc=\"Source train\", total=len(source_train_loader)):\n",
    "        x, y, gl = to_device(x, y, gl, conf.device)\n",
    "        # with autocast(conf.device, enabled=conf.mixed_precision):\n",
    "        logits = net(x)\n",
    "        losses = compute_src_losses(logits, y, gl, conf.binary, conf.use_group_labels)\n",
    "        xent = sum(losses)\n",
    "        writer.add_scalar(\"train/source_loss\", xent.item(), epoch * len(source_train_loader) + batch_idx)\n",
    "        \n",
    "        # # compute target logits with no grad \n",
    "        \n",
    "        # target loss \n",
    "        try: \n",
    "            target_batch = next(target_iter)\n",
    "        except StopIteration:\n",
    "            target_iter = iter(target_train_loader)\n",
    "            target_batch = next(target_iter)\n",
    "        target_x, target_y, target_gl = to_device(*target_batch, conf.device)\n",
    "        # with autocast(conf.device, enabled=conf.mixed_precision):\n",
    "        target_logits = net(target_x)\n",
    "        target_loss = loss_fn(target_logits)\n",
    "        writer.add_scalar(\"train/target_loss\", target_loss.item(), epoch * len(target_train_loader) + batch_idx)\n",
    "        writer.add_scalar(\"train/weighted_target_loss\", conf.aux_weight * target_loss.item(), epoch * len(target_train_loader) + batch_idx)\n",
    "        # full loss \n",
    "        full_loss = conf.source_weight * xent + conf.aux_weight * target_loss\n",
    "        # grad_scaler.scale(full_loss).backward()\n",
    "        full_loss.backward()\n",
    "        if (batch_idx + 1) % conf.gradient_accumulation_steps == 0:\n",
    "            # unscale and clip gradients\n",
    "            # grad_scaler.unscale_(opt)\n",
    "            # torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "\n",
    "            # update weights, clear gradients \n",
    "            # grad_scaler.step(opt)\n",
    "            opt.step()\n",
    "            # grad_scaler.update()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        metrics[f\"xent\"].append(xent.item())\n",
    "        metrics[f\"repulsion_loss\"].append(target_loss.item())\n",
    "    # Compute loss on target validation set (used for model selection)\n",
    "    # and aggregate metrics over the entire test set (should not really be using)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        net.eval()\n",
    "        # compute repulsion loss on target validation set (used for model selection)\n",
    "        target_losses_val = []\n",
    "        weighted_target_losses_val = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(target_val_loader, desc=\"Target val\"):\n",
    "                x, y, gl = to_device(*batch, conf.device)\n",
    "                logits_val = net(x)\n",
    "                target_loss_val = loss_fn(logits_val)\n",
    "                if not target_loss_val.isnan():\n",
    "                    target_losses_val.append(target_loss_val.item())\n",
    "                    weighted_target_losses_val.append(conf.aux_weight * target_loss_val.item())\n",
    "        metrics[f\"target_val_repulsion_loss\"].append(np.mean(target_losses_val))\n",
    "        metrics[f\"target_val_weighted_repulsion_loss\"].append(np.mean(weighted_target_losses_val))\n",
    "        writer.add_scalar(\"val/target_loss\", np.mean(target_losses_val), epoch)\n",
    "        writer.add_scalar(\"val/weighted_target_loss\", np.mean(weighted_target_losses_val), epoch)\n",
    "        # compute xent on source validation set\n",
    "        xent_val = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(source_val_loader, desc=\"Source val\"):\n",
    "                x, y, gl = to_device(*batch, conf.device)\n",
    "                logits_val = net(x)\n",
    "                losses_val = compute_src_losses(logits_val, y, gl, conf.binary, conf.use_group_labels)\n",
    "                xent_val.append(sum(losses_val).item())\n",
    "        metrics[f\"source_val_xent\"].append(np.mean(xent_val))\n",
    "        metrics[f\"val_loss\"].append(np.mean(target_losses_val) + np.mean(xent_val))\n",
    "        metrics[f\"val_weighted_loss\"].append(np.mean(weighted_target_losses_val) + np.mean(xent_val))\n",
    "        writer.add_scalar(\"val/source_loss\", np.mean(xent_val), epoch)\n",
    "        writer.add_scalar(\"val/val_loss\", np.mean(target_losses_val) + np.mean(xent_val), epoch)\n",
    "        writer.add_scalar(\"val/weighted_val_loss\", np.mean(weighted_target_losses_val) + np.mean(xent_val), epoch)\n",
    "        \n",
    "        # compute accuracy over target test set (used to evaluate actual performance)\n",
    "        total_correct = torch.zeros(conf.heads)\n",
    "        total_correct_alt = torch.zeros(conf.heads)\n",
    "        total_samples = 0\n",
    "\n",
    "        # store predictions\n",
    "        all_preds = [[] for _ in range(conf.heads)]\n",
    "        all_preds_alt = [[] for _ in range(conf.heads)]\n",
    "        all_labels = []\n",
    "        all_labels_alt = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_batch in tqdm(target_test_loader, desc=\"Target test\"):\n",
    "                test_x, test_y, test_gl = to_device(*test_batch, conf.device)\n",
    "                test_logits = net(test_x)\n",
    "                assert test_logits.shape == (batch_size(test_x), conf.heads * (1 if conf.binary else classes))\n",
    "                total_samples += test_y.size(0)\n",
    "\n",
    "                 # Store labels for AUROC\n",
    "                all_labels.extend(test_y.cpu().numpy())\n",
    "                all_labels_alt.extend(test_gl[:, alt_index].cpu().numpy())\n",
    "                \n",
    "                for i in range(conf.heads):\n",
    "                    total_correct[i] += compute_corrects(test_logits, i, test_y, conf.binary)\n",
    "                    total_correct_alt[i] += compute_corrects(test_logits, i, test_gl[:, alt_index], conf.binary)\n",
    "                    probs = torch.sigmoid(test_logits[:, i]).cpu().numpy()\n",
    "                    all_preds[i].extend(probs)\n",
    "        \n",
    "        # Compute and store AUROC for each head\n",
    "        for i in range(conf.heads):\n",
    "            auroc = roc_auc_score(all_labels, all_preds[i])\n",
    "            auroc_alt = roc_auc_score(all_labels_alt, all_preds[i])\n",
    "            metrics[f\"epoch_auroc_{i}\"].append(auroc)\n",
    "            metrics[f\"epoch_auroc_{i}_alt\"].append(auroc_alt)\n",
    "            writer.add_scalar(f\"val/auroc_{i}\", auroc, epoch)\n",
    "            writer.add_scalar(f\"val/auroc_{i}_alt\", auroc_alt, epoch)\n",
    "            print(f\"Epoch {epoch + 1} AUROC {i}: {auroc:.4f}, Alt: {auroc_alt:.4f}\")\n",
    "\n",
    "        # compute and store accuracy for each head\n",
    "        for i in range(conf.heads):\n",
    "            metrics[f\"epoch_acc_{i}\"].append((total_correct[i] / total_samples).item())\n",
    "            metrics[f\"epoch_acc_{i}_alt\"].append((total_correct_alt[i] / total_samples).item())\n",
    "            writer.add_scalar(f\"val/acc_{i}\", (total_correct[i] / total_samples).item(), epoch)\n",
    "            writer.add_scalar(f\"val/acc_{i}_alt\", (total_correct_alt[i] / total_samples).item(), epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Test Accuracies:\")\n",
    "        # print validation losses\n",
    "        print(f\"Target val repulsion loss: {metrics[f'target_val_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Target val weighted repulsion loss: {metrics[f'target_val_weighted_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Source val xent: {metrics[f'source_val_xent'][-1]:.4f}\")\n",
    "        print(f\"val loss: {metrics[f'val_loss'][-1]:.4f}\")\n",
    "        print(f\"val weighted loss: {metrics[f'val_weighted_loss'][-1]:.4f}\")\n",
    "        for i in range(conf.heads):\n",
    "            print(f\"Head {i}: {metrics[f'epoch_acc_{i}'][-1]:.4f}, Alt: {metrics[f'epoch_acc_{i}_alt'][-1]:.4f}\")\n",
    "        \n",
    "        \n",
    "        net.train()\n",
    "\n",
    "metrics = dict(metrics)\n",
    "# save metrics \n",
    "import json \n",
    "with open(f\"{conf.exp_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels, all_preds[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
