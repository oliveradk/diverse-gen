{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: separete target labeled and unlabeled loss \n",
    "# run experiment using source and target visisible (no unlabeled loss) # all vs any\n",
    "# run experiment using source and target visible, with unlabeled loss # all vs any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/nas/ucb/oliveradk/diverse-gen\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from losses.divdis import DivDisLoss \n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.conf import ConfLoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.smooth_top_loss import SmoothTopLoss\n",
    "from losses.pass_through import PassThroughLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from models.backbone import MultiHeadBackbone\n",
    "from utils.utils import batch_size, to_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "@dataclass \n",
    "class Config: \n",
    "    loss_type: LossType = LossType.TOPK\n",
    "    lr=2e-5\n",
    "    weight_decay=2e-2\n",
    "    epochs=5\n",
    "    scheduler=\"cosine\"\n",
    "    frac_warmup=0.05\n",
    "    num_epochs=5\n",
    "    effective_batch_size=32\n",
    "    forward_batch_size=32\n",
    "    micro_batch_size=4\n",
    "    use_visible_labels=True \n",
    "    use_negative_visible=False\n",
    "    all_measurements=False\n",
    "    seed=42\n",
    "    max_length=1024\n",
    "    dataset_len=256\n",
    "    binary=True\n",
    "    heads=2\n",
    "    train=True\n",
    "    freeze_model=False\n",
    "    load_prior_probe=False\n",
    "    source_weight=1.0\n",
    "    aux_weight=1.0\n",
    "    mix_rate_lower_bound=0.1\n",
    "    use_group_labels=False\n",
    "    num_workers=2\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    exp_dir=f\"output/mtd/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "def post_init(conf, overrride_keys):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrride_keys = []\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    overrides = OmegaConf.from_cli(sys.argv[1:])\n",
    "    overrride_keys = overrides.keys()\n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), overrides)\n",
    "    conf = Config(**conf_dict)\n",
    "post_init(conf, overrride_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = conf.exp_dir\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "# save full config to exp_dir\n",
    "with open(f\"{exp_dir}/config.yaml\", \"w\") as f:\n",
    "    OmegaConf.save(config=conf, f=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"oliverdk/codegen-350M-mono-measurement_pred\"\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    trust_remote_code=True, \n",
    "    device_map=conf.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True, \n",
    "    padding_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"redwoodresearch/diamonds-seed0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"data/diamonds/\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "def encode_dataset(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=conf.max_length,\n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(\n",
    "    encode_dataset,\n",
    "    batched=True,\n",
    "    cache_file_names={\n",
    "        split: f\"{dataset_dir}/seed0_{split}_{conf.dataset_len}_{conf.max_length}.arrow\"\n",
    "        for split in dataset.keys()\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiamondsDataset(Dataset):\n",
    "    def __init__(self, dataset, max_length=1024, negative_visible=False, all_measurements=False):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "        self.negative_visible = negative_visible\n",
    "        self.all_measurements = all_measurements\n",
    "\n",
    "        self.measurements = torch.tensor(self.dataset['measurements'])\n",
    "        self.ground_truth = torch.tensor(self.dataset['is_correct'])\n",
    "        self.is_trusted = torch.tensor(self.dataset['is_clean'])\n",
    "\n",
    "        # self.encodings = tokenizer(\n",
    "        #     self.dataset['text'],\n",
    "        #     max_length=self.max_length,\n",
    "        #     padding='max_length',\n",
    "        #     truncation=True,\n",
    "        #     return_tensors='pt'\n",
    "        # )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = {\n",
    "            \"input_ids\": torch.tensor(self.dataset['input_ids'][idx]),\n",
    "            \"attention_mask\": torch.tensor(self.dataset['attention_mask'][idx]), \n",
    "        }\n",
    "\n",
    "        # labels\n",
    "        label = self.ground_truth[idx]\n",
    "        tampering = torch.all(self.measurements[idx]) if self.all_measurements else torch.any(self.measurements[idx])\n",
    "        \n",
    "        # visible if failed tampering (some but not all measurements)\n",
    "        failed_tampering = torch.any(self.measurements[idx]) & ~torch.all(self.measurements[idx])\n",
    "        visible = failed_tampering \n",
    "        # visible if negative (implies no tampering b/c agent has no incentive to tamper)\n",
    "        if self.negative_visible:\n",
    "            negative = torch.tensor(not torch.any(self.measurements[idx]))\n",
    "            visible = torch.logical_or(visible, negative)\n",
    "        \n",
    "        group_labels = torch.stack((label, tampering, visible))\n",
    "\n",
    "\n",
    "        # set labels to floats \n",
    "        label = label.to(torch.float32)\n",
    "        group_labels = group_labels.to(torch.float32)\n",
    "        \n",
    "        return encoding, label, group_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.dataset_len is not None:\n",
    "    for k, subset in dataset.items():\n",
    "        dataset[k] = subset.select(range(conf.dataset_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source (is clean)\n",
    "val_frac = 0.2\n",
    "source_data = dataset[\"train\"].filter(lambda x: x[\"is_clean\"])\n",
    "splits = source_data.train_test_split(train_size=1-val_frac, test_size=val_frac, seed=conf.seed)\n",
    "dataset[\"source_train\"] = splits['train']\n",
    "dataset[\"source_val\"] = splits['test']\n",
    "\n",
    "# target (is not clean)\n",
    "target_data = dataset[\"train\"].filter(lambda x: not x[\"is_clean\"])\n",
    "dataset[\"target_train\"] = target_data\n",
    "dataset[\"target_val\"] = dataset[\"train_for_val\"]\n",
    "\n",
    "# test (validation)\n",
    "dataset[\"test\"] = dataset[\"validation\"]\n",
    "# only untrusted positive examples \n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: not x['is_clean'] and all(x[\"measurements\"]))\n",
    "\n",
    "# remove train and train_for_val\n",
    "dataset.pop(\"train\")\n",
    "dataset.pop(\"train_for_val\")\n",
    "dataset.pop(\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tampering(x):\n",
    "    return not x[\"is_correct\"] and any(x[\"measurements\"])\n",
    "\n",
    "def fake_positive(x):\n",
    "    return not x[\"is_correct\"] and all(x[\"measurements\"])\n",
    "\n",
    "def split_tampering_rate(dataset):\n",
    "    tampering_rate = len(dataset.filter(is_tampering)) / len(dataset)\n",
    "    return tampering_rate\n",
    "def split_fake_positive_rate(dataset):\n",
    "    fake_positive_rate = len(dataset.filter(fake_positive)) / len(dataset)\n",
    "    return fake_positive_rate\n",
    "source_train_tampering_rate = split_tampering_rate(dataset[\"source_train\"])\n",
    "target_train_tampering_rate = split_tampering_rate(dataset[\"target_train\"])\n",
    "source_val_tampering_rate = split_tampering_rate(dataset[\"source_val\"])\n",
    "target_val_tampering_rate = split_tampering_rate(dataset[\"target_val\"])\n",
    "test_tampering_rate = split_tampering_rate(dataset[\"test\"])   \n",
    "\n",
    "source_train_fake_positive_rate = split_fake_positive_rate(dataset[\"source_train\"])\n",
    "target_train_fake_positive_rate = split_fake_positive_rate(dataset[\"target_train\"])\n",
    "source_val_fake_positive_rate = split_fake_positive_rate(dataset[\"source_val\"])\n",
    "target_val_fake_positive_rate = split_fake_positive_rate(dataset[\"target_val\"])\n",
    "test_fake_positive_rate = split_fake_positive_rate(dataset[\"test\"])\n",
    "\n",
    "print(f\"source train: tampering {source_train_tampering_rate:.2f}, fake positive {source_train_fake_positive_rate:.2f}\")\n",
    "print(f\"target train: tampering {target_train_tampering_rate:.2f}, fake positive {target_train_fake_positive_rate:.2f}\")\n",
    "print(f\"source val: tampering {source_val_tampering_rate:.2f}, fake positive {source_val_fake_positive_rate:.2f}\")\n",
    "print(f\"target val: tampering {target_val_tampering_rate:.2f}, fake positive {target_val_fake_positive_rate:.2f}\")\n",
    "print(f\"test: tampering {test_tampering_rate:.2f}, fake positive {test_fake_positive_rate:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_ds = DiamondsDataset(dataset[\"source_train\"], conf.max_length, conf.use_negative_visible, conf.all_measurements)\n",
    "source_val_ds = DiamondsDataset(dataset[\"source_val\"], conf.max_length, conf.use_negative_visible, conf.all_measurements)\n",
    "target_train_ds = DiamondsDataset(dataset[\"target_train\"], conf.max_length, conf.use_negative_visible, conf.all_measurements)\n",
    "target_val_ds = DiamondsDataset(dataset[\"target_val\"], conf.max_length, conf.use_negative_visible, conf.all_measurements)\n",
    "test_ds = DiamondsDataset(dataset[\"test\"], conf.max_length, conf.use_negative_visible, conf.all_measurements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeasurementPredBackbone(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.pretrained_model.base_model(x['input_ids'], attention_mask=x['attention_mask'])\n",
    "        embd = out.last_hidden_state[:, -1, :]\n",
    "        return embd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = MeasurementPredBackbone(pretrained_model).to(conf.device)\n",
    "net = MultiHeadBackbone(pred_model, n_heads=2, feature_dim=1024, classes=1).to(conf.device)\n",
    "\n",
    "if conf.freeze_model:\n",
    "    for param in net.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# load weights of pretrained model aggregate probe to second net head\n",
    "if conf.load_prior_probe:\n",
    "    net.heads.weight.data[1, :] = pretrained_model.aggregate_probe.weight.data[0]\n",
    "    net.heads.bias.data[1] = pretrained_model.aggregate_probe.bias.data[0]\n",
    "\n",
    "source_train_loader = DataLoader(source_train_ds, batch_size=conf.micro_batch_size, num_workers=conf.num_workers)\n",
    "target_train_loader = DataLoader(target_train_ds, batch_size=conf.effective_batch_size, num_workers=conf.num_workers)\n",
    "source_val_loader = DataLoader(source_val_ds, batch_size=conf.micro_batch_size, num_workers=conf.num_workers)\n",
    "target_val_loader = DataLoader(target_val_ds, batch_size=conf.effective_batch_size, num_workers=conf.num_workers)\n",
    "target_test_loader = DataLoader(test_ds, batch_size=conf.forward_batch_size, num_workers=conf.num_workers)\n",
    "\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "num_training_steps = conf.num_epochs * len(source_train_loader) // (conf.effective_batch_size // conf.micro_batch_size)\n",
    "scheduler = get_scheduler(\n",
    "    name=conf.scheduler,\n",
    "    optimizer=opt,\n",
    "    num_warmup_steps=conf.frac_warmup * num_training_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=2)\n",
    "elif conf.loss_type == LossType.ERM:\n",
    "    loss_fn = PassThroughLoss()\n",
    "elif conf.loss_type == LossType.TOPK:\n",
    "    loss_fn = ACELoss(\n",
    "        heads=2, \n",
    "        classes=2, \n",
    "        binary=True, \n",
    "        mode=\"topk\", \n",
    "        group_mix_rates={(0, 1): conf.mix_rate_lower_bound},  # TODO: should ignore visible labels\n",
    "        # mix_rate=conf.mix_rate_lower_bound, \n",
    "        pseudo_label_all_groups=False, \n",
    "        device=conf.device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_src_losses(logits, y, gl, binary, use_group_labels):\n",
    "    logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "    labels = torch.cat([y, y], dim=-1) if not use_group_labels else gl\n",
    "    labels_chunked = torch.chunk(labels, conf.heads, dim=-1)\n",
    "    if binary:\n",
    "        losses = [F.binary_cross_entropy_with_logits(logit.squeeze(), y.squeeze().to(torch.float32)) for logit, y in zip(logits_chunked, labels_chunked)]\n",
    "    else:\n",
    "        losses = [F.cross_entropy(logit.squeeze(), y.squeeze().to(torch.long)) for logit, y in zip(logits_chunked, labels_chunked)]\n",
    "    return losses\n",
    "\n",
    "def compute_corrects(logits: torch.Tensor, head: int, y: torch.Tensor, binary: bool):\n",
    "    if binary:\n",
    "        return ((logits[:, head] > 0) == y.flatten()).sum().item()\n",
    "    else:\n",
    "        logits = logits.view(logits.size(0), conf.heads, -1)\n",
    "        return (logits[:, head].argmax(dim=-1) == y).sum().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change diciotary values to source loss, target loss\n",
    "\n",
    "classes = 2\n",
    "alt_index = 1\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# metrics\n",
    "metrics = defaultdict(list)\n",
    "writer = SummaryWriter(log_dir=conf.exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_batch(batch, slice):\n",
    "    if isinstance(batch, torch.Tensor):\n",
    "        return batch[slice]\n",
    "    elif isinstance(batch, dict):\n",
    "        return {k: v[slice] for k, v in batch.items()}\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported batch type: {type(batch)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_slice(idx, slice):\n",
    "    return idx >= slice.start and idx < slice.stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments to try: \n",
    "## using failed tampering instances as positive tampering (i.e. modify group labels to add a third, which is basically (visible), and the heads should be trained to ouptut the group labels on those)\n",
    "## use labels on negative examples (we know no tampering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_visible_target_loss(visible_logits, visible_gl):    \n",
    "\n",
    "    # visible logits chunked\n",
    "    logits_chunked = torch.chunk(visible_logits, conf.heads, dim=-1)\n",
    "\n",
    "    # visible group labels chunked\n",
    "    gl_chunked = torch.chunk(visible_gl[:, :2], conf.heads, dim=-1)\n",
    "    \n",
    "    # compute losses for each head\n",
    "    losses = [\n",
    "        F.binary_cross_entropy_with_logits(\n",
    "            logit.squeeze(), y_i.squeeze().to(torch.float32)\n",
    "        )\n",
    "        for logit, y_i in zip(logits_chunked, gl_chunked)\n",
    "    ]\n",
    "    return sum(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_loss(logits, y, gl, loss_fn, loss_type, use_visible_labels): \n",
    "    if not use_visible_labels:\n",
    "        return loss_fn(logits), torch.tensor(0.0)\n",
    "    \n",
    "    visible_mask = gl[:, 2].bool()\n",
    "    \n",
    "    # compute div loss\n",
    "    if loss_type == LossType.TOPK: \n",
    "        non_visible_logits = logits[~visible_mask]\n",
    "        div_loss = loss_fn(non_visible_logits)\n",
    "    else: \n",
    "        div_loss = loss_fn(logits)\n",
    "\n",
    "    visible_loss = torch.tensor(0.0)\n",
    "    if any(visible_mask):\n",
    "        visible_loss = compute_visible_target_loss(logits[visible_mask], gl[visible_mask])\n",
    "\n",
    "    return div_loss, visible_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy (both labels) and auroc (true vs fake positives) on test set\n",
    "def eval(net, loader, conf): \n",
    "    net.eval()\n",
    "\n",
    "    head_accs = []\n",
    "    head_accs_alt = []\n",
    "    head_aurocs = []\n",
    "    total_correct = torch.zeros(conf.heads)\n",
    "    total_correct_alt = torch.zeros(conf.heads)\n",
    "    total_samples = 0\n",
    "    all_preds = [[] for _ in range(conf.heads)]\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in tqdm(loader, desc=\"Target test\"):\n",
    "            test_x, test_y, test_gl = to_device(*test_batch, conf.device)\n",
    "            test_logits = net(test_x)\n",
    "            assert test_logits.shape == (batch_size(test_x), conf.heads * (1 if conf.binary else classes))\n",
    "            total_samples += test_y.size(0)\n",
    "\n",
    "                # Store labels for AUROC\n",
    "            all_labels.extend(test_y.cpu().numpy())\n",
    "            \n",
    "            for i in range(conf.heads):\n",
    "                total_correct[i] += compute_corrects(test_logits, i, test_y, conf.binary)\n",
    "                total_correct_alt[i] += compute_corrects(test_logits, i, test_gl[:, 1], conf.binary)\n",
    "                probs = torch.sigmoid(test_logits[:, i]).cpu().numpy()\n",
    "                all_preds[i].extend(probs)\n",
    "\n",
    "    # Compute and store AUROC for each head\n",
    "    for i in range(conf.heads):\n",
    "        auroc = roc_auc_score(all_labels, all_preds[i])\n",
    "        head_aurocs.append(auroc)\n",
    "\n",
    "    # compute and store accuracy for each head\n",
    "    for i in range(conf.heads):\n",
    "        head_accs.append((total_correct[i] / total_samples).item())\n",
    "        head_accs_alt.append((total_correct_alt[i] / total_samples).item())\n",
    "    return head_accs, head_accs_alt, head_aurocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not conf.train:\n",
    "    head_accs, head_accs_alt, head_aurocs = eval(net, target_test_loader, conf)\n",
    "    print(f\"Test Accuracies:\")\n",
    "    for i in range(conf.heads):\n",
    "        print(f\"Head {i}: {head_accs[i]:.4f}, Alt: {head_accs_alt[i]:.4f}\")\n",
    "    print(f\"Test AUROCs:\")\n",
    "    for i in range(conf.heads):\n",
    "        print(f\"Head {i}: {head_aurocs[i]:.4f}\")\n",
    "    # stop run all \n",
    "    raise ValueError(\"Stop run all (not an actual error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader with effective batch size, then iterate over micro batches within batch \n",
    "target_iter = iter(target_train_loader)\n",
    "target_batch = None\n",
    "target_logits = None\n",
    "\n",
    "for epoch in range(conf.epochs):\n",
    "    target_logit_ls = []\n",
    "    source_batch_loss = 0\n",
    "    source_batch_corrects = {i: 0 for i in range(conf.heads)}\n",
    "    target_batch_corrects = {(i, label): 0 for i in range(conf.heads) for label in [\"y\", \"gl\"]}\n",
    "    for batch_idx, (x, y, gl) in tqdm(enumerate(source_train_loader), desc=\"Source train\", total=len(source_train_loader)):\n",
    "        # compute source logits with micro batch \n",
    "        x, y, gl = to_device(x, y, gl, conf.device)\n",
    "        logits = net(x)\n",
    "        losses = compute_src_losses(logits, y, gl, conf.binary, conf.use_group_labels)\n",
    "        xent = sum(losses)\n",
    "        source_batch_loss += xent.item()\n",
    "\n",
    "        # computer source acc \n",
    "        for i in range(conf.heads):\n",
    "            source_batch_corrects[i] += compute_corrects(logits, i, y, conf.binary)\n",
    "        # compute target logits with no grad on forward batch \n",
    "        div_loss = torch.tensor(0.0)\n",
    "        visible_loss = torch.tensor(0.0)\n",
    "        if conf.aux_weight > 0 or conf.use_visible_labels:\n",
    "            if batch_idx % (conf.effective_batch_size // conf.micro_batch_size) == 0:\n",
    "                target_logits_ls = []\n",
    "                try: \n",
    "                    target_batch = next(target_iter)\n",
    "                except StopIteration:\n",
    "                    target_iter = iter(target_train_loader)\n",
    "                    target_batch = next(target_iter)\n",
    "                target_batch, target_y, target_gl = to_device(*target_batch, conf.device)\n",
    "                with torch.no_grad():\n",
    "                    target_logits_ls.append(net(target_batch).detach())\n",
    "                target_logits = torch.cat(target_logits_ls, dim=0)\n",
    "            # compute target logits with grad on micro batch\n",
    "            micro_batch_idx = batch_idx % (conf.effective_batch_size // conf.micro_batch_size)\n",
    "            micro_slice = slice(micro_batch_idx * conf.micro_batch_size, (micro_batch_idx + 1) * conf.micro_batch_size)\n",
    "            target_micro_batch = slice_batch(target_batch, micro_slice)\n",
    "            target_micro_logits = net(target_micro_batch)\n",
    "\n",
    "            cloned_target_logits= target_logits.clone().requires_grad_(True)\n",
    "            new_target_logits = torch.cat([\n",
    "                cloned_target_logits[i].unsqueeze(0) if \n",
    "                not in_slice(i, micro_slice) else target_micro_logits[i - micro_slice.start].unsqueeze(0)\n",
    "                for i in range(len(cloned_target_logits))\n",
    "            ])\n",
    "\n",
    "            div_loss, visible_loss = compute_target_loss(new_target_logits, target_y, target_gl, loss_fn, conf.loss_type, conf.use_visible_labels)\n",
    "            \n",
    "            # compute target acc \n",
    "            for i in range(conf.heads):\n",
    "                target_batch_corrects[(i, \"y\")] += compute_corrects(new_target_logits, i, target_y, conf.binary) \n",
    "                target_batch_corrects[(i, \"gl\")] += compute_corrects(new_target_logits, i, target_gl[:, 1], conf.binary)\n",
    "\n",
    "        # full loss (on micro batch)\n",
    "        full_loss = conf.source_weight * xent + conf.aux_weight * div_loss + visible_loss   \n",
    "        full_loss.backward() \n",
    "        \n",
    "        # update weights, clear gradients on effective batch\n",
    "        if (batch_idx + 1) % (conf.effective_batch_size // conf.micro_batch_size) == 0:\n",
    "            opt.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            source_batch_loss = source_batch_loss / conf.effective_batch_size\n",
    "            # compute batch metrics \n",
    "            effective_batch_idx = batch_idx // (conf.effective_batch_size // conf.micro_batch_size)\n",
    "            effective_num_batches = len(source_train_loader) // (conf.effective_batch_size // conf.micro_batch_size)\n",
    "            writer.add_scalar(\"train/source_loss\", source_batch_loss, epoch * effective_num_batches + effective_batch_idx)\n",
    "            if conf.aux_weight > 0:\n",
    "                writer.add_scalar(\"train/div_loss\", div_loss.item(), epoch * effective_num_batches + effective_batch_idx)\n",
    "            if conf.use_visible_labels:\n",
    "                writer.add_scalar(\"train/visible_loss\", visible_loss.item(), epoch * effective_num_batches + effective_batch_idx)\n",
    "            writer.add_scalar(\"train/full_loss\", source_batch_loss + conf.aux_weight * div_loss.item() + visible_loss.item(), epoch * effective_num_batches + effective_batch_idx)\n",
    "            \n",
    "            for i in range(conf.heads):\n",
    "                writer.add_scalar(f\"train/source_acc_{i}\", source_batch_corrects[i] / conf.effective_batch_size, epoch * effective_num_batches + effective_batch_idx)\n",
    "                if conf.aux_weight > 0 or conf.use_visible_labels:\n",
    "                    for label in [\"y\", \"gl\"]:\n",
    "                        writer.add_scalar(f\"train/target_acc_{i}_{label}\", target_batch_corrects[(i, label)] / conf.effective_batch_size, epoch * effective_num_batches + effective_batch_idx)\n",
    "            source_batch_loss = 0\n",
    "            source_batch_corrects = {i: 0 for i in range(conf.heads)}\n",
    "            target_batch_corrects = {(i, label): 0 for i in range(conf.heads) for label in [\"y\", \"gl\"]}\n",
    "    \n",
    "    # validation and test\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        net.eval()\n",
    "        # compute repulsion loss on target validation set (used for model selection)\n",
    "        div_losses_val = []\n",
    "        visible_losses_val = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(target_val_loader, desc=\"Target val\"):\n",
    "                x, y, gl = to_device(*batch, conf.device)\n",
    "                logits_val = net(x)\n",
    "                div_loss, visible_loss = compute_target_loss(logits_val, y, gl, loss_fn, conf.loss_type, conf.use_visible_labels)\n",
    "                div_losses_val.append(div_loss.item())\n",
    "                visible_losses_val.append(visible_loss.item())\n",
    "        \n",
    "        metrics[f\"val_target_div_loss\"].append(np.mean(div_losses_val))\n",
    "        metrics[f\"val_target_visible_loss\"].append(np.mean(visible_losses_val))\n",
    "        metrics[f\"val_target_weighted_div_loss\"].append(np.mean(div_losses_val) * conf.aux_weight)\n",
    "        metrics[f\"val_target_loss\"].append(np.mean(div_losses_val) * conf.aux_weight + np.mean(visible_losses_val))\n",
    "        \n",
    "        writer.add_scalar(\"val/div_loss\", metrics[f\"val_target_div_loss\"][-1], epoch)\n",
    "        writer.add_scalar(\"val/weighted_div_loss\", metrics[f\"val_target_weighted_div_loss\"][-1], epoch)\n",
    "        writer.add_scalar(\"val/visible_loss\", metrics[f\"val_target_visible_loss\"][-1], epoch)\n",
    "        writer.add_scalar(\"val/target_loss\", metrics[f\"val_target_loss\"][-1], epoch)\n",
    "        # compute xent on source validation set\n",
    "        xent_val = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(source_val_loader, desc=\"Source val\"):\n",
    "                x, y, gl = to_device(*batch, conf.device)\n",
    "                logits_val = net(x)\n",
    "                losses_val = compute_src_losses(logits_val, y, gl, conf.binary, conf.use_group_labels)\n",
    "                xent_val.append(sum(losses_val).item())\n",
    "        metrics[f\"val_source_xent\"].append(np.mean(xent_val))\n",
    "        writer.add_scalar(\"val/source_loss\", metrics[f\"val_source_xent\"][-1], epoch)\n",
    "        \n",
    "        metrics[f\"val_loss\"].append(metrics[f\"val_target_loss\"][-1] + metrics[f\"val_source_xent\"][-1])  \n",
    "        writer.add_scalar(\"val/val_loss\", metrics[f\"val_loss\"][-1], epoch)\n",
    "        \n",
    "        # test evaluation (acc, acc_alt, auroc)\n",
    "        head_accs, head_accs_alt, head_aurocs = eval(net, target_test_loader, conf)\n",
    "        for i in range(conf.heads):\n",
    "            metrics[f\"epoch_test_acc_{i}\"].append(head_accs[i])\n",
    "            metrics[f\"epoch_test_acc_{i}_alt\"].append(head_accs_alt[i])\n",
    "            metrics[f\"epoch_test_auroc_{i}\"].append(head_aurocs[i])\n",
    "            writer.add_scalar(f\"val/test_acc_{i}\", head_accs[i], epoch)\n",
    "            writer.add_scalar(f\"val/test_acc_{i}_alt\", head_accs_alt[i], epoch)\n",
    "            writer.add_scalar(f\"val/test_auroc_{i}\", head_aurocs[i], epoch)\n",
    "        \n",
    "        # print validation losses and test accs\n",
    "        print(f\"Epoch {epoch + 1} Test Accuracies:\")\n",
    "        print(f\"Target val div loss: {metrics[f'val_target_div_loss'][-1]:.4f}\")\n",
    "        print(f\"Target val weighted div loss: {metrics[f'val_target_weighted_div_loss'][-1]:.4f}\")\n",
    "        print(f\"Source val xent: {metrics[f'val_source_xent'][-1]:.4f}\")\n",
    "        print(f\"val loss: {metrics[f'val_loss'][-1]:.4f}\")\n",
    "        for i in range(conf.heads):\n",
    "            print(f\"Head {i}: {metrics[f'epoch_test_acc_{i}'][-1]:.4f}, Alt: {metrics[f'epoch_test_acc_{i}_alt'][-1]:.4f}\")\n",
    "            print(f\"Head {i} auroc: {metrics[f'epoch_test_auroc_{i}'][-1]:.4f}\")\n",
    "        \n",
    "        net.train()\n",
    "\n",
    "metrics = dict(metrics)\n",
    "# save metrics \n",
    "import json \n",
    "with open(f\"{conf.exp_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.aux_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
