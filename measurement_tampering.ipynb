{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/nas/ucb/oliveradk/diverse-gen\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from losses.divdis import DivDisLoss \n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.conf import ConfLoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.smooth_top_loss import SmoothTopLoss\n",
    "from losses.pass_through import PassThroughLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from models.backbone import MultiHeadBackbone\n",
    "from utils.utils import batch_size, to_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "@dataclass \n",
    "class Config: \n",
    "    seed: int = 42\n",
    "    # loss\n",
    "    loss_type: LossType = LossType.TOPK\n",
    "    one_sided_ace: bool = True\n",
    "    ace_agree: bool = False\n",
    "    pseudo_label_all_groups: bool = False\n",
    "    source_weight: float = 1.0\n",
    "    aux_weight: float = 1.0\n",
    "    mix_rate_lower_bound: float = 0.1\n",
    "    mix_rate_schedule: Optional[str] = None\n",
    "    mix_rate_t0: Optional[int] = None\n",
    "    mix_rate_t1: Optional[int] = None\n",
    "    # model\n",
    "    model: str = \"codegen-350M-mono-measurement_pred-diamonds-seed0\"#\"pythia-1_4b-deduped-measurement_pred-generated_stories\"\n",
    "    binary: bool = True\n",
    "    heads: int = 2\n",
    "    train: bool = True\n",
    "    freeze_model: bool = False\n",
    "    load_prior_probe: bool = False\n",
    "    # data\n",
    "    dataset: str = \"diamonds-seed0\" #\"generated_stories\"\n",
    "    max_length: int = 1024\n",
    "    feature_dim: int = 1024\n",
    "    dataset_len: Optional[int] = None\n",
    "    split_source_target: bool = True\n",
    "    target_only_disagree: bool = False\n",
    "    source_labels: Optional[list[str| None]] = None # field(default_factory=lambda: [\"sensors_agree\"])\n",
    "    target_labels: Optional[list[str| None]] = None # field(default_factory=lambda: [\"sensors_agree\"])\n",
    "    # training\n",
    "    lr: float = 2e-5 \n",
    "    weight_decay: float = 2e-2\n",
    "    epochs: int = 5\n",
    "    scheduler: str = \"cosine\"\n",
    "    frac_warmup: float = 0.10\n",
    "    num_epochs: int = 5\n",
    "    effective_batch_size: int = 32\n",
    "    forward_batch_size: int = 32\n",
    "    micro_batch_size: int = 4\n",
    "    # misc\n",
    "    bootstrap_eval: bool = True\n",
    "    n_bootstrap_samples: int = 100\n",
    "    num_workers: int = 1\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    exp_dir: str = f\"output/mtd/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "def post_init(conf, overrride_keys):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topk_tamper_conf = {\n",
    "#     \"loss_type\": LossType.TOPK,\n",
    "#     \"one_sided_ace\": True,\n",
    "#     \"ace_agree\": True,\n",
    "#     \"target_only_disagree\": True,\n",
    "#     \"split_source_target\": True,\n",
    "#     \"pseudo_label_all_groups\": True,\n",
    "#     \"source_labels\": [None, \"sensors_agree\"],\n",
    "#     \"target_labels\": [None, \"sensors_agree\"], \n",
    "#     \"dataset_len\": 128\n",
    "# }\n",
    "# conf_dict = OmegaConf.merge(OmegaConf.structured(conf), topk_tamper_conf)\n",
    "# conf = Config(**conf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topk_tamper_conf = {\n",
    "#     \"loss_type\": LossType.TOPK,\n",
    "#     \"one_sided_ace\": True,\n",
    "#     \"ace_agree\": True,\n",
    "#     \"target_only_disagree\": True,\n",
    "#     \"split_source_target\": True,\n",
    "#     \"pseudo_label_all_groups\": True,\n",
    "#     \"source_labels\": [None, \"sensors_agree\"],\n",
    "#     \"target_labels\": [None, \"sensors_agree\"]\n",
    "# }\n",
    "# conf_dict = OmegaConf.merge(OmegaConf.structured(conf), topk_tamper_conf)\n",
    "# conf = Config(**conf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tamper_conf = {\n",
    "#     \"loss_type\": LossType.ERM,\n",
    "#     \"target_only_disagree\": True,\n",
    "#     \"split_source_target\": False,\n",
    "#     \"source_labels\": [\"sensors_agree\"],\n",
    "#     \"target_labels\": [\"sensors_agree\"], \n",
    "#     \"heads\": 1\n",
    "# }\n",
    "# conf_dict = OmegaConf.merge(OmegaConf.structured(conf), tamper_conf)\n",
    "# conf = Config(**conf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_stories_conf_update = {\n",
    "#     \"model\": \"pythia-1_4b-deduped-measurement_pred-generated_stories\",\n",
    "#     \"dataset\": \"generated_stories\",\n",
    "#     \"max_length\": 1536,\n",
    "#     \"feature_dim\": 2048,\n",
    "# }\n",
    "# conf_dict = OmegaConf.merge(OmegaConf.structured(conf), gen_stories_conf_update)\n",
    "# conf = Config(**conf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrride_keys = []\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    overrides = OmegaConf.from_cli(sys.argv[1:])\n",
    "    print(\"overrides\", overrides)\n",
    "    overrride_keys = overrides.keys()\n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), overrides)\n",
    "    conf = Config(**conf_dict)\n",
    "post_init(conf, overrride_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = conf.exp_dir\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "# save full config to exp_dir\n",
    "with open(f\"{exp_dir}/config.yaml\", \"w\") as f:\n",
    "    OmegaConf.save(config=conf, f=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"oliverdk/{conf.model}\"\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    trust_remote_code=True, \n",
    "    device_map=conf.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True, \n",
    "    padding_side=\"left\", \n",
    "    truncation_side=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pad token and init sensor loc finder\n",
    "pretrained_model.set_pad_token(tokenizer)\n",
    "pretrained_model.init_sensor_loc_finder(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(f\"redwoodresearch/{conf.dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeasurementDataset(Dataset):\n",
    "    def __init__(self, dataset, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.measurements = torch.tensor(self.dataset['measurements'])\n",
    "        self.ground_truth = torch.tensor(self.dataset['is_correct'])\n",
    "        self.is_trusted = torch.tensor(self.dataset['is_clean'])\n",
    "\n",
    "        self.encodings = tokenizer(\n",
    "            self.dataset['text'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = {\n",
    "            \"input_ids\": self.encodings['input_ids'][idx],\n",
    "            \"attention_mask\": self.encodings['attention_mask'][idx], \n",
    "        }\n",
    "\n",
    "        # labels\n",
    "        gt = self.ground_truth[idx]\n",
    "        all_sensors = torch.all(self.measurements[idx])\n",
    "        sensors_agree = torch.all(self.measurements[idx] == self.measurements[idx][0]) \n",
    "        group_labels = torch.stack((gt, all_sensors, sensors_agree))\n",
    "\n",
    "        # set labels to floats \n",
    "        label = gt.to(torch.float32)\n",
    "        group_labels = group_labels.to(torch.float32)\n",
    "        \n",
    "        return encoding, label, group_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.dataset_len is not None:\n",
    "    for k, subset in dataset.items():\n",
    "        # select random inidices \n",
    "        dataset[k] = subset.select(indices=np.random.choice(len(subset), min(conf.dataset_len, len(subset)), replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_same(ls):\n",
    "    return all(x == ls[0] for x in ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source (is clean)\n",
    "val_frac = 0.2\n",
    "\n",
    "if conf.target_only_disagree and not conf.split_source_target:\n",
    "    # only clean examples or examples where sensors disagree filter out examples where not clean and sensors agree \n",
    "    dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x[\"is_clean\"] or not all_same(x['measurements']))\n",
    "\n",
    "if conf.split_source_target: # standard split for diverse gen methods\n",
    "    source_data = dataset[\"train\"].filter(lambda x: x[\"is_clean\"])\n",
    "    splits = source_data.train_test_split(train_size=1-val_frac, test_size=val_frac, seed=conf.seed)\n",
    "    dataset[\"source_train\"] = splits['train']\n",
    "    dataset[\"source_val\"] = splits['test']\n",
    "\n",
    "    # target (is not clean)\n",
    "    target_data = dataset[\"train\"].filter(lambda x: not x[\"is_clean\"])\n",
    "    if \"train_for_val\" in dataset:\n",
    "        dataset[\"target_train\"] = target_data\n",
    "        dataset[\"target_val\"] = dataset[\"train_for_val\"]\n",
    "    else:\n",
    "        target_splits = target_data.train_test_split(train_size=1-val_frac, test_size=val_frac, seed=conf.seed)\n",
    "        dataset[\"target_train\"] = target_splits['train']\n",
    "        dataset[\"target_val\"] = target_splits['test']\n",
    "else: \n",
    "    # TODO: should use train for val if present\n",
    "    # combine source and target (trusted and untrusted) \n",
    "    # uses source labels, but None defaults to all sensors\n",
    "    splits = dataset[\"train\"].train_test_split(train_size=1-val_frac, test_size=val_frac, seed=conf.seed)\n",
    "    dataset[\"source_train\"] = splits['train']\n",
    "    dataset[\"source_val\"] = splits['test']\n",
    "\n",
    "# test (validation)\n",
    "dataset[\"test\"] = dataset[\"validation\"]\n",
    "# only untrusted positive examples \n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: not x['is_clean'] and all(x[\"measurements\"]))\n",
    "\n",
    "# remove train and train_for_val\n",
    "dataset.pop(\"train\")\n",
    "if \"train_for_val\" in dataset:\n",
    "    dataset.pop(\"train_for_val\")\n",
    "dataset.pop(\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tampering(x):\n",
    "    return not x[\"is_correct\"] and any(x[\"measurements\"])\n",
    "\n",
    "def fake_positive(x):\n",
    "    return not x[\"is_correct\"] and all(x[\"measurements\"])\n",
    "\n",
    "def split_tampering_rate(dataset):\n",
    "    tampering_rate = len(dataset.filter(is_tampering)) / len(dataset)\n",
    "    return tampering_rate\n",
    "def split_fake_positive_rate(dataset):\n",
    "    fake_positive_rate = len(dataset.filter(fake_positive)) / len(dataset)\n",
    "    return fake_positive_rate\n",
    "source_train_tampering_rate = split_tampering_rate(dataset[\"source_train\"])\n",
    "source_val_tampering_rate = split_tampering_rate(dataset[\"source_val\"])\n",
    "if conf.split_source_target:\n",
    "    target_train_tampering_rate = split_tampering_rate(dataset[\"target_train\"])\n",
    "    target_val_tampering_rate = split_tampering_rate(dataset[\"target_val\"])\n",
    "test_tampering_rate = split_tampering_rate(dataset[\"test\"])   \n",
    "\n",
    "source_train_fake_positive_rate = split_fake_positive_rate(dataset[\"source_train\"])\n",
    "source_val_fake_positive_rate = split_fake_positive_rate(dataset[\"source_val\"])\n",
    "if conf.split_source_target:\n",
    "    target_train_fake_positive_rate = split_fake_positive_rate(dataset[\"target_train\"])\n",
    "    target_val_fake_positive_rate = split_fake_positive_rate(dataset[\"target_val\"])\n",
    "test_fake_positive_rate = split_fake_positive_rate(dataset[\"test\"])\n",
    "\n",
    "if is_notebook():\n",
    "    print(f\"source train: tampering {source_train_tampering_rate:.2f}, fake positive {source_train_fake_positive_rate:.2f}\")\n",
    "    print(f\"source val: tampering {source_val_tampering_rate:.2f}, fake positive {source_val_fake_positive_rate:.2f}\")\n",
    "    if conf.split_source_target:\n",
    "        print(f\"target train: tampering {target_train_tampering_rate:.2f}, fake positive {target_train_fake_positive_rate:.2f}\")\n",
    "        print(f\"target val: tampering {target_val_tampering_rate:.2f}, fake positive {target_val_fake_positive_rate:.2f}\")\n",
    "    print(f\"test: tampering {test_tampering_rate:.2f}, fake positive {test_fake_positive_rate:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_ds = MeasurementDataset(dataset[\"source_train\"], conf.max_length)\n",
    "source_val_ds = MeasurementDataset(dataset[\"source_val\"], conf.max_length)\n",
    "if conf.split_source_target:\n",
    "    target_train_ds = MeasurementDataset(dataset[\"target_train\"], conf.max_length)\n",
    "    target_val_ds = MeasurementDataset(dataset[\"target_val\"], conf.max_length)\n",
    "test_ds = MeasurementDataset(dataset[\"test\"], conf.max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeasurementPredBackbone(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.pretrained_model.base_model(x['input_ids'], attention_mask=x['attention_mask'])\n",
    "        sensor_locs = self.pretrained_model.find_sensor_locs(x['input_ids'])\n",
    "        sensor_embs = out.last_hidden_state.gather(\n",
    "            1, sensor_locs.unsqueeze(-1).expand(-1, -1, out.last_hidden_state.size(-1))\n",
    "        )\n",
    "        assert sensor_embs.shape == (x['input_ids'].size(0), 4, out.last_hidden_state.size(-1))\n",
    "        aggregate_sensor_embs = sensor_embs[:, -1, :].squeeze(1)\n",
    "        assert aggregate_sensor_embs.shape == (x['input_ids'].size(0), out.last_hidden_state.size(-1))\n",
    "        return aggregate_sensor_embs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = MeasurementPredBackbone(pretrained_model).to(conf.device)\n",
    "net = MultiHeadBackbone(pred_model, classes=[1 for _ in range(conf.heads)], feature_dim=conf.feature_dim).to(conf.device)\n",
    "\n",
    "if conf.freeze_model:\n",
    "    for param in net.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# load weights of pretrained model aggregate probe to second net head\n",
    "if conf.load_prior_probe: # last head \n",
    "    net.heads.weight.data[-1, :] = pretrained_model.aggregate_probe.weight.data[0]\n",
    "    net.heads.bias.data[-1] = pretrained_model.aggregate_probe.bias.data[0]\n",
    "\n",
    "source_train_loader = DataLoader(source_train_ds, batch_size=conf.micro_batch_size, num_workers=conf.num_workers)\n",
    "source_val_loader = DataLoader(source_val_ds, batch_size=conf.micro_batch_size, num_workers=conf.num_workers)\n",
    "if conf.split_source_target:\n",
    "    target_train_loader = DataLoader(target_train_ds, batch_size=conf.effective_batch_size, num_workers=conf.num_workers)\n",
    "    target_val_loader = DataLoader(target_val_ds, batch_size=conf.effective_batch_size, num_workers=conf.num_workers)\n",
    "target_test_loader = DataLoader(test_ds, batch_size=conf.forward_batch_size, num_workers=conf.num_workers)\n",
    "\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "num_training_steps = conf.num_epochs * len(source_train_loader) // (conf.effective_batch_size // conf.micro_batch_size)\n",
    "scheduler = get_scheduler(\n",
    "    name=conf.scheduler,\n",
    "    optimizer=opt,\n",
    "    num_warmup_steps=round(conf.frac_warmup * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "def get_mix_rate(conf, mix_rate_lb_override: Optional[float] = None):\n",
    "    mix_rate_lb = mix_rate_lb_override if mix_rate_lb_override is not None else conf.mix_rate_lower_bound\n",
    "    if conf.one_sided_ace:\n",
    "        group_mix_rates = {(0,0) if conf.ace_agree else (0,1): mix_rate_lb}\n",
    "        mix_rate = None\n",
    "    else:\n",
    "        mix_rate = mix_rate_lb\n",
    "        group_mix_rates = None\n",
    "    return mix_rate, group_mix_rates\n",
    "\n",
    "\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=2)\n",
    "elif conf.loss_type == LossType.ERM:\n",
    "    loss_fn = PassThroughLoss()\n",
    "elif conf.loss_type == LossType.TOPK:\n",
    "    mix_rate, group_mix_rates = get_mix_rate(conf)\n",
    "    loss_fn = ACELoss(\n",
    "        classes_per_head=[1 for _ in range(conf.heads)], \n",
    "        mode=\"topk\", \n",
    "        group_mix_rates=group_mix_rates,  # TODO: should ignore visible labels\n",
    "        mix_rate=mix_rate,\n",
    "        device=conf.device\n",
    "    )\n",
    "\n",
    "val_loss_fn = loss_fn\n",
    "if conf.loss_type == LossType.TOPK and conf.mix_rate_schedule is not None:\n",
    "    val_loss_fn = copy.deepcopy(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal\n",
    "def compute_src_losses(\n",
    "    logits, y, gl, binary,\n",
    "    source_labels: Optional[list[Literal[\"all_sensors\", \"sensors_agree\", None]]] = None\n",
    "):\n",
    "    logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "    head_labels = []\n",
    "    for i in range(conf.heads):\n",
    "        if source_labels is None or source_labels[i] is None:\n",
    "            head_labels.append(gl[:, 1]) # same as y labels on source, but different on target\n",
    "        elif source_labels[i] == \"all_sensors\":\n",
    "            head_labels.append(gl[:, 1])\n",
    "        elif source_labels[i] == \"sensors_agree\":\n",
    "            head_labels.append(gl[:, 2])\n",
    "    labels = torch.cat(head_labels, dim=-1)\n",
    "    labels_chunked = torch.chunk(labels, conf.heads, dim=-1)\n",
    "    if binary:\n",
    "        losses = [F.binary_cross_entropy_with_logits(logit.view(-1), label.view(-1).to(torch.float32)) for logit, label in zip(logits_chunked, labels_chunked)]\n",
    "    else:\n",
    "        losses = [F.cross_entropy(logit.view(-1), label.view(-1).to(torch.long)) for logit, label in zip(logits_chunked, labels_chunked)]\n",
    "    return losses\n",
    "\n",
    "def compute_corrects(logits: torch.Tensor, head: int, y: torch.Tensor, binary: bool):\n",
    "    if binary:\n",
    "        preds = (logits[:, head] > 0).to(torch.float32)\n",
    "        assert preds.shape == (logits.size(0), ), f\"preds shape {preds.shape}, logits shape {logits.shape}\"\n",
    "        return ((preds == y.flatten()).sum().item())\n",
    "    else:\n",
    "        logits = logits.view(logits.size(0), conf.heads, -1)\n",
    "        return (logits[:, head].argmax(dim=-1) == y).sum().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change diciotary values to source loss, target loss\n",
    "\n",
    "classes = 2\n",
    "alt_index = 1\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# metrics\n",
    "metrics = defaultdict(list)\n",
    "writer = SummaryWriter(log_dir=conf.exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_batch(batch, slice):\n",
    "    if isinstance(batch, torch.Tensor):\n",
    "        return batch[slice]\n",
    "    elif isinstance(batch, dict):\n",
    "        return {k: v[slice] for k, v in batch.items()}\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported batch type: {type(batch)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_slice(idx, slice):\n",
    "    return idx >= slice.start and idx < slice.stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_labeled_target_loss(\n",
    "        logits, gl, target_labels: list[Literal[\"all_sensors\", \"sensors_agree\", None]]\n",
    "    ):    \n",
    "    # visible logits chunked\n",
    "    logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "\n",
    "    losses = []\n",
    "    for i in range(conf.heads):\n",
    "        if target_labels[i] is None: # no label for this head\n",
    "            continue\n",
    "        if target_labels[i] == \"all_sensors\":\n",
    "            y_i = gl[:, 1]\n",
    "        elif target_labels[i] == \"sensors_agree\":\n",
    "            y_i = gl[:, 2]\n",
    "        losses.append(F.binary_cross_entropy_with_logits(logits_chunked[i].squeeze(), y_i.squeeze().to(torch.float32)))\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    return sum(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_loss(\n",
    "        logits, y, gl, loss_fn, loss_type, \n",
    "        target_labels: Optional[list[Literal[\"all_sensors\", \"sensors_agree\", None]]] = None, \n",
    "        only_disagreeing_labels: bool = False\n",
    "    ): \n",
    "    # separate instances based on whether they have disagreeing measurements (i.e. gl[:, 2] == 0)\n",
    "    div_logits = logits \n",
    "    labeled_logits = logits \n",
    "    labeled_gl = gl \n",
    "    div_loss_kwargs = {}\n",
    "    if loss_type == LossType.TOPK:\n",
    "        div_loss_kwargs[\"virtual_bs\"] = logits.shape[0]\n",
    "    \n",
    "    if only_disagreeing_labels:\n",
    "        disagreeing_mask = gl[:, 2] == 0\n",
    "        div_logits = logits[~disagreeing_mask]\n",
    "        labeled_logits = logits[disagreeing_mask]\n",
    "        labeled_gl = gl[disagreeing_mask]\n",
    "\n",
    "    div_loss = loss_fn(div_logits, **div_loss_kwargs)\n",
    "    labeled_loss = torch.tensor(0.0)    \n",
    "    if target_labels is not None:\n",
    "        labeled_loss = compute_labeled_target_loss(labeled_logits, labeled_gl, target_labels)\n",
    "    return div_loss, labeled_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't worry about acc boostrapping \n",
    "# only bootstrap auroc \n",
    "# always return auroc's as list, so we can compute mean and std (even if length 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, loader, conf, bootstrap=False, n_bootstrap_samples=100, bootstrap_seed=0, fraction=0.5): \n",
    "    net.eval()\n",
    "\n",
    "    total_correct = torch.zeros(conf.heads)\n",
    "    total_correct_groups = {\n",
    "        \"all_sensors\": torch.zeros(conf.heads),\n",
    "        \"sensors_agree\": torch.zeros(conf.heads)\n",
    "    }\n",
    "    total_samples = 0\n",
    "    all_preds = [[] for _ in range(conf.heads)]\n",
    "    all_labels = []\n",
    "    all_group_labels = {\n",
    "        \"all_sensors\": [],\n",
    "        \"sensors_agree\": []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in tqdm(loader, desc=\"Target test\"):\n",
    "            test_x, test_y, test_gl = to_device(*test_batch, conf.device)\n",
    "            test_logits = net(test_x)\n",
    "            assert test_logits.shape == (batch_size(test_x), conf.heads * (1 if conf.binary else classes))\n",
    "            total_samples += test_y.size(0)\n",
    "\n",
    "            # Store labels for AUROC\n",
    "            all_labels.extend(test_y.cpu().numpy())\n",
    "            all_group_labels[\"all_sensors\"].extend(test_gl[:, 1].cpu().numpy())\n",
    "            all_group_labels[\"sensors_agree\"].extend(test_gl[:, 2].cpu().numpy())\n",
    "            \n",
    "            for i in range(conf.heads):\n",
    "                total_correct[i] += compute_corrects(test_logits, i, test_y, conf.binary)\n",
    "                total_correct_groups[\"all_sensors\"][i] += compute_corrects(test_logits, i, test_gl[:, 1], conf.binary)\n",
    "                total_correct_groups[\"sensors_agree\"][i] += compute_corrects(test_logits, i, test_gl[:, 2], conf.binary)\n",
    "                probs = torch.sigmoid(test_logits[:, i]).cpu().numpy()\n",
    "                all_preds[i].extend(probs)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = [np.array(preds) for preds in all_preds]\n",
    "    all_group_labels = {k: np.array(v) for k, v in all_group_labels.items()}\n",
    "\n",
    "    # Compute point estimates for accuracies\n",
    "    head_accs = [(total_correct[i] / total_samples).item() for i in range(conf.heads)]\n",
    "    head_accs_groups = {\n",
    "        group: [(total_correct_groups[group][i] / total_samples).item() for i in range(conf.heads)]\n",
    "        for group in [\"all_sensors\", \"sensors_agree\"]\n",
    "    }\n",
    "\n",
    "    # Compute AUROC (with or without bootstrapping)\n",
    "    if not bootstrap:\n",
    "        # Single AUROC computation per head, wrapped in a list\n",
    "        head_aurocs = [[roc_auc_score(all_labels, preds)] for preds in all_preds]\n",
    "    else:\n",
    "        # Bootstrap AUROC computation\n",
    "        head_aurocs = [[] for _ in range(conf.heads)]\n",
    "        np_gen = np.random.default_rng(bootstrap_seed)\n",
    "\n",
    "        correct_indices = np.where(all_labels == 1)[0]\n",
    "        incorrect_indices = np.where(all_labels == 0)[0]\n",
    "        correct_preds = [\n",
    "            all_preds[i][correct_indices] for i in range(conf.heads)\n",
    "        ]\n",
    "        incorrect_preds = [\n",
    "            all_preds[i][incorrect_indices] for i in range(conf.heads)\n",
    "        ]\n",
    "        \n",
    "        for _ in range(n_bootstrap_samples):\n",
    "            # split into corrects vs incorrects (based on y label)\n",
    "            for i in range(conf.heads):\n",
    "                correct_preds_sampled = np_gen.choice(correct_preds[i], size=round(len(correct_preds[i]) * fraction))\n",
    "                incorrect_preds_sampled = np_gen.choice(incorrect_preds[i], size=round(len(incorrect_preds[i]) * fraction))\n",
    "                gt = np.concatenate([np.ones_like(correct_preds_sampled), np.zeros_like(incorrect_preds_sampled)])\n",
    "                scores = np.concatenate([correct_preds_sampled, incorrect_preds_sampled])\n",
    "                try:\n",
    "                    bootstrap_auroc = roc_auc_score(gt, scores)\n",
    "                except ValueError:\n",
    "                    bootstrap_auroc = np.nan\n",
    "                head_aurocs[i].append(bootstrap_auroc)\n",
    "    \n",
    "    return head_accs, head_accs_groups, head_aurocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not conf.train:\n",
    "    metrics = {}\n",
    "    head_accs, head_accs_groups, head_aurocs = eval(\n",
    "        net, target_test_loader, conf, bootstrap=conf.bootstrap_eval, \n",
    "        n_bootstrap_samples=conf.n_bootstrap_samples, bootstrap_seed=conf.seed, fraction=0.5\n",
    "    )\n",
    "\n",
    "    # Initialize metrics lists\n",
    "    test_groups = [\"all_sensors\"]\n",
    "    for i in range(conf.heads):\n",
    "        # acc\n",
    "        metrics[f\"epoch_test_acc_{i}\"] = [head_accs[i]]\n",
    "        for group in test_groups:\n",
    "            metrics[f\"epoch_test_acc_{i}_{group}\"] = [head_accs_groups[group][i]]\n",
    "        # auroc\n",
    "        metrics[f\"epoch_test_auroc_{i}\"] = [np.array(head_aurocs[i]).mean()]\n",
    "        metrics[f\"epoch_test_auroc_{i}_std\"] = [np.array(head_aurocs[i]).std()]\n",
    "\n",
    "    # Save metrics\n",
    "    import json\n",
    "    with open(f\"{conf.exp_dir}/metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    for k, v in metrics.items():\n",
    "        print(k, v)\n",
    "    raise ValueError(\"Stop run all (not an actual error)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_target(conf: Config):\n",
    "    return conf.split_source_target and (conf.aux_weight > 0 or conf.target_labels is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader with effective batch size, then iterate over micro batches within batch \n",
    "if conf.split_source_target:\n",
    "    target_iter = iter(target_train_loader)\n",
    "    target_batch = None\n",
    "    target_logits = None\n",
    "\n",
    "for epoch in range(conf.epochs):\n",
    "    target_logit_ls = []\n",
    "    source_batch_loss = 0\n",
    "    source_batch_corrects = {i: 0 for i in range(conf.heads)}\n",
    "    target_batch_corrects = {(i, label): 0 for i in range(conf.heads) for label in [\"y\", \"all_sensors\", \"sensors_agree\"]}\n",
    "    for batch_idx, (x, y, gl) in tqdm(enumerate(source_train_loader), desc=\"Train\", total=len(source_train_loader)):\n",
    "\n",
    "        # update mix rate \n",
    "        if conf.loss_type == LossType.TOPK and conf.mix_rate_schedule is not None:\n",
    "            if epoch >= conf.mix_rate_t1: \n",
    "                mix_rate = conf.mix_rate_lower_bound\n",
    "            elif epoch < conf.mix_rate_t0:\n",
    "                mix_rate = 0\n",
    "            else:\n",
    "                mix_rate = conf.mix_rate_lower_bound * ((epoch - conf.mix_rate_t0) / (conf.mix_rate_t1 - conf.mix_rate_t0))\n",
    "            print(f\"Epoch {epoch} mix rate: {mix_rate}\")\n",
    "            _, group_mix_rates = get_mix_rate(conf, mix_rate_lb_override=mix_rate)\n",
    "            loss_fn.group_mix_rates = group_mix_rates\n",
    "\n",
    "        # compute source logits with micro batch \n",
    "        x, y, gl = to_device(x, y, gl, conf.device)\n",
    "        logits = net(x)\n",
    "        losses = compute_src_losses(logits, y, gl, conf.binary, conf.source_labels)\n",
    "        xent = sum(losses)\n",
    "        source_batch_loss += xent.item()\n",
    "\n",
    "        # computer source acc \n",
    "        for i in range(conf.heads):\n",
    "            source_batch_corrects[i] += compute_corrects(logits, i, y, conf.binary)\n",
    "        # compute target logits with no grad on forward batch \n",
    "        div_loss = torch.tensor(0.0)\n",
    "        labeled_target_loss = torch.tensor(0.0)\n",
    "        if train_target(conf):\n",
    "            if batch_idx % (conf.effective_batch_size // conf.micro_batch_size) == 0:\n",
    "                target_logits_ls = []\n",
    "                try: \n",
    "                    target_batch = next(target_iter)\n",
    "                    if target_batch[1].shape[0] != conf.effective_batch_size:\n",
    "                        raise StopIteration\n",
    "                except StopIteration:\n",
    "                    target_iter = iter(target_train_loader)\n",
    "                    target_batch = next(target_iter)\n",
    "                target_batch, target_y, target_gl = to_device(*target_batch, conf.device)\n",
    "                with torch.no_grad():\n",
    "                    target_logits_ls.append(net(target_batch).detach())\n",
    "                target_logits = torch.cat(target_logits_ls, dim=0)\n",
    "            # compute target logits with grad on micro batch\n",
    "            micro_batch_idx = batch_idx % (conf.effective_batch_size // conf.micro_batch_size)\n",
    "            micro_slice = slice(micro_batch_idx * conf.micro_batch_size, (micro_batch_idx + 1) * conf.micro_batch_size)\n",
    "            target_micro_batch = slice_batch(target_batch, micro_slice)\n",
    "            target_micro_logits = net(target_micro_batch)\n",
    "\n",
    "            cloned_target_logits= target_logits.clone().requires_grad_(True)\n",
    "            new_target_logits = torch.cat([\n",
    "                cloned_target_logits[i].unsqueeze(0) if \n",
    "                not in_slice(i, micro_slice) else target_micro_logits[i - micro_slice.start].unsqueeze(0)\n",
    "                for i in range(len(cloned_target_logits))\n",
    "            ])\n",
    "\n",
    "            div_loss, labeled_target_loss = compute_target_loss(\n",
    "                new_target_logits, target_y, target_gl, loss_fn, conf.loss_type, conf.target_labels, \n",
    "                only_disagreeing_labels=conf.target_only_disagree\n",
    "            )\n",
    "\n",
    "        # full loss (on micro batch)\n",
    "        full_loss = conf.source_weight * xent + conf.aux_weight * div_loss + labeled_target_loss   \n",
    "        full_loss.backward() \n",
    "        \n",
    "        # update weights, clear gradients on effective batch\n",
    "        if (batch_idx + 1) % (conf.effective_batch_size // conf.micro_batch_size) == 0:\n",
    "            opt.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # compute target acc \n",
    "            if train_target(conf):\n",
    "                for i in range(conf.heads):\n",
    "                    target_batch_corrects[(i, \"y\")] += compute_corrects(new_target_logits, i, target_y, conf.binary) \n",
    "                    target_batch_corrects[(i, \"all_sensors\")] += compute_corrects(new_target_logits, i, target_gl[:, 1], conf.binary)\n",
    "                    target_batch_corrects[(i, \"sensors_agree\")] += compute_corrects(new_target_logits, i, target_gl[:, 2], conf.binary)\n",
    "\n",
    "            source_batch_loss = source_batch_loss / conf.effective_batch_size\n",
    "            # compute batch metrics \n",
    "            effective_batch_idx = batch_idx // (conf.effective_batch_size // conf.micro_batch_size)\n",
    "            effective_num_batches = len(source_train_loader) // (conf.effective_batch_size // conf.micro_batch_size)\n",
    "            writer.add_scalar(\"train/source_loss\", source_batch_loss, epoch * effective_num_batches + effective_batch_idx)\n",
    "            if conf.aux_weight > 0:\n",
    "                writer.add_scalar(\"train/div_loss\", div_loss.item(), epoch * effective_num_batches + effective_batch_idx)\n",
    "            if conf.target_labels is not None:\n",
    "                writer.add_scalar(\"train/labeled_target_loss\", labeled_target_loss.item(), epoch * effective_num_batches + effective_batch_idx)\n",
    "            writer.add_scalar(\"train/full_loss\", source_batch_loss + conf.aux_weight * div_loss.item() + labeled_target_loss.item(), epoch * effective_num_batches + effective_batch_idx)\n",
    "            \n",
    "            for i in range(conf.heads):\n",
    "                writer.add_scalar(f\"train/source_acc_{i}\", source_batch_corrects[i] / conf.effective_batch_size, epoch * effective_num_batches + effective_batch_idx)\n",
    "                if train_target(conf):\n",
    "                    for label in [\"y\", \"all_sensors\", \"sensors_agree\"]:\n",
    "                        writer.add_scalar(f\"train/target_acc_{i}_{label}\", target_batch_corrects[(i, label)] / conf.effective_batch_size, epoch * effective_num_batches + effective_batch_idx)\n",
    "            source_batch_loss = 0\n",
    "            source_batch_corrects = {i: 0 for i in range(conf.heads)}\n",
    "            target_batch_corrects = {(i, label): 0 for i in range(conf.heads) for label in [\"y\", \"all_sensors\", \"sensors_agree\"]}\n",
    "    \n",
    "    # validation and test\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        net.eval()\n",
    "        # compute xent on source validation set\n",
    "        xent_val = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(source_val_loader, desc=\"Source val\"):\n",
    "                x, y, gl = to_device(*batch, conf.device)\n",
    "                logits_val = net(x)\n",
    "                losses_val = compute_src_losses(logits_val, y, gl, conf.binary, conf.source_labels)\n",
    "                xent_val.append(sum(losses_val).item())\n",
    "        metrics[f\"val_source_xent\"].append(np.mean(xent_val))\n",
    "        writer.add_scalar(\"val/source_loss\", metrics[f\"val_source_xent\"][-1], epoch)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # compute div loss on target validation set (used for model selection)\n",
    "        if train_target(conf):\n",
    "            div_losses_val = []\n",
    "            labeled_target_losses_val = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(target_val_loader, desc=\"Target val\"):\n",
    "                    x, y, gl = to_device(*batch, conf.device)\n",
    "                    logits_val = net(x)\n",
    "                    div_loss, labeled_target_loss = compute_target_loss(\n",
    "                        logits_val, y, gl, val_loss_fn, conf.loss_type, conf.target_labels, \n",
    "                        only_disagreeing_labels=conf.target_only_disagree\n",
    "                    )\n",
    "                    div_losses_val.append(div_loss.item())\n",
    "                    labeled_target_losses_val.append(labeled_target_loss.item())\n",
    "            \n",
    "            metrics[f\"val_target_div_loss\"].append(np.mean(div_losses_val))\n",
    "            metrics[f\"val_target_labeled_loss\"].append(np.mean(labeled_target_losses_val))\n",
    "            metrics[f\"val_target_weighted_div_loss\"].append(np.mean(div_losses_val) * conf.aux_weight)\n",
    "            metrics[f\"val_target_loss\"].append(np.mean(div_losses_val) * conf.aux_weight + np.mean(labeled_target_losses_val))\n",
    "            \n",
    "            writer.add_scalar(\"val/div_loss\", metrics[f\"val_target_div_loss\"][-1], epoch)\n",
    "            writer.add_scalar(\"val/weighted_div_loss\", metrics[f\"val_target_weighted_div_loss\"][-1], epoch)\n",
    "            writer.add_scalar(\"val/labeled_target_loss\", metrics[f\"val_target_labeled_loss\"][-1], epoch)\n",
    "            writer.add_scalar(\"val/target_loss\", metrics[f\"val_target_loss\"][-1], epoch)\n",
    "\n",
    "        # total validation loss\n",
    "        val_loss = metrics[f\"val_source_xent\"][-1]\n",
    "        if train_target(conf):\n",
    "            val_loss += metrics[f\"val_target_loss\"][-1]\n",
    "        metrics[f\"val_loss\"].append(val_loss)  \n",
    "        writer.add_scalar(\"val/val_loss\", val_loss, epoch)\n",
    "       \n",
    "        \n",
    "        # test evaluation (acc, acc_alt, auroc)\n",
    "        head_accs, head_accs_groups, head_aurocs = eval(\n",
    "            net, target_test_loader, conf, bootstrap=conf.bootstrap_eval, \n",
    "            n_bootstrap_samples=conf.n_bootstrap_samples, bootstrap_seed=conf.seed\n",
    "        )\n",
    "        test_groups = [\"all_sensors\"]\n",
    "        for i in range(conf.heads):\n",
    "            # acc \n",
    "            metrics[f\"epoch_test_acc_{i}\"].append(head_accs[i])\n",
    "            for group in test_groups:\n",
    "                metrics[f\"epoch_test_acc_{i}_{group}\"].append(head_accs_groups[group][i])\n",
    "            writer.add_scalar(f\"val/test_acc_{i}\", head_accs[i], epoch)\n",
    "            for group in test_groups:\n",
    "                writer.add_scalar(f\"val/test_acc_{i}_{group}\", head_accs_groups[group][i], epoch)\n",
    "            # auroc\n",
    "            metrics[f\"epoch_test_auroc_{i}\"].append(np.array(head_aurocs[i]).mean())\n",
    "            metrics[f\"epoch_test_auroc_{i}_std\"].append(np.array(head_aurocs[i]).std())\n",
    "            writer.add_scalar(f\"val/test_auroc_{i}\", np.array(head_aurocs[i]).mean(), epoch)                \n",
    "        \n",
    "        # print validation losses and test accs\n",
    "        print(f\"Epoch {epoch + 1} Test Accuracies:\")\n",
    "        print(f\"Source val xent: {metrics[f'val_source_xent'][-1]:.4f}\")\n",
    "        if train_target(conf):\n",
    "            print(f\"Target val div loss: {metrics[f'val_target_div_loss'][-1]:.4f}\")\n",
    "            print(f\"Target val weighted div loss: {metrics[f'val_target_weighted_div_loss'][-1]:.4f}\")\n",
    "        print(f\"val loss: {metrics[f'val_loss'][-1]:.4f}\")\n",
    "        for i in range(conf.heads):\n",
    "            print(\n",
    "                f\"Head {i}: {metrics[f'epoch_test_acc_{i}'][-1]:.4f}\", \n",
    "                *[f\"{group}: {metrics[f'epoch_test_acc_{i}_{group}'][-1]:.4f}\" for group in test_groups]\n",
    "            )\n",
    "            print(f\"Head {i} auroc: {metrics[f'epoch_test_auroc_{i}'][-1]:.4f}\")\n",
    "        \n",
    "        net.train()\n",
    "\n",
    "metrics = dict(metrics)\n",
    "# save metrics \n",
    "import json \n",
    "with open(f\"{conf.exp_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
