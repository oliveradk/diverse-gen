{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random as rnd\n",
    "from typing import Optional, Callable\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as  pd\n",
    "import torchvision.utils as vision_utils\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from losses.divdis import DivDisLoss \n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from models.backbone import MultiHeadBackbone\n",
    "from models.multi_model import MultiNetModel\n",
    "from models.lenet import LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "@dataclass\n",
    "class Config():\n",
    "    seed: int = 45\n",
    "    loss_type: LossType = LossType.PROB\n",
    "    batch_size: int = 128\n",
    "    target_batch_size: int = 128\n",
    "    epochs: int = 20\n",
    "    heads: int = 2 \n",
    "    model: str = \"Resnet50\"\n",
    "    shared_backbone: bool = True\n",
    "    aux_weight: float = 1.0\n",
    "    mix_rate: Optional[float] = 0.5\n",
    "    l_01_mix_rate: Optional[float] = None # TODO: geneneralize\n",
    "    l_10_mix_rate: Optional[float] = None\n",
    "    gamma: Optional[float] = 1.0\n",
    "    mix_rate_lower_bound: Optional[float] = 0.5\n",
    "    inbalance_ratio: Optional[bool] = True\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    vertical: bool = True\n",
    "    device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "def post_init(conf: Config):\n",
    "    if conf.l_01_mix_rate is not None and conf.l_10_mix_rate is None:\n",
    "        conf.l_10_mix_rate = 0.0\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_01_mix_rate\n",
    "        assert conf.mix_rate == conf.l_01_mix_rate\n",
    "    elif conf.l_01_mix_rate is None and conf.l_10_mix_rate is not None:\n",
    "        conf.l_01_mix_rate = 0.0\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_10_mix_rate\n",
    "        assert conf.mix_rate == conf.l_10_mix_rate\n",
    "    elif conf.l_01_mix_rate is not None and conf.l_10_mix_rate is not None:\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_01_mix_rate + conf.l_10_mix_rate\n",
    "        assert conf.mix_rate == conf.l_01_mix_rate + conf.l_10_mix_rate\n",
    "    else: # both are none \n",
    "        assert conf.mix_rate is not None\n",
    "        conf.l_01_mix_rate = conf.mix_rate / 2\n",
    "        conf.l_10_mix_rate = conf.mix_rate / 2\n",
    "    \n",
    "    if conf.mix_rate_lower_bound is None:\n",
    "        conf.mix_rate_lower_bound = conf.mix_rate\n",
    "\n",
    "    if conf.loss_type == LossType.DIVDIS:\n",
    "        conf.aux_weight = 10.0\n",
    "    if conf.loss_type == LossType.EXP: \n",
    "        conf.target_batch_size = round(4 / conf.mix_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config \n",
    "conf = Config()\n",
    "#get config overrides if runnign from command line\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), OmegaConf.from_cli(sys.argv[1:]))\n",
    "    conf = Config(**conf_dict)\n",
    "post_init(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory from config\n",
    "def conf_to_dir_name(conf: Config):        \n",
    "    dir_name = f\"{conf.loss_type.name}_{conf.model}_{conf.mix_rate}_{conf.mix_rate_lower_bound}_{conf.epochs}_{conf.seed}\"\n",
    "    return dir_name\n",
    "cifar_mnist_dir_name = \"output/cifar_mnist\"\n",
    "dir_name = f\"{cifar_mnist_dir_name}/{conf_to_dir_name(conf)}\"\n",
    "datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "exp_dir = f\"{dir_name}/{datetime_str}\"\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "# save full config to exp_dir\n",
    "with open(f\"{exp_dir}/config.yaml\", \"w\") as f:\n",
    "    OmegaConf.save(config=conf, f=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(conf.seed)\n",
    "np.random.seed(conf.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transform = None\n",
    "if conf.model == \"Resnet50\":\n",
    "    from torchvision import models\n",
    "    from torchvision.models.resnet import ResNet50_Weights\n",
    "    resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    model_builder = lambda: torch.nn.Sequential(*list(resnet50.children())[:-1])\n",
    "    feature_dim = 2048\n",
    "elif conf.model == \"ClipViT\":\n",
    "    from transformers import CLIPVisionModel\n",
    "    model_builder = lambda: CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    feature_dim = 768\n",
    "    input_size = 96\n",
    "    model_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "    ])\n",
    "elif conf.model == \"LeNet\":\n",
    "    from models.lenet import LeNet\n",
    "    from functools import partial\n",
    "    model_builder = lambda: partial(LeNet, num_classes=1, dropout_p=0.0)\n",
    "    feature_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: F.pad(x.repeat(3, 1, 1), (2, 2, 2, 2), 'constant', 0))\n",
    "])\n",
    "\n",
    "\n",
    "# get full datasets \n",
    "mnist_train = torchvision.datasets.MNIST('./data/mnist/', train=True, download=True, transform=mnist_transform)\n",
    "cifar_train = torchvision.datasets.CIFAR10('./data/cifar10/', train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.MNIST('./data/mnist/', train=False, download=True, transform=mnist_transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10('./data/cifar10/', train=False, download=True, transform=transform)\n",
    "\n",
    "mnist_target, mnist_train, mnist_source_val, mnist_target_val = random_split(mnist_train, [10000, 45000, 2500, 2500], generator=torch.Generator().manual_seed(42))\n",
    "cifar_target, cifar_train, cifar_source_val, cifar_target_val = random_split(cifar_train, [10000, 35000, 2500, 2500], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that generates dataset of concatenated cifar and mnist images\n",
    "# 1-mix_rate is the number of cifar cars and 0's + cifar trucks and 1's (default)\n",
    "# 0_9 corresponds to cifar trucks and mnist 0's \n",
    "# 1_1 corresponds to cifar cars and mnist 1's \n",
    "def generate_dataset(mnist_data, cifar_data, mix_rate_0_9, mix_rate_1_1, vertical=False, transform=None):\n",
    "    # filter by labels\n",
    "    mnist_0 = [(img, label) for img, label in mnist_data if label == 0]\n",
    "    mnist_1 = [(img, label) for img, label in mnist_data if label == 1]\n",
    "    cifar_1 = [(img, label) for img, label in cifar_data if label == 1]\n",
    "    cifar_9 = [(img, label) for img, label in cifar_data if label == 9]\n",
    "    # get number of samples\n",
    "    num_samples = min(len(mnist_0), len(mnist_1), len(cifar_1), len(cifar_9))\n",
    "    data_pairs = []\n",
    "    num_clean = int(num_samples * (1-mix_rate_0_9 - mix_rate_1_1)) \n",
    "    num_mixed_0_9 = int(num_samples * mix_rate_0_9) \n",
    "    num_mixed_1_1 = int(num_samples * mix_rate_1_1) \n",
    "    i = 0\n",
    "    for _ in range(num_clean // 2):\n",
    "        # cars and 0's\n",
    "        data_pairs.append(((cifar_1[i][0], mnist_0[i][0]), 0, (0, 0))) \n",
    "        # trucks and 1's\n",
    "        data_pairs.append(((cifar_9[i][0], mnist_1[i][0]), 1, (1, 1)))\n",
    "        i+=1\n",
    "    for _ in range(num_mixed_0_9):\n",
    "        # trucks and 0's\n",
    "        data_pairs.append(((cifar_9[i][0], mnist_0[i][0]), 1, (1, 0)))\n",
    "        i+=1\n",
    "    for _ in range(num_mixed_1_1):\n",
    "        # cars and 1's\n",
    "        data_pairs.append(((cifar_1[i][0], mnist_1[i][0]), 0, (0, 1)))\n",
    "        i+=1\n",
    "    # construct dataset\n",
    "    images, labels, group_labels = zip(*data_pairs)\n",
    "    # concatenate images\n",
    "    cat_dim = 1 if vertical else 2\n",
    "    images = [torch.cat([mnist_img, cifar_img], dim=cat_dim) for cifar_img, mnist_img in images]\n",
    "    images = torch.stack(images)\n",
    "    # labels and group labels \n",
    "    labels = torch.tensor(labels).to(torch.float32)\n",
    "    group_labels = torch.tensor([list(gl) for gl in group_labels]).to(torch.float32)\n",
    "    # shuffle dataset\n",
    "    shuffle = torch.randperm(len(images))\n",
    "    images = images[shuffle]\n",
    "    if transform is not None:\n",
    "        images = transform(images)\n",
    "    labels = labels[shuffle]\n",
    "    group_labels = group_labels[shuffle]\n",
    "    dataset = TensorDataset(images, labels, group_labels)\n",
    "    return dataset\n",
    "\n",
    "# generate datasets\n",
    "source_train = generate_dataset(mnist_train, cifar_train, mix_rate_0_9=0.0, mix_rate_1_1=0.0, vertical=conf.vertical, transform=model_transform)\n",
    "source_val = generate_dataset(mnist_source_val, cifar_source_val, mix_rate_0_9=0.0, mix_rate_1_1=0.0, vertical=conf.vertical, transform=model_transform)\n",
    "target_train = generate_dataset(mnist_target, cifar_target, mix_rate_0_9=conf.l_01_mix_rate, mix_rate_1_1=conf.l_10_mix_rate, vertical=conf.vertical, transform=model_transform)\n",
    "target_val = generate_dataset(mnist_target_val, cifar_target_val, mix_rate_0_9=conf.l_01_mix_rate, mix_rate_1_1=conf.l_10_mix_rate, vertical=conf.vertical, transform=model_transform)\n",
    "target_test = generate_dataset(mnist_test, cifar_test, mix_rate_0_9=0.25, mix_rate_1_1=0.25, vertical=conf.vertical, transform=model_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([gl[0] == gl[1] for _, _, gl in target_test]) / len(target_test) == 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot source images with vision_utils.make_grid\n",
    "cifar_mnist_grid = torch.stack([source_train[i][0] for i in range(20)])\n",
    "grid_img = vision_utils.make_grid(cifar_mnist_grid, nrow=10, normalize=True, padding=1)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target train images with vision_utils.make_grid\n",
    "cifar_mnist_grid = torch.stack([target_train[i][0] for i in range(20)])\n",
    "grid_img = vision_utils.make_grid(cifar_mnist_grid, nrow=10, normalize=True, padding=1)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.shared_backbone:\n",
    "    net = MultiHeadBackbone(model_builder(), conf.heads, feature_dim)\n",
    "else:\n",
    "    net = MultiNetModel(heads=conf.heads, model_builder=model_builder)\n",
    "net = net.to(conf.device)\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=conf.heads)\n",
    "else:\n",
    "    loss_fn = ACELoss(\n",
    "        heads=conf.heads, \n",
    "        mode=conf.loss_type.value, \n",
    "        gamma=conf.gamma,\n",
    "        inbalance_ratio=conf.inbalance_ratio,\n",
    "        l_01_rate=conf.mix_rate_lower_bound / 2, \n",
    "        l_10_rate=conf.mix_rate_lower_bound / 2, \n",
    "        device=conf.device\n",
    "    )\n",
    "# data loaders \n",
    "source_train_loader = DataLoader(source_train, batch_size=conf.batch_size, shuffle=True)\n",
    "target_train_loader = DataLoader(target_train, batch_size=conf.target_batch_size, shuffle=True)\n",
    "target_val_loader = DataLoader(target_val, batch_size=conf.target_batch_size, shuffle=True)\n",
    "target_test_loader = DataLoader(target_test, batch_size=conf.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "target_iter = iter(target_train_loader)\n",
    "for epoch in range(conf.epochs):\n",
    "    for x, y, gl in tqdm(source_train_loader, desc=\"Source train\"):\n",
    "        x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "        logits = net(x)\n",
    "        logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "        losses = [F.binary_cross_entropy_with_logits(logit.squeeze(), y) for logit in logits_chunked]\n",
    "        xent = sum(losses)\n",
    "\n",
    "        try: \n",
    "            target_x, target_y, target_gl = next(target_iter)\n",
    "        except StopIteration:\n",
    "            target_iter = iter(target_train_loader)\n",
    "            target_x, target_y, target_gl = next(target_iter)\n",
    "        target_x, target_y, target_gl = target_x.to(conf.device), target_y.to(conf.device), target_gl.to(conf.device)\n",
    "        target_logits = net(target_x)\n",
    "\n",
    "        repulsion_loss_args = []\n",
    "        repulsion_loss = loss_fn(target_logits, *repulsion_loss_args)\n",
    "        full_loss = xent + conf.aux_weight * repulsion_loss\n",
    "        opt.zero_grad()\n",
    "        full_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        metrics[f\"xent\"].append(xent.item())\n",
    "        metrics[f\"repulsion_loss\"].append(repulsion_loss.item())\n",
    "    # Compute loss on target validation set (used for model selection)\n",
    "    # and aggregate metrics over the entire test set (should not really be using)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        net.eval()\n",
    "        # compute repulsion loss on target validation set (used for model selection)\n",
    "        repulsion_loss_val = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, gl in tqdm(target_val_loader, desc=\"Target val\"):\n",
    "                x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "                logits_val = net(x)\n",
    "                repulsion_loss_val.append(conf.aux_weight * loss_fn(logits_val, * repulsion_loss_args).item())\n",
    "        metrics[f\"target_val_repulsion_loss\"].append(np.mean(repulsion_loss_val))\n",
    "        # compute xent on source validation set\n",
    "        xent_val = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, gl in tqdm(source_train_loader, desc=\"Source val\"):\n",
    "                x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "                logits_val = net(x)\n",
    "                logits_chunked_val = torch.chunk(logits_val, conf.heads, dim=-1)\n",
    "                losses_val = [F.binary_cross_entropy_with_logits(logit.squeeze(), y) for logit in logits_chunked_val]\n",
    "                xent_val.append(sum(losses_val).item())\n",
    "        metrics[f\"source_val_xent\"].append(np.mean(xent_val))\n",
    "        metrics[f\"target_val_loss\"].append(np.mean(repulsion_loss_val) + np.mean(xent_val))\n",
    "\n",
    "        # compute accuracy over target test set (used to evaluate actual performance)\n",
    "        total_correct = torch.zeros(conf.heads)\n",
    "        total_correct_alt = torch.zeros(conf.heads)\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y, test_gl in target_test_loader:\n",
    "                test_x, test_y, test_gl = test_x.to(conf.device), test_y.to(conf.device), test_gl.to(conf.device)\n",
    "                test_logits = net(test_x).squeeze()\n",
    "                total_samples += test_y.size(0)\n",
    "                \n",
    "                for i in range(conf.heads):\n",
    "                    total_correct[i] += ((test_logits[:, i] > 0) == test_y.flatten()).sum().item()\n",
    "                    total_correct_alt[i] += ((test_logits[:, i] > 0) == test_gl[:, 1].flatten()).sum().item()\n",
    "        \n",
    "        for i in range(conf.heads):\n",
    "            metrics[f\"epoch_acc_{i}\"].append((total_correct[i] / total_samples).item())\n",
    "            metrics[f\"epoch_acc_{i}_alt\"].append((total_correct_alt[i] / total_samples).item())\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Test Accuracies:\")\n",
    "        # print validation losses\n",
    "        print(f\"Target val repulsion loss: {metrics[f'target_val_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Target val xent: {metrics[f'source_val_xent'][-1]:.4f}\")\n",
    "        print(f\"Target val loss: {metrics[f'target_val_loss'][-1]:.4f}\")\n",
    "        for i in range(conf.heads):\n",
    "            print(f\"Head {i}: {metrics[f'epoch_acc_{i}'][-1]:.4f}, Alt: {metrics[f'epoch_acc_{i}_alt'][-1]:.4f}\")\n",
    "        \n",
    "        net.train()\n",
    "\n",
    "metrics = dict(metrics)\n",
    "# save metrics \n",
    "import json \n",
    "with open(f\"{exp_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"xent\"], label=\"xent\", color=\"red\")\n",
    "plt.plot(metrics[\"repulsion_loss\"], label=\"repulsion_loss\", color=\"blue\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print loss\n",
    "# plt.plot(metrics[\"xent\"], label=\"xent\", color=\"pink\")\n",
    "# plt.plot(metrics[\"repulsion_loss\"], label=\"repulsion_loss\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"source_val_xent\"], label=\"source_val_xent\", color=\"red\")\n",
    "plt.plot(metrics[\"target_val_repulsion_loss\"], label=\"target_val_repulsion_loss\", color=\"blue\")\n",
    "plt.plot(metrics[\"target_val_loss\"], label=\"target_val_loss\", color=\"green\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics\n",
    "# plot acc_0 and acc_1 and acc_0_alt and acc_1_alt\n",
    "plt.plot(metrics[\"epoch_acc_0\"], label=\"acc_0\", color=\"blue\")\n",
    "plt.plot(metrics[\"epoch_acc_1\"], label=\"acc_1\", color=\"green\")\n",
    "plt.plot(metrics[\"epoch_acc_0_alt\"], label=\"acc_0_alt\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"epoch_acc_1_alt\"], label=\"acc_1_alt\", color=\"lightgreen\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-gen-KG5DY0Zz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
