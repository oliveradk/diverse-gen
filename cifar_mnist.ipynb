{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random as rnd\n",
    "from typing import Optional, Callable\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as  pd\n",
    "import torchvision.utils as vision_utils\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from losses.divdis import DivDisLoss \n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from models.backbone import MultiHeadBackbone\n",
    "from models.multi_model import MultiNetModel\n",
    "from models.lenet import LeNet\n",
    "\n",
    "from datasets.cifar_mnist import get_cifar_mnist_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add dbat \n",
    "# TODO: add other vision datasets \n",
    "# TODO: add language datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "@dataclass\n",
    "class Config():\n",
    "    seed: int = 45\n",
    "    loss_type: LossType = LossType.PROB\n",
    "    batch_size: int = 128\n",
    "    target_batch_size: int = 128\n",
    "    epochs: int = 100\n",
    "    heads: int = 2 \n",
    "    model: str = \"Resnet50\"\n",
    "    shared_backbone: bool = True\n",
    "    aux_weight: float = 1.0\n",
    "    mix_rate: Optional[float] = 0.5\n",
    "    l_01_mix_rate: Optional[float] = None # TODO: geneneralize\n",
    "    l_10_mix_rate: Optional[float] = None\n",
    "    gamma: Optional[float] = 1.0\n",
    "    mix_rate_lower_bound: Optional[float] = 0.5\n",
    "    inbalance_ratio: Optional[bool] = True\n",
    "    lr: float = 1e-3 # default for clipvit\n",
    "    weight_decay: float = 1e-5\n",
    "    lr_scheduler: Optional[str] = \"cosine\"# \"cosine\"\n",
    "    num_cycles: float = 0.5\n",
    "    frac_warmup: float = 0.05\n",
    "    vertical: bool = True\n",
    "    device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def post_init(conf: Config, overrides: list[str]=[]):\n",
    "    if conf.l_01_mix_rate is not None and conf.l_10_mix_rate is None:\n",
    "        conf.l_10_mix_rate = 0.0\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_01_mix_rate\n",
    "        assert conf.mix_rate == conf.l_01_mix_rate\n",
    "    elif conf.l_01_mix_rate is None and conf.l_10_mix_rate is not None:\n",
    "        conf.l_01_mix_rate = 0.0\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_10_mix_rate\n",
    "        assert conf.mix_rate == conf.l_10_mix_rate\n",
    "    elif conf.l_01_mix_rate is not None and conf.l_10_mix_rate is not None:\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_01_mix_rate + conf.l_10_mix_rate\n",
    "        assert conf.mix_rate == conf.l_01_mix_rate + conf.l_10_mix_rate\n",
    "    else: # both are none \n",
    "        assert conf.mix_rate is not None\n",
    "        conf.l_01_mix_rate = conf.mix_rate / 2\n",
    "        conf.l_10_mix_rate = conf.mix_rate / 2\n",
    "    \n",
    "    if conf.mix_rate_lower_bound is None:\n",
    "        conf.mix_rate_lower_bound = conf.mix_rate\n",
    "    \n",
    "    if conf.loss_type == LossType.DIVDIS and \"aux_weight\" not in overrides:\n",
    "        conf.aux_weight = 10.0\n",
    "    # if conf.loss_type == LossType.EXP and \"target_batch_size\" not in overrides: \n",
    "    #     conf.target_batch_size = round(4 / conf.mix_rate_lower_bound)\n",
    "    if conf.model == \"ClipViT\" and \"lr\" not in overrides:\n",
    "        conf.lr = 5e-5\n",
    "    if conf.model == \"ClipViT\" and \"lr_scheduler\" not in overrides:\n",
    "        conf.lr_scheduler = \"cosine\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config \n",
    "conf = Config()\n",
    "#get config overrides if runnign from command line\n",
    "overrride_keys = []\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    overrides = OmegaConf.from_cli(sys.argv[1:])\n",
    "    overrride_keys = overrides.keys()\n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), overrides)\n",
    "    conf = Config(**conf_dict)\n",
    "post_init(conf, overrride_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory from config\n",
    "def conf_to_dir_name(conf: Config):        \n",
    "    dir_name = f\"{conf.loss_type.name}_{conf.model}_{conf.mix_rate}_{conf.mix_rate_lower_bound}_{conf.lr}_{conf.aux_weight}_{conf.epochs}_{conf.seed}\"\n",
    "    return dir_name\n",
    "cifar_mnist_dir_name = \"output/cifar_mnist\"\n",
    "dir_name = f\"{cifar_mnist_dir_name}/{conf_to_dir_name(conf)}\"\n",
    "datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "exp_dir = f\"{dir_name}/{datetime_str}\"\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "# save full config to exp_dir\n",
    "with open(f\"{exp_dir}/config.yaml\", \"w\") as f:\n",
    "    OmegaConf.save(config=conf, f=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(conf.seed)\n",
    "np.random.seed(conf.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transform = None\n",
    "if conf.model == \"Resnet50\":\n",
    "    from torchvision import models\n",
    "    from torchvision.models.resnet import ResNet50_Weights\n",
    "    resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    model_builder = lambda: torch.nn.Sequential(*list(resnet50.children())[:-1])\n",
    "    resnet_50_transforms = ResNet50_Weights.DEFAULT.transforms()\n",
    "    model_transform = transforms.Compose([\n",
    "        # transforms.Resize(resnet_50_transforms.resize_size * 2, interpolation=resnet_50_transforms.interpolation),\n",
    "        # transforms.CenterCrop(resnet_50_transforms.crop_size),\n",
    "        transforms.Normalize(mean=resnet_50_transforms.mean, std=resnet_50_transforms.std)\n",
    "    ])\n",
    "    feature_dim = 2048\n",
    "elif conf.model == \"ClipViT\":\n",
    "    from models.clip_vit import ClipViT\n",
    "    model_builder = lambda: ClipViT()\n",
    "    feature_dim = 768\n",
    "    input_size = 96\n",
    "    model_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "    ])\n",
    "elif conf.model == \"LeNet\":\n",
    "    from models.lenet import LeNet\n",
    "    from functools import partial\n",
    "    model_builder = lambda: partial(LeNet, num_classes=1, dropout_p=0.0)\n",
    "    feature_dim = 256\n",
    "else: \n",
    "    raise ValueError(f\"Model {conf.model} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train, source_val, target_train, target_val, target_test = get_cifar_mnist_datasets(\n",
    "    vertical=conf.vertical, \n",
    "    mix_rate_0_9=conf.l_01_mix_rate, \n",
    "    mix_rate_1_1=conf.l_10_mix_rate, \n",
    "    transform=model_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([gl[0] == gl[1] for _, _, gl in target_test]) / len(target_test) == 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot source images with vision_utils.make_grid\n",
    "cifar_mnist_grid = torch.stack([source_train[i][0] for i in range(20)])\n",
    "grid_img = vision_utils.make_grid(cifar_mnist_grid, nrow=10, normalize=True, padding=1)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target train images with vision_utils.make_grid\n",
    "cifar_mnist_grid = torch.stack([target_train[i][0] for i in range(20)])\n",
    "grid_img = vision_utils.make_grid(cifar_mnist_grid, nrow=10, normalize=True, padding=1)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders \n",
    "source_train_loader = DataLoader(source_train, batch_size=conf.batch_size, shuffle=True)\n",
    "target_train_loader = DataLoader(target_train, batch_size=conf.target_batch_size, shuffle=True)\n",
    "target_val_loader = DataLoader(target_val, batch_size=conf.target_batch_size, shuffle=True)\n",
    "target_test_loader = DataLoader(target_test, batch_size=conf.batch_size, shuffle=True)\n",
    "\n",
    "# classifiers\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "if conf.shared_backbone:\n",
    "    net = MultiHeadBackbone(model_builder(), conf.heads, feature_dim)\n",
    "else:\n",
    "    net = MultiNetModel(heads=conf.heads, model_builder=model_builder)\n",
    "net = net.to(conf.device)\n",
    "\n",
    "# optimizer\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "num_steps = conf.epochs * len(source_train_loader)\n",
    "if conf.lr_scheduler == \"cosine\":       \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        opt, \n",
    "        num_warmup_steps=num_steps * conf.frac_warmup, \n",
    "        num_training_steps=num_steps, \n",
    "        num_cycles=conf.num_cycles\n",
    "    )\n",
    "else: \n",
    "    # constant learning rate\n",
    "    scheduler = None\n",
    "\n",
    "# loss function\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=conf.heads)\n",
    "else:\n",
    "    loss_fn = ACELoss(\n",
    "        heads=conf.heads, \n",
    "        mode=conf.loss_type.value, \n",
    "        gamma=conf.gamma,\n",
    "        inbalance_ratio=conf.inbalance_ratio,\n",
    "        l_01_rate=conf.mix_rate_lower_bound / 2, \n",
    "        l_10_rate=conf.mix_rate_lower_bound / 2, \n",
    "        device=conf.device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses.ace import compute_head_losses\n",
    "\n",
    "def get_orderings(logits: torch.Tensor):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    head_0_0, head_0_1, head_1_0, head_1_1 = compute_head_losses(probs)\n",
    "    loss_0_1 = head_0_0 + head_1_1\n",
    "    loss_1_0 = head_1_0 + head_0_1\n",
    "    loss_0_1, indices_0_1 = loss_0_1.sort()\n",
    "    loss_1_0, indices_1_0 = loss_1_0.sort()\n",
    "    return loss_0_1, loss_1_0, indices_0_1, indices_1_0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "target_iter = iter(target_train_loader)\n",
    "for epoch in range(conf.epochs):\n",
    "    for x, y, gl in tqdm(source_train_loader, desc=\"Source train\"):\n",
    "        x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "        logits = net(x)\n",
    "        logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "        losses = [F.binary_cross_entropy_with_logits(logit.squeeze(), y) for logit in logits_chunked]\n",
    "        xent = sum(losses)\n",
    "\n",
    "        try: \n",
    "            target_x, target_y, target_gl = next(target_iter)\n",
    "        except StopIteration:\n",
    "            target_iter = iter(target_train_loader)\n",
    "            target_x, target_y, target_gl = next(target_iter)\n",
    "        target_x, target_y, target_gl = target_x.to(conf.device), target_y.to(conf.device), target_gl.to(conf.device)\n",
    "        target_logits = net(target_x)\n",
    "\n",
    "        repulsion_loss_args = []\n",
    "        repulsion_loss = loss_fn(target_logits, *repulsion_loss_args)\n",
    "        # TODO: log ordering of target instances according to L_01 and L_10\n",
    "        loss_0_1, loss_1_0, indices_0_1, indices_1_0 = get_orderings(target_logits)\n",
    "        # log group labels according to ordering\n",
    "        metrics[f\"target_loss_0_1_ordering\"].append(target_gl[indices_0_1].tolist())\n",
    "        metrics[f\"target_loss_1_0_ordering\"].append(target_gl[indices_1_0].tolist())\n",
    "        # log false positive and false negative rates for each loss \n",
    "        # i.e. fp = number of instances in top batch_size * mix_rate_lower_bound / 2 that don't have (0,1)/(1/0)\n",
    "        # i.e. fn = number of instances not in top batch_size * mix_rate_lower_bound / 2 that have (0,1)/(1/0)\n",
    "        k = conf.target_batch_size * conf.mix_rate_lower_bound / 2 \n",
    "        fp_0_1 = (target_gl[indices_0_1[:int(k)]] != torch.tensor([0, 1]).to(conf.device)).all(dim=1).float().mean().item()\n",
    "        fp_1_0 = (target_gl[indices_1_0[:int(k)]] != torch.tensor([1, 0]).to(conf.device)).all(dim=1).float().mean().item()\n",
    "        fn_0_1 = (target_gl[indices_0_1[int(k):]] == torch.tensor([0, 1]).to(conf.device)).all(dim=1).float().mean().item() \n",
    "        fn_1_0 = (target_gl[indices_1_0[int(k):]] == torch.tensor([1, 0]).to(conf.device)).all(dim=1).float().mean().item()\n",
    "        metrics[f\"target_fp_0_1\"].append(fp_0_1)\n",
    "        metrics[f\"target_fp_1_0\"].append(fp_1_0)\n",
    "        metrics[f\"target_fn_0_1\"].append(fn_0_1)\n",
    "        metrics[f\"target_fn_1_0\"].append(fn_1_0)\n",
    "        full_loss = xent + conf.aux_weight * repulsion_loss\n",
    "        opt.zero_grad()\n",
    "        full_loss.backward()\n",
    "        opt.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        metrics[f\"xent\"].append(xent.item())\n",
    "        metrics[f\"repulsion_loss\"].append(repulsion_loss.item())\n",
    "    # Compute loss on target validation set (used for model selection)\n",
    "    # and aggregate metrics over the entire test set (should not really be using)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        net.eval()\n",
    "        # compute repulsion loss on target validation set (used for model selection)\n",
    "        repulsion_losses_val = []\n",
    "        weighted_repulsion_losses_val = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, gl in tqdm(target_val_loader, desc=\"Target val\"):\n",
    "                x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "                logits_val = net(x)\n",
    "                repulsion_loss_val = loss_fn(logits_val, *repulsion_loss_args)\n",
    "                repulsion_losses_val.append(repulsion_loss_val.item())\n",
    "                weighted_repulsion_losses_val.append(conf.aux_weight * repulsion_loss_val.item())\n",
    "        metrics[f\"target_val_repulsion_loss\"].append(np.mean(repulsion_losses_val))\n",
    "        metrics[f\"target_val_weighted_repulsion_loss\"].append(np.mean(weighted_repulsion_losses_val))\n",
    "        # compute xent on source validation set\n",
    "        xent_val = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, gl in tqdm(source_train_loader, desc=\"Source val\"):\n",
    "                x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "                logits_val = net(x)\n",
    "                logits_chunked_val = torch.chunk(logits_val, conf.heads, dim=-1)\n",
    "                losses_val = [F.binary_cross_entropy_with_logits(logit.squeeze(), y) for logit in logits_chunked_val]\n",
    "                xent_val.append(sum(losses_val).item())\n",
    "        metrics[f\"source_val_xent\"].append(np.mean(xent_val))\n",
    "        metrics[f\"val_loss\"].append(np.mean(repulsion_losses_val) + np.mean(xent_val))\n",
    "        metrics[f\"val_weighted_loss\"].append(np.mean(weighted_repulsion_losses_val) + np.mean(xent_val))\n",
    "\n",
    "        # compute accuracy over target test set (used to evaluate actual performance)\n",
    "        total_correct = torch.zeros(conf.heads)\n",
    "        total_correct_alt = torch.zeros(conf.heads)\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y, test_gl in target_test_loader:\n",
    "                test_x, test_y, test_gl = test_x.to(conf.device), test_y.to(conf.device), test_gl.to(conf.device)\n",
    "                test_logits = net(test_x).squeeze()\n",
    "                total_samples += test_y.size(0)\n",
    "                \n",
    "                for i in range(conf.heads):\n",
    "                    total_correct[i] += ((test_logits[:, i] > 0) == test_y.flatten()).sum().item()\n",
    "                    total_correct_alt[i] += ((test_logits[:, i] > 0) == test_gl[:, 1].flatten()).sum().item()\n",
    "        \n",
    "        for i in range(conf.heads):\n",
    "            metrics[f\"epoch_acc_{i}\"].append((total_correct[i] / total_samples).item())\n",
    "            metrics[f\"epoch_acc_{i}_alt\"].append((total_correct_alt[i] / total_samples).item())\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Test Accuracies:\")\n",
    "        # print validation losses\n",
    "        print(f\"Target val repulsion loss: {metrics[f'target_val_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Target val weighted repulsion loss: {metrics[f'target_val_weighted_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Source val xent: {metrics[f'source_val_xent'][-1]:.4f}\")\n",
    "        print(f\"val loss: {metrics[f'val_loss'][-1]:.4f}\")\n",
    "        print(f\"val weighted loss: {metrics[f'val_weighted_loss'][-1]:.4f}\")\n",
    "        for i in range(conf.heads):\n",
    "            print(f\"Head {i}: {metrics[f'epoch_acc_{i}'][-1]:.4f}, Alt: {metrics[f'epoch_acc_{i}_alt'][-1]:.4f}\")\n",
    "        \n",
    "        net.train()\n",
    "\n",
    "metrics = dict(metrics)\n",
    "# save metrics \n",
    "import json \n",
    "with open(f\"{exp_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"xent\"], label=\"xent\", color=\"red\")\n",
    "plt.plot(metrics[\"repulsion_loss\"], label=\"repulsion_loss\", color=\"blue\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print loss\n",
    "# plt.plot(metrics[\"xent\"], label=\"xent\", color=\"pink\")\n",
    "# plt.plot(metrics[\"repulsion_loss\"], label=\"repulsion_loss\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"source_val_xent\"], label=\"source_val_xent\", color=\"red\")\n",
    "plt.plot(metrics[\"target_val_weighted_repulsion_loss\"], label=\"target_val_repulsion_loss\", color=\"blue\")\n",
    "plt.plot(metrics[\"val_weighted_loss\"], label=\"val_loss\", color=\"green\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics\n",
    "# plot acc_0 and acc_1 and acc_0_alt and acc_1_alt\n",
    "plt.plot(metrics[\"epoch_acc_0\"], label=\"acc_0\", color=\"blue\")\n",
    "plt.plot(metrics[\"epoch_acc_1\"], label=\"acc_1\", color=\"green\")\n",
    "plt.plot(metrics[\"epoch_acc_0_alt\"], label=\"acc_0_alt\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"epoch_acc_1_alt\"], label=\"acc_1_alt\", color=\"lightgreen\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot false positive and false negative rates\n",
    "plt.plot(metrics[\"target_fp_0_1\"], label=\"target_fp_0_1\", color=\"blue\")\n",
    "plt.plot(metrics[\"target_fp_1_0\"], label=\"target_fp_1_0\", color=\"green\")\n",
    "plt.plot(metrics[\"target_fn_0_1\"], label=\"target_fn_0_1\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"target_fn_1_0\"], label=\"target_fn_1_0\", color=\"lightgreen\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index of minimum val_weighted_loss, target_val_weighted_repulsion_loss \n",
    "min_val_weighted_loss_idx = np.argmin(metrics[\"val_weighted_loss\"])\n",
    "min_target_val_weighted_repulsion_loss_idx = np.argmin(metrics[\"target_val_weighted_repulsion_loss\"])\n",
    "# get maximum acc (max of max(acc_0, acc_1))\n",
    "accs = np.maximum(np.array(metrics[\"epoch_acc_0\"]), np.array(metrics[\"epoch_acc_1\"]))\n",
    "max_acc_idx = np.argmax(accs)\n",
    "# get accs for min val_weighted_loss and min target_val_weighted_repulsion_loss \n",
    "val_weighted_loss_acc = accs[min_val_weighted_loss_idx]\n",
    "target_val_weighted_repulsion_loss_acc = accs[min_target_val_weighted_repulsion_loss_idx]\n",
    "max_acc = accs[max_acc_idx]\n",
    "\n",
    "# plot max_acc, val_weighted_loss_acc, target_val_weighted_repulsion_loss_acc as a bar chart \n",
    "plt.bar([\"max\", \"weighted_loss\", \"weighted_repulsion_loss\"], [max_acc, val_weighted_loss_acc, target_val_weighted_repulsion_loss_acc])\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-gen-KG5DY0Zz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
