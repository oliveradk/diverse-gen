{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# set directory\n",
    "os.chdir(\"/nas/ucb/oliveradk/diverse-gen/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from typing import Optional, Literal, Callable\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import submitit\n",
    "from submitit.core.utils import CommandFunction\n",
    "import nevergrad as ng\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from losses.loss_types import LossType\n",
    "from utils.exp_utils import get_executor, get_executor_local, run_experiments\n",
    "from utils.utils import conf_to_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_NAME = \"spur_corr_exp.py\"\n",
    "EXP_DIR = Path(\"output/real_data_group_labels_exps\")\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 2, 3]\n",
    "\n",
    "# TODO: add aux weights based on tuining\n",
    "method_configs = {\n",
    "    \"DivDis\": {\"loss_type\": LossType.DIVDIS, \"aux_weight\": 2.5},\n",
    "    \"TopK 0.1\": {\"loss_type\": LossType.TOPK, \"mix_rate_lower_bound\": 0.1, \"aux_weight\": 2.5},\n",
    "    \"TopK 0.5\": {\"loss_type\": LossType.TOPK, \"mix_rate_lower_bound\": 0.5, \"aux_weight\": 2.5},\n",
    "    \"DBAT\": {\"loss_type\": LossType.DBAT, \"shared_backbone\": False, \"freeze_heads\": True, \"binary\": True, \"batch_size\": 16, \"target_batch_size\": 32},\n",
    "}\n",
    "\n",
    "dataset_configs = {\n",
    "    \"waterbirds\": {\"dataset\": \"waterbirds\", \"model\": \"Resnet50\", \"epochs\": 5, \"source_cc\": False, \"use_group_labels\": True},\n",
    "}\n",
    "\n",
    "configs = {\n",
    "    (ds_name, method_name, seed): {**ds_config, **method_config, \"seed\": seed} \n",
    "    for (ds_name, ds_config), (method_name, method_config) in product(dataset_configs.items(), method_configs.items())\n",
    "    for seed in seeds\n",
    "}\n",
    "\n",
    "def get_conf_dir(ds_name, method_name, seed):\n",
    "    return f\"{EXP_DIR}/{ds_name}_{method_name}/{seed}\"\n",
    "\n",
    "for (ds_name, method_name, seed), conf in configs.items():\n",
    "    exp_dir = get_conf_dir(ds_name, method_name, seed)\n",
    "    conf[\"exp_dir\"] = exp_dir\n",
    "    conf[\"plot_activations\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = get_executor(EXP_DIR, mem_gb=16)\n",
    "jobs = run_experiments(executor, list(configs.values()), SCRIPT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.4295823276042938,\n",
       "  0.4325163960456848,\n",
       "  0.7816706895828247,\n",
       "  0.7221263647079468,\n",
       "  0.8864342570304871],\n",
       " [0.8983430862426758,\n",
       "  0.9176734685897827,\n",
       "  0.8935105204582214,\n",
       "  0.8940283060073853,\n",
       "  0.9038660526275635])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbat_conf = [conf for conf in configs.values() if conf[\"loss_type\"] == LossType.DBAT][0]\n",
    "exp_metrics = get_exp_metrics(dbat_conf)\n",
    "exp_metrics[\"test_acc_0\"], exp_metrics[\"test_acc_1\"]\n",
    "# hmm I might be processing dbat results wrong\n",
    "# either validation loss is 0 to selecting wrong ac, or mismatch between val loss or idx labels\n",
    "# should go back and check, recompute results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pathlib import Path\n",
    "def get_exp_metrics(conf: dict):\n",
    "    if not (Path(conf[\"exp_dir\"]) / \"metrics.json\").exists():\n",
    "        raise FileNotFoundError(f\"Metrics file not found for experiment {conf['exp_dir']}\")\n",
    "    with open(Path(conf[\"exp_dir\"]) / \"metrics.json\", \"r\") as f:\n",
    "        exp_metrics = json.load(f)\n",
    "    return exp_metrics\n",
    "\n",
    "def get_max_acc(\n",
    "    exp_metrics: dict,\n",
    "    acc_metric: Literal[\"test_acc\", \"test_worst_acc\", \"test_acc_alt\"]=\"test_acc\",\n",
    "    model_selection: Literal[\"acc\", \"loss\", \"weighted_loss\", \"repulsion_loss\"]=\"acc\", \n",
    "    head_idx: int = 0, \n",
    "    head_1_epochs: Optional[int] = None\n",
    "):\n",
    "    if head_1_epochs is not None:\n",
    "        exp_metrics = {k: v[head_1_epochs:] for k, v in exp_metrics.items()}\n",
    "    accs = np.array(exp_metrics[f'{acc_metric}_{head_idx}'])\n",
    "    if model_selection == \"acc\": \n",
    "        max_acc_idx= np.argmax(accs)\n",
    "    elif model_selection == \"loss\":\n",
    "        max_acc_idx = np.argmin(exp_metrics[\"val_loss\"])\n",
    "    elif model_selection == \"weighted_loss\":\n",
    "        max_acc_idx = np.argmin(exp_metrics[\"val_weighted_loss\"])\n",
    "    elif model_selection == \"repulsion_loss\":\n",
    "        max_acc_idx = np.argmin(exp_metrics[\"target_val_weighted_repulsion_loss\"])\n",
    "    else: \n",
    "        raise ValueError(f\"Invalid model selection: {model_selection}\")\n",
    "    accs = accs[max_acc_idx]\n",
    "    return accs\n",
    "\n",
    "# data structure: dictionary with keys method types, values dict[mix_rate, list[len(seeds)]] of cifar accuracies (for now ignore case where mix_rate != mix_rate_lower_bound)\n",
    "def get_acc_results(\n",
    "    exp_configs: list[dict],\n",
    "    acc_metric: Literal[\"test_acc\", \"test_worst_acc\", \"test_acc_alt\"]=\"test_acc\",\n",
    "    model_selection: Literal[\"acc\", \"loss\", \"weighted_loss\", \"repulsion_loss\"]=\"acc\",\n",
    "    verbose: bool=False, \n",
    "    head_idx: int= 0\n",
    "):\n",
    "    results = []\n",
    "    for conf in exp_configs:\n",
    "        try:\n",
    "            exp_metrics = get_exp_metrics(conf)\n",
    "            head_1_epochs = 2 if conf[\"loss_type\"] == LossType.DBAT else None\n",
    "            max_acc = get_max_acc(exp_metrics, acc_metric, model_selection, head_idx, head_1_epochs)\n",
    "            results.append(max_acc)\n",
    "        except FileNotFoundError:\n",
    "            if verbose:\n",
    "                print(f\"Metrics file not found for experiment {conf['exp_dir']}\")\n",
    "            continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "exps_by_method = defaultdict(list)\n",
    "for (ds_name, method_name, seed), conf in configs.items():\n",
    "    exps_by_method[method_name].append(conf)\n",
    "\n",
    "results = {\n",
    "    method_name: get_acc_results(\n",
    "        method_exps, model_selection=\"acc\", acc_metric=\"test_acc\", head_idx=0 if method_name != \"DBAT\" else 1, verbose=True\n",
    "    )\n",
    "    for method_name, method_exps in exps_by_method.items()\n",
    "}\n",
    "\n",
    "results_alt = {\n",
    "    method_name: get_acc_results(method_exps, model_selection=\"acc\", acc_metric=\"test_acc\", head_idx=1 if method_name != \"DBAT\" else 0, verbose=True)\n",
    "    for method_name, method_exps in exps_by_method.items()\n",
    "}\n",
    "\n",
    "results_worst = {\n",
    "    method_name: get_acc_results(method_exps, model_selection=\"acc\", acc_metric=\"test_worst_acc\", head_idx=0 if method_name != \"DBAT\" else 1, verbose=True)\n",
    "    for method_name, method_exps in exps_by_method.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'DivDis': [0.9207801222801208, 0.9190542101860046, 0.9328615665435791],\n",
       "  'TopK 0.1': [0.8973075747489929, 0.8545046448707581, 0.8917846083641052],\n",
       "  'TopK 0.5': [0.934242308139801, 0.9092164039611816, 0.9092164039611816],\n",
       "  'DBAT': [0.9038660526275635, 0.9290645718574524, 0.9190542101860046]},\n",
       " {'DivDis': [0.9288919568061829, 0.9290645718574524, 0.9154297709465027],\n",
       "  'TopK 0.1': [0.9057645797729492, 0.9335519671440125, 0.912150502204895],\n",
       "  'TopK 0.5': [0.9537452459335327, 0.9376941919326782, 0.9473593235015869],\n",
       "  'DBAT': [0.8864342570304871, 0.9223334193229675, 0.9192267656326294]},\n",
       " {'DivDis': [0.6806853413581848, 0.6495327353477478, 0.704049825668335],\n",
       "  'TopK 0.1': [0.6510903239250183, 0.559190034866333, 0.722741425037384],\n",
       "  'TopK 0.5': [0.8520249128341675, 0.7476718425750732, 0.8052959442138672],\n",
       "  'DBAT': [0.8150776028633118, 0.8709534406661987, 0.8541020154953003]})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, results_alt, results_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Method & Average Acc & Alternative Acc & Worst-Group Acc \\\\\n",
      "\\midrule\n",
      "DivDis & 92.4 ± 0.6 & 92.4 ± 0.6 & 67.8 ± 2.2 \\\\\n",
      "TopK 0.1 & 88.1 ± 1.9 & 91.7 ± 1.2 & 64.4 ± 6.7 \\\\\n",
      "TopK 0.5 & 91.8 ± 1.2 & 94.6 ± 0.7 & 80.2 ± 4.3 \\\\\n",
      "DBAT & 91.7 ± 1.0 & 90.9 ± 1.6 & 84.7 ± 2.3 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Method': [],\n",
    "    'Average Acc': [],\n",
    "    'Alternative Acc': [],\n",
    "    'Worst-Group Acc': []\n",
    "})\n",
    "\n",
    "for method in results.keys():\n",
    "    avg_acc = f\"{np.mean(results[method])*100:.1f} ± {np.std(results[method])*100:.1f}\"\n",
    "    alt_acc = f\"{np.mean(results_alt[method])*100:.1f} ± {np.std(results_alt[method])*100:.1f}\"\n",
    "    worst_acc = f\"{np.mean(results_worst[method])*100:.1f} ± {np.std(results_worst[method])*100:.1f}\"\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame({\n",
    "        'Method': [method],\n",
    "        'Average Acc': [avg_acc],\n",
    "        'Alternative Acc': [alt_acc],\n",
    "        'Worst-Group Acc': [worst_acc]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Print LaTeX table\n",
    "print(df.to_latex(index=False, escape=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
