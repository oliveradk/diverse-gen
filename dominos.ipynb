{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random as rnd\n",
    "from typing import Optional, Callable\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as  pd\n",
    "import torchvision.utils as vision_utils\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from losses.divdis import DivDisLoss \n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from models.backbone import MultiHeadBackbone\n",
    "from models.multi_model import MultiNetModel\n",
    "from models.lenet import LeNet\n",
    "\n",
    "from datasets.cifar_mnist import get_cifar_mnist_datasets\n",
    "from datasets.fmnist_mnist import get_fmnist_mnist_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add dbat \n",
    "# TODO: add other vision datasets \n",
    "# TODO: add language datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "@dataclass\n",
    "class Config():\n",
    "    seed: int = 45\n",
    "    dataset: str = \"cifar_mnist\"\n",
    "    loss_type: LossType = LossType.TOPK\n",
    "    batch_size: int = 32#128\n",
    "    target_batch_size: int = 128#128\n",
    "    epochs: int = 10\n",
    "    heads: int = 2 \n",
    "    model: str = \"Resnet50\"\n",
    "    shared_backbone: bool = True\n",
    "    source_weight: float = 1.0\n",
    "    aux_weight: float = 1.0\n",
    "    source_mix_rate: float = 0.0\n",
    "    source_l_01_mix_rate: Optional[float] = None\n",
    "    source_l_10_mix_rate: Optional[float] = None\n",
    "    mix_rate: Optional[float] = 0.9\n",
    "    l_01_mix_rate: Optional[float] = None # TODO: geneneralize\n",
    "    l_10_mix_rate: Optional[float] = None\n",
    "    mix_rate_lower_bound: Optional[float] = 0.5\n",
    "    all_unlabeled: bool = False\n",
    "    inbalance_ratio: Optional[bool] = False\n",
    "    lr: float = 1e-3 #1e-3\n",
    "    weight_decay: float = 1e-3 #1e-4\n",
    "    lr_scheduler: Optional[str] = None #\"cosine\"# \"cosine\"\n",
    "    num_cycles: float = 0.5\n",
    "    frac_warmup: float = 0.05\n",
    "    vertical: bool = True\n",
    "    device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    exp_name: str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def post_init(conf: Config, overrides: list[str]=[]):\n",
    "    if conf.l_01_mix_rate is not None and conf.l_10_mix_rate is None:\n",
    "        conf.l_10_mix_rate = 0.0\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_01_mix_rate\n",
    "        assert conf.mix_rate == conf.l_01_mix_rate\n",
    "    elif conf.l_01_mix_rate is None and conf.l_10_mix_rate is not None:\n",
    "        conf.l_01_mix_rate = 0.0\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_10_mix_rate\n",
    "        assert conf.mix_rate == conf.l_10_mix_rate\n",
    "    elif conf.l_01_mix_rate is not None and conf.l_10_mix_rate is not None:\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_01_mix_rate + conf.l_10_mix_rate\n",
    "        assert conf.mix_rate == conf.l_01_mix_rate + conf.l_10_mix_rate\n",
    "    else: # both are none \n",
    "        assert conf.mix_rate is not None\n",
    "        conf.l_01_mix_rate = conf.mix_rate / 2\n",
    "        conf.l_10_mix_rate = conf.mix_rate / 2\n",
    "    \n",
    "    conf.source_l_01_mix_rate = conf.source_mix_rate / 2\n",
    "    conf.source_l_10_mix_rate = conf.source_mix_rate / 2\n",
    "    \n",
    "    if conf.mix_rate_lower_bound is None:\n",
    "        conf.mix_rate_lower_bound = conf.mix_rate\n",
    "    \n",
    "    if conf.loss_type == LossType.DIVDIS and \"aux_weight\" not in overrides:\n",
    "        conf.aux_weight = 1.0\n",
    "    if conf.model == \"ClipViT\" and \"lr\" not in overrides:\n",
    "        conf.lr = 5e-5\n",
    "    if conf.model == \"ClipViT\" and \"lr_scheduler\" not in overrides:\n",
    "        conf.lr_scheduler = \"cosine\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config\n",
    "conf = Config()\n",
    "#get config overrides if runnign from command line\n",
    "overrride_keys = []\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    overrides = OmegaConf.from_cli(sys.argv[1:])\n",
    "    overrride_keys = overrides.keys()\n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), overrides)\n",
    "    conf = Config(**conf_dict)\n",
    "post_init(conf, overrride_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory from config\n",
    "from dataclasses import asdict\n",
    "exp_dir = f\"output/dominos/{conf.exp_name}\"\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "# save full config to exp_dir\n",
    "with open(f\"{exp_dir}/config.yaml\", \"w\") as f:\n",
    "    OmegaConf.save(config=conf, f=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(conf.seed)\n",
    "np.random.seed(conf.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transform = None\n",
    "pad_sides = False\n",
    "if conf.model == \"Resnet50\":\n",
    "    from torchvision import models\n",
    "    from torchvision.models.resnet import ResNet50_Weights\n",
    "    resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    model_builder = lambda: torch.nn.Sequential(*list(resnet50.children())[:-1])\n",
    "    resnet_50_transforms = ResNet50_Weights.DEFAULT.transforms()\n",
    "    model_transform = transforms.Compose([\n",
    "        transforms.Resize(resnet_50_transforms.resize_size * 2, interpolation=resnet_50_transforms.interpolation),\n",
    "        transforms.CenterCrop(resnet_50_transforms.crop_size),\n",
    "        transforms.Normalize(mean=resnet_50_transforms.mean, std=resnet_50_transforms.std)\n",
    "    ])\n",
    "    pad_sides = True\n",
    "    feature_dim = 2048\n",
    "elif conf.model == \"ClipViT\":\n",
    "    # from models.clip_vit import ClipViT\n",
    "    # model_builder = lambda: ClipViT()\n",
    "    # feature_dim = 768\n",
    "    # input_size = 96\n",
    "    # model_transform = transforms.Compose([\n",
    "    #     transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "    # ])\n",
    "    import clip \n",
    "    clip_model, preprocess = clip.load('ViT-B/32', device='cpu')\n",
    "    model_builder = lambda: clip_model.visual\n",
    "    model_transform = transforms.Compose([\n",
    "        preprocess.transforms[0],\n",
    "        preprocess.transforms[1],\n",
    "        preprocess.transforms[4]\n",
    "    ])\n",
    "    feature_dim = 512\n",
    "    pad_sides = True\n",
    "elif conf.model == \"LeNet\":\n",
    "    from models.lenet import LeNet\n",
    "    from functools import partial\n",
    "    model_builder = lambda: partial(LeNet, num_classes=1, dropout_p=0.0)\n",
    "    feature_dim = 256\n",
    "else: \n",
    "    raise ValueError(f\"Model {conf.model} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.dataset == \"cifar_mnist\":\n",
    "    source_train, source_val, target_train, target_val, target_test = get_cifar_mnist_datasets(\n",
    "        vertical=conf.vertical, \n",
    "        source_mix_rate_0_1=conf.source_l_01_mix_rate, \n",
    "        source_mix_rate_1_0=conf.source_l_10_mix_rate, \n",
    "        target_mix_rate_0_1=conf.l_01_mix_rate, \n",
    "        target_mix_rate_1_0=conf.l_10_mix_rate, \n",
    "        transform=model_transform, \n",
    "        pad_sides=pad_sides\n",
    "    )\n",
    "elif conf.dataset == \"fmnist_mnist\":\n",
    "    source_train, source_val, target_train, target_val, target_test = get_fmnist_mnist_datasets(\n",
    "        vertical=conf.vertical, \n",
    "        source_mix_rate_0_1=conf.source_l_01_mix_rate, \n",
    "        source_mix_rate_1_0=conf.source_l_10_mix_rate, \n",
    "        target_mix_rate_0_1=conf.l_01_mix_rate, \n",
    "        target_mix_rate_1_0=conf.l_10_mix_rate, \n",
    "        transform=model_transform, \n",
    "        pad_sides=pad_sides\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Dataset {conf.dataset} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_labels = [source_train[i][1:3] for i in range(len(source_train))]\n",
    "assert all([l[1][0] == l[1][1] for l in source_labels])\n",
    "target_labels = [target_train[i][1:3] for i in range(len(target_train))]\n",
    "np.mean([l[1][0] == l[1][1] for l in target_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot image \n",
    "img = source_train[0][0]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target train images with vision_utils.make_grid\n",
    "cifar_mnist_grid = torch.stack([target_train[i][0] for i in range(20)])\n",
    "grid_img = vision_utils.make_grid(cifar_mnist_grid, nrow=10, normalize=True, padding=1)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders \n",
    "source_train_loader = DataLoader(source_train, batch_size=conf.batch_size, shuffle=True)\n",
    "source_val_loader = DataLoader(source_val, batch_size=conf.batch_size, shuffle=True)\n",
    "target_train_loader = DataLoader(target_train, batch_size=conf.target_batch_size, shuffle=True)\n",
    "target_val_loader = DataLoader(target_val, batch_size=conf.target_batch_size, shuffle=True)\n",
    "target_test_loader = DataLoader(target_test, batch_size=conf.batch_size, shuffle=True)\n",
    "\n",
    "# classifiers\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "if conf.shared_backbone:\n",
    "    net = MultiHeadBackbone(model_builder(), conf.heads, feature_dim)\n",
    "else:\n",
    "    net = MultiNetModel(heads=conf.heads, model_builder=model_builder)\n",
    "net = net.to(conf.device)\n",
    "\n",
    "# optimizer\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "num_steps = conf.epochs * len(source_train_loader)\n",
    "if conf.lr_scheduler == \"cosine\":       \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        opt, \n",
    "        num_warmup_steps=num_steps * conf.frac_warmup, \n",
    "        num_training_steps=num_steps, \n",
    "        num_cycles=conf.num_cycles\n",
    "    )\n",
    "else: \n",
    "    # constant learning rate\n",
    "    scheduler = None\n",
    "\n",
    "# loss function\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=conf.heads)\n",
    "else:\n",
    "    loss_fn = ACELoss(\n",
    "        heads=conf.heads, \n",
    "        mode=conf.loss_type.value, \n",
    "        inbalance_ratio=conf.inbalance_ratio,\n",
    "        l_01_rate=conf.mix_rate_lower_bound / 2, \n",
    "        l_10_rate=conf.mix_rate_lower_bound / 2, \n",
    "        all_unlabeled=conf.all_unlabeled,\n",
    "        device=conf.device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accs(logits: torch.Tensor, gl: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        acc = torch.zeros(conf.heads)\n",
    "        acc_alt = torch.zeros(conf.heads)\n",
    "        for i in range(conf.heads):\n",
    "            acc[i] += ((logits[:, i] > 0) == gl[:, 0].flatten()).to(torch.float32).mean().item()\n",
    "            acc_alt[i] += ((logits[:, i] > 0) == gl[:, 1].flatten()).to(torch.float32).mean().item()\n",
    "    return acc, acc_alt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data using first two principle componets of final layer activations\n",
    "from sklearn.decomposition import PCA\n",
    "activations = []\n",
    "labels = []\n",
    "model = model_builder()\n",
    "model = model.to(conf.device)\n",
    "for x, y, gl in tqdm(target_test_loader):\n",
    "    x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "    acts = model(x)\n",
    "    activations.append((acts.detach().cpu()))\n",
    "    labels.append(gl)\n",
    "activations = torch.cat(activations, dim=0).squeeze()\n",
    "labels = torch.cat(labels, dim=0)\n",
    "labels = labels.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(activations)\n",
    "activations_pca = pca.transform(activations)\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot first label\n",
    "scatter1 = ax1.scatter(activations_pca[:, 0], activations_pca[:, 1], c=labels[:, 0].to('cpu'), cmap=\"viridis\")\n",
    "ax1.set_title('Label 0')\n",
    "\n",
    "# Plot second label\n",
    "scatter2 = ax2.scatter(activations_pca[:, 0], activations_pca[:, 1], c=labels[:, 1].to('cpu'), cmap=\"viridis\")\n",
    "ax2.set_title('Label 1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "component_range = [2**i for i in range(1, 9)]\n",
    "n_components_accs = []\n",
    "for n_components in tqdm(component_range):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(activations)\n",
    "    activations_pca = pca.transform(activations)\n",
    "    # fit probe \n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(activations_pca, labels[:, 0].to('cpu').numpy())\n",
    "    acc = lr.score(activations_pca, labels[:, 0].to('cpu').numpy())\n",
    "    n_components_accs.append(acc)\n",
    "plt.plot(component_range, n_components_accs, label=\"accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit linear probe \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "lr.fit(activations.to('cpu').numpy(), labels[:, 0].to('cpu').numpy())\n",
    "# get accuracy \n",
    "acc = lr.score(activations.to('cpu').numpy(), labels[:, 0].to('cpu').numpy())\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(activations)\n",
    "activations_pca = pca.transform(activations)\n",
    "# Create a figure with two 3D subplots side by side\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# First 3D plot for label 0\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "scatter1 = ax1.scatter(activations_pca[:, 0], activations_pca[:, 1], activations_pca[:,2], \n",
    "                      c=labels[:, 0].to('cpu'), cmap=\"viridis\")\n",
    "ax1.view_init(0, 185, 0)\n",
    "ax1.set_title('Label 0')\n",
    "\n",
    "# Second 3D plot for label 1\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "scatter2 = ax2.scatter(activations_pca[:, 0], activations_pca[:, 1], activations_pca[:,2], \n",
    "                      c=labels[:, 1].to('cpu'), cmap=\"viridis\")\n",
    "ax2.view_init(0, 185, 0)\n",
    "ax2.set_title('Label 1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.l_01_mix_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "target_iter = iter(target_train_loader)\n",
    "for epoch in range(conf.epochs):\n",
    "    for batch_idx, (x, y, gl) in tqdm(enumerate(source_train_loader), desc=\"Source train\", total=len(source_train_loader)):\n",
    "        x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "        logits = net(x)\n",
    "        logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "        # source loss \n",
    "        losses = [F.binary_cross_entropy_with_logits(logit.squeeze(), y) for logit in logits_chunked]\n",
    "        xent = sum(losses)\n",
    "        # target loss \n",
    "        try: \n",
    "            target_x, target_y, target_gl = next(target_iter)\n",
    "        except StopIteration:\n",
    "            target_iter = iter(target_train_loader)\n",
    "            target_x, target_y, target_gl = next(target_iter)\n",
    "        target_x, target_y, target_gl = target_x.to(conf.device), target_y.to(conf.device), target_gl.to(conf.device)\n",
    "        target_logits = net(target_x)\n",
    "        repulsion_loss = loss_fn(target_logits)\n",
    "        # full loss \n",
    "        full_loss = conf.source_weight * xent + conf.aux_weight * repulsion_loss\n",
    "        opt.zero_grad()\n",
    "        full_loss.backward()\n",
    "        opt.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        metrics[f\"xent\"].append(xent.item())\n",
    "        metrics[f\"repulsion_loss\"].append(repulsion_loss.item())\n",
    "    # Compute loss on target validation set (used for model selection)\n",
    "    # and aggregate metrics over the entire test set (should not really be using)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        net.eval()\n",
    "        # compute repulsion loss on target validation set (used for model selection)\n",
    "        repulsion_losses_val = []\n",
    "        weighted_repulsion_losses_val = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, gl in tqdm(target_val_loader, desc=\"Target val\"):\n",
    "                x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "                logits_val = net(x)\n",
    "                repulsion_loss_val = loss_fn(logits_val)\n",
    "                repulsion_losses_val.append(repulsion_loss_val.item())\n",
    "                weighted_repulsion_losses_val.append(conf.aux_weight * repulsion_loss_val.item())\n",
    "        metrics[f\"target_val_repulsion_loss\"].append(np.mean(repulsion_losses_val))\n",
    "        metrics[f\"target_val_weighted_repulsion_loss\"].append(np.mean(weighted_repulsion_losses_val))\n",
    "        # compute xent on source validation set\n",
    "        xent_val = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, gl in tqdm(source_val_loader, desc=\"Source val\"):\n",
    "                x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "                logits_val = net(x)\n",
    "                logits_chunked_val = torch.chunk(logits_val, conf.heads, dim=-1)\n",
    "                losses_val = [F.binary_cross_entropy_with_logits(logit.squeeze(), y) for logit in logits_chunked_val]\n",
    "                xent_val.append(sum(losses_val).item())\n",
    "        metrics[f\"source_val_xent\"].append(np.mean(xent_val))\n",
    "        metrics[f\"val_loss\"].append(np.mean(repulsion_losses_val) + np.mean(xent_val))\n",
    "        metrics[f\"val_weighted_loss\"].append(np.mean(weighted_repulsion_losses_val) + np.mean(xent_val))\n",
    "\n",
    "        # compute accuracy over target test set (used to evaluate actual performance)\n",
    "        total_correct = torch.zeros(conf.heads)\n",
    "        total_correct_alt = torch.zeros(conf.heads)\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y, test_gl in target_test_loader:\n",
    "                test_x, test_y, test_gl = test_x.to(conf.device), test_y.to(conf.device), test_gl.to(conf.device)\n",
    "                test_logits = net(test_x).squeeze()\n",
    "                total_samples += test_y.size(0)\n",
    "                \n",
    "                for i in range(conf.heads):\n",
    "                    total_correct[i] += ((test_logits[:, i] > 0) == test_y.flatten()).sum().item()\n",
    "                    total_correct_alt[i] += ((test_logits[:, i] > 0) == test_gl[:, 1].flatten()).sum().item()\n",
    "        \n",
    "        for i in range(conf.heads):\n",
    "            metrics[f\"epoch_acc_{i}\"].append((total_correct[i] / total_samples).item())\n",
    "            metrics[f\"epoch_acc_{i}_alt\"].append((total_correct_alt[i] / total_samples).item())\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Test Accuracies:\")\n",
    "        # print validation losses\n",
    "        print(f\"Target val repulsion loss: {metrics[f'target_val_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Target val weighted repulsion loss: {metrics[f'target_val_weighted_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Source val xent: {metrics[f'source_val_xent'][-1]:.4f}\")\n",
    "        print(f\"val loss: {metrics[f'val_loss'][-1]:.4f}\")\n",
    "        print(f\"val weighted loss: {metrics[f'val_weighted_loss'][-1]:.4f}\")\n",
    "        for i in range(conf.heads):\n",
    "            print(f\"Head {i}: {metrics[f'epoch_acc_{i}'][-1]:.4f}, Alt: {metrics[f'epoch_acc_{i}_alt'][-1]:.4f}\")\n",
    "        \n",
    "        net.train()\n",
    "\n",
    "metrics = dict(metrics)\n",
    "# save metrics \n",
    "import json \n",
    "with open(f\"{exp_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"xent\"], label=\"xent\", color=\"red\")\n",
    "plt.plot(metrics[\"repulsion_loss\"], label=\"repulsion_loss\", color=\"blue\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print loss\n",
    "# plt.plot(metrics[\"xent\"], label=\"xent\", color=\"pink\")\n",
    "# plt.plot(metrics[\"repulsion_loss\"], label=\"repulsion_loss\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"source_val_xent\"], label=\"source_val_xent\", color=\"red\")\n",
    "plt.plot(metrics[\"target_val_weighted_repulsion_loss\"], label=\"target_val_repulsion_loss\", color=\"blue\")\n",
    "plt.plot(metrics[\"val_weighted_loss\"], label=\"val_loss\", color=\"green\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()\n",
    "plt.savefig(f\"{exp_dir}/val_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics\n",
    "# plot acc_0 and acc_1 and acc_0_alt and acc_1_alt\n",
    "plt.plot(metrics[\"epoch_acc_0\"], label=\"acc_0\", color=\"blue\")\n",
    "plt.plot(metrics[\"epoch_acc_1\"], label=\"acc_1\", color=\"green\")\n",
    "plt.plot(metrics[\"epoch_acc_0_alt\"], label=\"acc_0_alt\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"epoch_acc_1_alt\"], label=\"acc_1_alt\", color=\"lightgreen\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()\n",
    "plt.savefig(f\"{exp_dir}/acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index of minimum val_weighted_loss, target_val_weighted_repulsion_loss \n",
    "min_val_weighted_loss_idx = np.argmin(metrics[\"val_weighted_loss\"])\n",
    "min_target_val_weighted_repulsion_loss_idx = np.argmin(metrics[\"target_val_weighted_repulsion_loss\"])\n",
    "# get maximum acc (max of max(acc_0, acc_1))\n",
    "accs = np.maximum(np.array(metrics[\"epoch_acc_0\"]), np.array(metrics[\"epoch_acc_1\"]))\n",
    "max_acc_idx = np.argmax(accs)\n",
    "print(f\"max_acc_idx: {max_acc_idx}\")\n",
    "print(f\"min_val_weighted_loss_idx: {min_val_weighted_loss_idx}\")\n",
    "print(f\"min_target_val_weighted_repulsion_loss_idx: {min_target_val_weighted_repulsion_loss_idx}\")\n",
    "# get accs for min val_weighted_loss and min target_val_weighted_repulsion_loss \n",
    "val_weighted_loss_acc = accs[min_val_weighted_loss_idx]\n",
    "target_val_weighted_repulsion_loss_acc = accs[min_target_val_weighted_repulsion_loss_idx]\n",
    "max_acc = accs[max_acc_idx]\n",
    "\n",
    "# plot max_acc, val_weighted_loss_acc, target_val_weighted_repulsion_loss_acc as a bar chart \n",
    "plt.bar([\"max\", \"weighted_loss\", \"weighted_repulsion_loss\"], [max_acc, val_weighted_loss_acc, target_val_weighted_repulsion_loss_acc])\n",
    "# show y ticks at every 0.1 \n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()\n",
    "plt.savefig(f\"{exp_dir}/max_acc_model_selection.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-gen-KG5DY0Zz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
