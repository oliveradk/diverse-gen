{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "# set directory\n",
    "os.chdir(\"/nas/ucb/oliveradk/diverse-gen/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from typing import Optional, Literal, Callable\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import submitit\n",
    "from submitit.core.utils import CommandFunction\n",
    "import nevergrad as ng\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from losses.loss_types import LossType\n",
    "from utils.exp_utils import get_executor, get_executor_local, run_experiments\n",
    "from utils.utils import conf_to_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_NAME = \"spur_corr_exp.py\"\n",
    "EXP_DIR = Path(\"output/real_data_exps\")\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 2, 3]\n",
    "\n",
    "# TODO: add aux weights based on tuining\n",
    "method_configs = {\n",
    "    \"DivDis\": {\"loss_type\": LossType.DIVDIS, \"aux_weight\": 3},\n",
    "    \"TopK 0.1\": {\"loss_type\": LossType.TOPK, \"mix_rate_lower_bound\": 0.1, \"aux_weight\": 1},\n",
    "    \"TopK 0.5\": {\"loss_type\": LossType.TOPK, \"mix_rate_lower_bound\": 0.5, \"aux_weight\": 3},\n",
    "    \"DBAT\": {\"loss_type\": LossType.DBAT, \"shared_backbone\": False, \"freeze_heads\": True, \"binary\": True, \"batch_size\": 16, \"target_batch_size\": 32, \"aux_weight\": 0.5},\n",
    "    \"ERM\": {\"loss_type\": LossType.ERM, \"aux_weight\": 0},\n",
    "}\n",
    "\n",
    "dataset_configs = {\n",
    "    \"waterbirds\": {\"dataset\": \"waterbirds\", \"model\": \"Resnet50\", \"epochs\": 5, \"source_cc\": False},\n",
    "}\n",
    "\n",
    "configs = {\n",
    "    (ds_name, method_name, seed): {**ds_config, **method_config, \"seed\": seed} \n",
    "    for (ds_name, ds_config), (method_name, method_config) in product(dataset_configs.items(), method_configs.items())\n",
    "    for seed in seeds\n",
    "}\n",
    "\n",
    "def get_conf_dir(conf_name: tuple):\n",
    "    ds, method, seed = conf_name\n",
    "    return f\"{EXP_DIR}/{ds}_{method}/{seed}\"\n",
    "    # return f\"{ds}_{method}/{seed}\" # TODO: fix\n",
    "\n",
    "for (ds_name, method_name, seed), conf in configs.items():\n",
    "    exp_dir = get_conf_dir((ds_name, method_name, seed))\n",
    "    conf[\"exp_dir\"] = exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = get_executor(EXP_DIR, mem_gb=16)\n",
    "jobs = run_experiments(executor, list(configs.values()), SCRIPT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pathlib import Path\n",
    "def get_exp_metrics(conf: dict):\n",
    "    if not (Path(conf[\"exp_dir\"]) / \"metrics.json\").exists():\n",
    "        raise FileNotFoundError(f\"Metrics file not found for experiment {conf['exp_dir']}\")\n",
    "    with open(Path(conf[\"exp_dir\"]) / \"metrics.json\", \"r\") as f:\n",
    "        exp_metrics = json.load(f)\n",
    "    return exp_metrics\n",
    "\n",
    "def get_max_acc(\n",
    "    exp_metrics: dict,\n",
    "    acc_metric: Literal[\"test_acc\", \"test_worst_acc\", \"test_acc_alt\"]=\"test_acc\",\n",
    "    model_selection: Literal[\"acc\", \"loss\", \"weighted_loss\", \"repulsion_loss\"]=\"acc\"\n",
    "):\n",
    "    max_accs = np.maximum(np.array(exp_metrics[f'{acc_metric}_0']), np.array(exp_metrics[f'{acc_metric}_1']))\n",
    "    if model_selection == \"acc\": \n",
    "        max_acc_idx= np.argmax(max_accs)\n",
    "    elif model_selection == \"loss\":\n",
    "        max_acc_idx = np.argmin(exp_metrics[\"val_loss\"])\n",
    "    elif model_selection == \"weighted_loss\":\n",
    "        max_acc_idx = np.argmin(exp_metrics[\"val_weighted_loss\"])\n",
    "    elif model_selection == \"repulsion_loss\":\n",
    "        max_acc_idx = np.argmin(exp_metrics[\"target_val_weighted_repulsion_loss\"])\n",
    "    else: \n",
    "        raise ValueError(f\"Invalid model selection: {model_selection}\")\n",
    "    max_acc = max_accs[max_acc_idx]\n",
    "    return max_acc\n",
    "\n",
    "# data structure: dictionary with keys method types, values dict[mix_rate, list[len(seeds)]] of cifar accuracies (for now ignore case where mix_rate != mix_rate_lower_bound)\n",
    "def get_acc_results(\n",
    "    exp_configs: list[dict],\n",
    "    acc_metric: Literal[\"test_acc\", \"test_worst_acc\", \"test_acc_alt\"]=\"test_acc\",\n",
    "    model_selection: Literal[\"acc\", \"loss\", \"weighted_loss\", \"repulsion_loss\"]=\"acc\",\n",
    "    verbose: bool=False\n",
    "):\n",
    "    results = []\n",
    "    for conf in exp_configs:\n",
    "        try:\n",
    "            exp_metrics = get_exp_metrics(conf)\n",
    "            max_acc = get_max_acc(exp_metrics, acc_metric, model_selection)\n",
    "            results.append(max_acc)\n",
    "        except FileNotFoundError:\n",
    "            if verbose:\n",
    "                print(f\"Metrics file not found for experiment {conf['exp_dir']}\")\n",
    "            continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "exps_by_method = defaultdict(list)\n",
    "for (ds_name, method_name, seed), conf in configs.items():\n",
    "    exps_by_method[method_name].append(conf)\n",
    "\n",
    "results = {\n",
    "    method_name: get_acc_results(method_exps, model_selection=\"acc\", acc_metric=\"test_acc\")\n",
    "    for method_name, method_exps in exps_by_method.items()\n",
    "}\n",
    "\n",
    "results_alt = {\n",
    "    method_name: get_acc_results(method_exps, model_selection=\"acc\", acc_metric=\"test_acc_alt\")\n",
    "    for method_name, method_exps in exps_by_method.items()\n",
    "}\n",
    "\n",
    "results_worst = {\n",
    "    method_name: get_acc_results(method_exps, model_selection=\"acc\", acc_metric=\"test_worst_acc\")\n",
    "    for method_name, method_exps in exps_by_method.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:\n",
      "DivDis    : 0.929 ± 0.010\n",
      "TopK 0.1  : 0.897 ± 0.007\n",
      "TopK 0.5  : 0.906 ± 0.016\n",
      "DBAT      : 0.866 ± 0.009\n",
      "ERM       : 0.891 ± 0.012\n",
      "\n",
      "Alternative Accuracy:\n",
      "DivDis    : 0.804 ± 0.052\n",
      "TopK 0.1  : 0.732 ± 0.019\n",
      "TopK 0.5  : 0.925 ± 0.003\n",
      "DBAT      : 0.707 ± 0.009\n",
      "ERM       : 0.638 ± 0.014\n",
      "\n",
      "Worst-Group Accuracy:\n",
      "DivDis    : 0.739 ± 0.047\n",
      "TopK 0.1  : 0.718 ± 0.058\n",
      "TopK 0.5  : 0.789 ± 0.042\n",
      "DBAT      : 0.625 ± 0.020\n",
      "ERM       : 0.630 ± 0.047\n"
     ]
    }
   ],
   "source": [
    "def print_stats(results_dict):\n",
    "    stats = {}\n",
    "    for method, data in results_dict.items():\n",
    "        # Get the values for mix_rate 0.0 since that's what we have in the results\n",
    "        values = data\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        stats[method] = {'mean': mean, 'std': std}\n",
    "    return stats\n",
    "\n",
    "print(\"Average Accuracy:\")\n",
    "for method, stats in print_stats(results).items():\n",
    "    print(f\"{method:10}: {stats['mean']:.3f} ± {stats['std']:.3f}\")\n",
    "\n",
    "print(\"\\nAlternative Accuracy:\")\n",
    "for method, stats in print_stats(results_alt).items():\n",
    "    print(f\"{method:10}: {stats['mean']:.3f} ± {stats['std']:.3f}\")\n",
    "\n",
    "print(\"\\nWorst-Group Accuracy:\")\n",
    "for method, stats in print_stats(results_worst).items():\n",
    "    print(f\"{method:10}: {stats['mean']:.3f} ± {stats['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'DivDis': [0.9373489618301392, 0.9344149231910706, 0.9152571558952332],\n",
       "  'TopK 0.1': [0.9036934971809387, 0.900414228439331, 0.8874697685241699],\n",
       "  'TopK 0.5': [0.8929927349090576, 0.8962720036506653, 0.9285467863082886],\n",
       "  'DBAT': [0.8555402159690857, 0.8765964508056641, 0.8669313192367554]},\n",
       " {'DivDis': [0.7402485609054565, 0.8034173250198364, 0.867966890335083],\n",
       "  'TopK 0.1': [0.7323092818260193, 0.7556092739105225, 0.7093545198440552],\n",
       "  'TopK 0.5': [0.9221608638763428, 0.9299275279045105, 0.9244045615196228],\n",
       "  'DBAT': [0.7138419151306152, 0.714359700679779, 0.6939938068389893]},\n",
       " {'DivDis': [0.7663551568984985, 0.777258574962616, 0.672897219657898],\n",
       "  'TopK 0.1': [0.781931459903717, 0.7303769588470459, 0.6417445540428162],\n",
       "  'TopK 0.5': [0.8302180767059326, 0.7305296063423157, 0.8052959442138672],\n",
       "  'DBAT': [0.6536585092544556, 0.6106430292129517, 0.6097561120986938]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, results_alt, results_worst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
