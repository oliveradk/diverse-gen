{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/nas/ucb/oliveradk/diverse-gen/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from diverse_gen.utils.exp_utils import get_conf_dir\n",
    "from diverse_gen.utils.proc_data_utils import get_exp_metrics, get_max_acc, get_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"output/incomplete_waterbirds/main\"\n",
    "GROUP_LABELS_DIR = \"output/incomplete_waterbirds/group_labels\"\n",
    "RESULTS_DIR = \"results/incomplete_waterbirds\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = [\n",
    "    \"TopK_0.1\", \n",
    "    \"TopK_0.5\", \n",
    "    \"ERM\", \n",
    "    \"DBAT\", \n",
    "    \"DivDis\"\n",
    "]\n",
    "SEEDS = [1, 2, 3]\n",
    "DATASETS = [\"waterbirds\"]\n",
    "MIX_RATES = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(acc_metric: str, dir_path: str, methods: list[str], head_idx: Optional[int] = None):\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    for dataset, method in itertools.product(DATASETS, methods):\n",
    "        exp_dirs = []\n",
    "        mix_rates = MIX_RATES\n",
    "        model_selection = \"val_loss\" if method != \"DBAT\" else \"val_source_loss\"\n",
    "        for mix_rate, seed in itertools.product(mix_rates, SEEDS):\n",
    "            exp_dirs.append(get_conf_dir((dataset, method, mix_rate, seed), dir_path))\n",
    "        perf_source_acc = dataset == \"toy_grid\"\n",
    "        try: \n",
    "            results[dataset][method] = get_acc_results(\n",
    "                exp_dirs=exp_dirs, acc_metric=acc_metric, model_selection=model_selection, \n",
    "                perf_source_acc=perf_source_acc, verbose=True, head_idx=head_idx\n",
    "            )\n",
    "        except Exception as e: \n",
    "            print(f\"Error getting results for {dataset} {method}: {e}\")\n",
    "            raise e\n",
    "    # recusrively convert default dict to dict\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(results: dict[dict]): \n",
    "    # Create a list to store flattened data\n",
    "    flattened_data = []\n",
    "\n",
    "    # Iterate through the nested structure\n",
    "    for dataset, method_dict in results.items():\n",
    "        for method, mix_rate_dict in method_dict.items():\n",
    "            for mix_rate, accuracies in mix_rate_dict.items():\n",
    "                # For each accuracy value in the list\n",
    "                for acc in accuracies:\n",
    "                    flattened_data.append({\n",
    "                        'Dataset': dataset,\n",
    "                        'Method': method,\n",
    "                        'Mix_Rate': mix_rate,\n",
    "                        'Accuracy': acc\n",
    "                    })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    df.sort_values(['Dataset', 'Method', 'Mix_Rate'], inplace=True)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "metrics = json.load(open(\"output/incomplete_waterbirds/group_labels/waterbirds_DivDis_None/1/metrics.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = to_df(get_results(\"test_acc\", dir_path=MAIN_DIR, methods=METHODS))\n",
    "alt_acc_df = to_df(get_results(\"test_acc_alt\", dir_path=MAIN_DIR, methods=METHODS))\n",
    "worst_acc_df = to_df(get_results(\"test_worst_acc\", dir_path=MAIN_DIR, methods=METHODS))\n",
    "\n",
    "acc_df.to_csv(os.path.join(RESULTS_DIR, \"acc_df.csv\"), index=False)\n",
    "alt_acc_df.to_csv(os.path.join(RESULTS_DIR, \"alt_acc_df.csv\"), index=False)\n",
    "worst_acc_df.to_csv(os.path.join(RESULTS_DIR, \"worst_acc_df.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS_NO_ERM = list(set(METHODS) - {\"ERM\"})\n",
    "acc_gl_df = to_df(get_results(\n",
    "    \"test_acc\", dir_path=GROUP_LABELS_DIR, methods=METHODS_NO_ERM, head_idx=0\n",
    "))\n",
    "alt_acc_gl_df = to_df(get_results(\n",
    "    \"test_acc\", dir_path=GROUP_LABELS_DIR, methods=METHODS_NO_ERM, head_idx=1\n",
    "))\n",
    "worst_acc_gl_df = to_df(get_results(\n",
    "    \"test_worst_acc\", dir_path=GROUP_LABELS_DIR, methods=METHODS_NO_ERM, head_idx=0\n",
    "))\n",
    "\n",
    "acc_gl_df.to_csv(os.path.join(RESULTS_DIR, \"acc_gl_df.csv\"), index=False)\n",
    "alt_acc_gl_df.to_csv(os.path.join(RESULTS_DIR, \"alt_acc_gl_df.csv\"), index=False)\n",
    "worst_acc_gl_df.to_csv(os.path.join(RESULTS_DIR, \"worst_acc_gl_df.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = json.load(open(\"output/incomplete_waterbirds/group_labels/waterbirds_DivDis_None/1/metrics.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with all metrics\n",
    "def print_latex_table(acc_df, alt_acc_df, worst_acc_df):\n",
    "    df = pd.DataFrame({\n",
    "        'Method': [],\n",
    "        'Average Acc': [],\n",
    "        'Alternative Acc': [],\n",
    "        'Worst-Group Acc': []\n",
    "    })\n",
    "\n",
    "    for method in METHODS:\n",
    "        avg_acc = f\"{acc_df[acc_df['Method'] == method]['Accuracy'].mean()*100:.1f} ± {acc_df[acc_df['Method'] == method]['Accuracy'].std()*100:.1f}\"\n",
    "        alt_acc = f\"{alt_acc_df[alt_acc_df['Method'] == method]['Accuracy'].mean()*100:.1f} ± {alt_acc_df[alt_acc_df['Method'] == method]['Accuracy'].std()*100:.1f}\"\n",
    "        worst_acc = f\"{worst_acc_df[worst_acc_df['Method'] == method]['Accuracy'].mean()*100:.1f} ± {worst_acc_df[worst_acc_df['Method'] == method]['Accuracy'].std()*100:.1f}\"\n",
    "        \n",
    "        df = pd.concat([df, pd.DataFrame({\n",
    "            'Method': [method.replace(\"_\", \" \")],\n",
    "            'Average Acc': [avg_acc],\n",
    "            'Alternative Acc': [alt_acc],\n",
    "            'Worst-Group Acc': [worst_acc]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "    # Print LaTeX table\n",
    "    print(df.to_latex(index=False, escape=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Method & Average Acc & Alternative Acc & Worst-Group Acc \\\\\n",
      "\\midrule\n",
      "TopK 0.1 & 88.9 ± 1.3 & 74.0 ± 3.6 & 53.9 ± 12.8 \\\\\n",
      "TopK 0.5 & 92.8 ± 0.4 & 90.7 ± 1.1 & 70.9 ± 2.3 \\\\\n",
      "ERM & 84.7 ± 2.9 & 63.1 ± 2.9 & 51.1 ± 10.2 \\\\\n",
      "DBAT & 79.8 ± 0.9 & 70.2 ± 2.2 & 61.5 ± 3.7 \\\\\n",
      "DivDis & 91.0 ± 3.2 & 72.9 ± 1.3 & 67.7 ± 8.1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_latex_table(acc_df, alt_acc_df, worst_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Method & Average Acc & Alternative Acc & Worst-Group Acc \\\\\n",
      "\\midrule\n",
      "TopK 0.1 & 87.9 ± 0.7 & 89.7 ± 2.6 & 54.3 ± 8.8 \\\\\n",
      "TopK 0.5 & 92.5 ± 0.9 & 94.8 ± 0.5 & 75.6 ± 7.0 \\\\\n",
      "ERM & nan ± nan & nan ± nan & nan ± nan \\\\\n",
      "DBAT & 90.7 ± 1.8 & 91.6 ± 1.2 & 67.5 ± 9.0 \\\\\n",
      "DivDis & 91.7 ± 1.4 & 92.4 ± 0.9 & 66.9 ± 3.2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_latex_table(acc_gl_df, alt_acc_gl_df, worst_acc_gl_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
