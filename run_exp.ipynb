{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random as rnd\n",
    "from typing import Optional, Callable\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as  pd\n",
    "import torchvision.utils as vision_utils\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from losses.divdis import DivDisLoss \n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.conf import ConfLoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.smooth_top_loss import SmoothTopLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from models.backbone import MultiHeadBackbone\n",
    "from models.multi_model import MultiNetModel, freeze_heads\n",
    "from models.lenet import LeNet\n",
    "\n",
    "from spurious_datasets.cifar_mnist import get_cifar_mnist_datasets\n",
    "from spurious_datasets.fmnist_mnist import get_fmnist_mnist_datasets\n",
    "from spurious_datasets.toy_grid import get_toy_grid_datasets\n",
    "from spurious_datasets.waterbirds import get_waterbirds_datasets\n",
    "from spurious_datasets.multi_nli import get_multi_nli_datasets\n",
    "from spurious_datasets.celebA import get_celebA_datasets\n",
    "from config import Config, post_init\n",
    "from utils.utils import to_device, batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config(\n",
    "    seed=45,\n",
    "    dataset=\"celebA-0\",\n",
    "    loss_type=LossType.TOPK,\n",
    "    batch_size=16,\n",
    "    target_batch_size=32,\n",
    "    epochs=10,\n",
    "    heads=2,\n",
    "    binary=True, # True\n",
    "    model=\"Resnet50\",\n",
    "    shared_backbone=True,\n",
    "    source_weight=1.0,\n",
    "    aux_weight=1.0,\n",
    "    source_mix_rate=0.0,\n",
    "    source_l_01_mix_rate=None,\n",
    "    source_l_10_mix_rate=None,\n",
    "    mix_rate=0.5,\n",
    "    aggregate_mix_rate=False,#TODO: True\n",
    "    l_01_mix_rate=None,\n",
    "    l_10_mix_rate=None,\n",
    "    mix_rate_lower_bound=0.5,\n",
    "    l_01_mix_rate_lower_bound=None, # 0.4\n",
    "    l_10_mix_rate_lower_bound=None, # 0.1\n",
    "    all_unlabeled=False,\n",
    "    inbalance_ratio=False,\n",
    "    lr=1e-4, # 1e-3 maybe?\n",
    "    weight_decay=1e-4,\n",
    "    lr_scheduler=None,\n",
    "    num_cycles=0.5,\n",
    "    frac_warmup=0.05,\n",
    "    max_length=256,\n",
    "    num_workers=6,\n",
    "    freeze_heads=False,\n",
    "    head_1_epochs=5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"),\n",
    "    exp_dir=f\"output/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n",
    "    plot_activations=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # toy grid configs \n",
    "# if conf.dataset == \"toy_grid\":\n",
    "#     conf.model = \"toy_model\"\n",
    "#     conf.epochs = 100\n",
    "if conf.model == \"ClipViT\":\n",
    "    # conf.epochs = 5\n",
    "    conf.lr = 1e-5\n",
    "# Resnet50 Configs\n",
    "if conf.model == \"Resnet50\":\n",
    "    conf.lr = 1e-4 # probably too high, should be 1e-4\n",
    "if conf.dataset == \"multi_nli\":\n",
    "    conf.model = \"bert\"\n",
    "    conf.lr = 1e-5\n",
    "    conf.lr_scheduler = \"cosine\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get config overrides if runnign from command line\n",
    "overrride_keys = []\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    overrides = OmegaConf.from_cli(sys.argv[1:])\n",
    "    overrride_keys = overrides.keys()\n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), overrides)\n",
    "    conf = Config(**conf_dict)\n",
    "post_init(conf, overrride_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory from config\n",
    "from dataclasses import asdict\n",
    "exp_dir = conf.exp_dir\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "# save full config to exp_dir\n",
    "with open(f\"{exp_dir}/config.yaml\", \"w\") as f:\n",
    "    OmegaConf.save(config=conf, f=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(conf.seed)\n",
    "np.random.seed(conf.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transform = None\n",
    "pad_sides = False\n",
    "tokenizer = None\n",
    "if conf.model == \"Resnet50\":\n",
    "    from torchvision import models\n",
    "    from torchvision.models.resnet import ResNet50_Weights\n",
    "    resnet_builder = lambda: models.resnet50(weights=ResNet50_Weights.DEFAULT)  \n",
    "    model_builder = lambda: torch.nn.Sequential(*list(resnet_builder().children())[:-1])\n",
    "    resnet_50_transforms = ResNet50_Weights.DEFAULT.transforms()\n",
    "    model_transform = transforms.Compose([\n",
    "        transforms.Resize(resnet_50_transforms.resize_size * 2, interpolation=resnet_50_transforms.interpolation),\n",
    "        transforms.CenterCrop(resnet_50_transforms.crop_size),\n",
    "        transforms.Normalize(mean=resnet_50_transforms.mean, std=resnet_50_transforms.std)\n",
    "    ])\n",
    "    pad_sides = True\n",
    "    feature_dim = 2048\n",
    "elif conf.model == \"ClipViT\":\n",
    "    # from models.clip_vit import ClipViT\n",
    "    # model_builder = lambda: ClipViT()\n",
    "    # feature_dim = 768\n",
    "    # input_size = 96\n",
    "    # model_transform = transforms.Compose([\n",
    "    #     transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "    # ])\n",
    "    import clip \n",
    "    preprocess = clip.clip._transform(224)\n",
    "    clip_builder = lambda: clip.load('ViT-B/32', device='cpu')[0]\n",
    "    model_builder = lambda: clip_builder().visual\n",
    "    model_transform = transforms.Compose([\n",
    "        preprocess.transforms[0],\n",
    "        preprocess.transforms[1],\n",
    "        preprocess.transforms[4]\n",
    "    ])\n",
    "    feature_dim = 512\n",
    "    pad_sides = True\n",
    "elif conf.model == \"bert\":\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    from models.hf_wrapper import HFWrapper\n",
    "    bert_builder = lambda: BertModel.from_pretrained('bert-base-uncased')\n",
    "    model_builder = lambda: HFWrapper(bert_builder())\n",
    "    feature_dim = 768\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "elif conf.model == \"toy_model\":\n",
    "    model_builder = lambda: nn.Sequential(\n",
    "        nn.Linear(2, 40), nn.ReLU(), nn.Linear(40, 40), nn.ReLU()\n",
    "    )\n",
    "    feature_dim = 40\n",
    "elif conf.model == \"LeNet\":\n",
    "    from models.lenet import LeNet\n",
    "    from functools import partial\n",
    "    model_builder = lambda: partial(LeNet, num_classes=1, dropout_p=0.0)\n",
    "    feature_dim = 256\n",
    "else: \n",
    "    raise ValueError(f\"Model {conf.model} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = None\n",
    "alt_index = 1\n",
    "classes = 2\n",
    "is_img = True\n",
    "\n",
    "if conf.dataset == \"toy_grid\":\n",
    "    source_train, source_val, target_train, target_val, target_test = get_toy_grid_datasets(\n",
    "        source_mix_rate_0_1=conf.source_l_01_mix_rate, \n",
    "        source_mix_rate_1_0=conf.source_l_10_mix_rate, \n",
    "        target_mix_rate_0_1=conf.l_01_mix_rate, \n",
    "        target_mix_rate_1_0=conf.l_10_mix_rate, \n",
    "    )\n",
    "elif conf.dataset == \"cifar_mnist\":\n",
    "    source_train, source_val, target_train, target_val, target_test = get_cifar_mnist_datasets(\n",
    "        source_mix_rate_0_1=conf.source_l_01_mix_rate, \n",
    "        source_mix_rate_1_0=conf.source_l_10_mix_rate, \n",
    "        target_mix_rate_0_1=conf.l_01_mix_rate, \n",
    "        target_mix_rate_1_0=conf.l_10_mix_rate, \n",
    "        transform=model_transform, \n",
    "        pad_sides=pad_sides\n",
    "    )\n",
    "\n",
    "elif conf.dataset == \"fmnist_mnist\":\n",
    "    source_train, source_val, target_train, target_val, target_test = get_fmnist_mnist_datasets(\n",
    "        source_mix_rate_0_1=conf.source_l_01_mix_rate, \n",
    "        source_mix_rate_1_0=conf.source_l_10_mix_rate, \n",
    "        target_mix_rate_0_1=conf.l_01_mix_rate, \n",
    "        target_mix_rate_1_0=conf.l_10_mix_rate, \n",
    "        transform=model_transform, \n",
    "        pad_sides=pad_sides\n",
    "    )\n",
    "elif conf.dataset == \"waterbirds\":\n",
    "    source_train, source_val, target_train, target_val, target_test = get_waterbirds_datasets(\n",
    "        mix_rate=conf.mix_rate, \n",
    "        transform=model_transform, \n",
    "    )\n",
    "    collate_fn = source_train.dataset.collate\n",
    "    alt_index = 0\n",
    "elif conf.dataset.startswith(\"celebA\"):\n",
    "    if conf.dataset == \"celebA-0\":\n",
    "        gt_feat = \"Male\"\n",
    "        spur_feat = \"Blond_Hair\"\n",
    "        inv_spur_feat = True\n",
    "    elif conf.dataset == \"celebA-1\":\n",
    "        gt_feat = \"Mouth_Slightly_Open\"\n",
    "        spur_feat = \"Wearing_Lipstick\"\n",
    "        inv_spur_feat = False\n",
    "    elif conf.dataset == \"celebA-2\":\n",
    "        gt_feat = \"Wavy_Hair\"\n",
    "        spur_feat = \"High_Cheekbones\"\n",
    "        inv_spur_feat = False\n",
    "    else: \n",
    "        raise ValueError(f\"Dataset {conf.dataset} not supported\")\n",
    "    source_train, source_val, target_train, target_val, target_test = get_celebA_datasets(\n",
    "        mix_rate=conf.mix_rate, \n",
    "        transform=model_transform, \n",
    "        gt_feat=gt_feat,\n",
    "        spur_feat=spur_feat,\n",
    "        inv_spur_feat=inv_spur_feat\n",
    "    )\n",
    "elif conf.dataset == \"multi_nli\":\n",
    "    source_train, source_val, target_train, target_val, target_test = get_multi_nli_datasets(\n",
    "        mix_rate=conf.mix_rate,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=conf.max_length, \n",
    "        dataset_length=None\n",
    "    )\n",
    "    is_img = False\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Dataset {conf.dataset} not supported\")\n",
    "\n",
    "# if classes == 2 and conf.binary:\n",
    "#     classes = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot image \n",
    "img, y, gl = source_train[0]\n",
    "# pad \n",
    "# to PIL image \n",
    "\n",
    "# img = transforms.ToPILImage()(img)\n",
    "# img\n",
    "if is_img and img.dim() == 3 and is_notebook():\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    # show without axis \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target train images with vision_utils.make_grid\n",
    "if is_img and img.dim() == 3 and is_notebook():\n",
    "    cifar_mnist_grid = torch.stack([target_train[i][0] for i in range(20)])\n",
    "    grid_img = vision_utils.make_grid(cifar_mnist_grid, nrow=10, normalize=True, padding=1)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders \n",
    "source_train_loader = DataLoader(source_train, batch_size=conf.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=conf.num_workers)\n",
    "source_val_loader = DataLoader(source_val, batch_size=conf.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=conf.num_workers)\n",
    "target_train_loader = DataLoader(target_train, batch_size=conf.target_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=conf.num_workers)\n",
    "target_val_loader = DataLoader(target_val, batch_size=conf.target_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=conf.num_workers)\n",
    "target_test_loader = DataLoader(target_test, batch_size=conf.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=conf.num_workers)\n",
    "\n",
    "# classifiers\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "if conf.shared_backbone:\n",
    "    net = MultiHeadBackbone(model_builder(), conf.heads, feature_dim, classes if not conf.binary else 1)\n",
    "else:\n",
    "    print(\"warning, not using shared backbone untested\")\n",
    "    net = MultiNetModel(heads=conf.heads, model_builder=model_builder, feature_dim=feature_dim)\n",
    "net = net.to(conf.device)\n",
    "\n",
    "# optimizer\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "num_steps = conf.epochs * len(source_train_loader)\n",
    "if conf.lr_scheduler == \"cosine\":       \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        opt, \n",
    "        num_warmup_steps=num_steps * conf.frac_warmup, \n",
    "        num_training_steps=num_steps, \n",
    "        num_cycles=conf.num_cycles\n",
    "    )\n",
    "else: \n",
    "    # constant learning rate\n",
    "    scheduler = None\n",
    "\n",
    "# loss function\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=conf.heads)\n",
    "elif conf.loss_type == LossType.DBAT:\n",
    "    loss_fn = DBatLoss(heads=conf.heads)\n",
    "elif conf.loss_type == LossType.CONF:\n",
    "    loss_fn = ConfLoss()\n",
    "elif conf.loss_type == LossType.SMOOTH:\n",
    "    loss_fn = SmoothTopLoss(\n",
    "        criterion=partial(F.binary_cross_entropy_with_logits, reduction='none'), \n",
    "        device=conf.device\n",
    "    )\n",
    "elif conf.loss_type in [LossType.TOPK, LossType.EXP, LossType.PROB]:\n",
    "    if conf.aggregate_mix_rate:\n",
    "        mix_rate = conf.mix_rate_lower_bound \n",
    "        group_mix_rates = None\n",
    "    else:\n",
    "        mix_rate = None \n",
    "        group_mix_rates = {(0, 1): conf.l_01_mix_rate_lower_bound, (1, 0): conf.l_10_mix_rate_lower_bound}\n",
    "    loss_fn = ACELoss(\n",
    "        heads=conf.heads, \n",
    "        classes=classes,\n",
    "        binary=conf.binary,\n",
    "        mode=conf.loss_type.value, \n",
    "        inbalance_ratio=conf.inbalance_ratio,\n",
    "        mix_rate=mix_rate,\n",
    "        group_mix_rates=group_mix_rates,\n",
    "        all_unlabeled=conf.all_unlabeled,\n",
    "        device=conf.device\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Loss type {conf.loss_type} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accs(logits: torch.Tensor, gl: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        acc = torch.zeros(conf.heads)\n",
    "        acc_alt = torch.zeros(conf.heads)\n",
    "        for i in range(conf.heads):\n",
    "            acc[i] += ((logits[:, i] > 0) == gl[:, 1-alt_index].flatten()).to(torch.float32).mean().item()\n",
    "            acc_alt[i] += ((logits[:, i] > 0) == gl[:, alt_index].flatten()).to(torch.float32).mean().item()\n",
    "    return acc, acc_alt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_and_labels(model: nn.Module, loader: DataLoader):\n",
    "    activations = []\n",
    "    labels = []\n",
    "    model = model_builder()\n",
    "    model = model.to(conf.device)\n",
    "    for x, y, gl in tqdm(loader):\n",
    "        x, y, gl = x.to(conf.device), y.to(conf.device), gl.to(conf.device)\n",
    "        acts = model(x)\n",
    "        activations.append((acts.detach().cpu()))\n",
    "        labels.append(gl)\n",
    "    activations = torch.cat(activations, dim=0).squeeze()\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    labels = labels.squeeze()\n",
    "    return activations, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        activations, labels = get_acts_and_labels(model, loader)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(activations)\n",
    "    activations_pca = pca.transform(activations)\n",
    "\n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot first label\n",
    "    scatter1 = ax1.scatter(activations_pca[:, 0], activations_pca[:, 1], c=labels[:, 0].to('cpu'), cmap=\"viridis\")\n",
    "    ax1.set_title('Label 0')\n",
    "\n",
    "    # Plot second label\n",
    "    scatter2 = ax2.scatter(activations_pca[:, 0], activations_pca[:, 1], c=labels[:, 1].to('cpu'), cmap=\"viridis\")\n",
    "    ax2.set_title('Label 1')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data using first two principle componets of final layer activations\n",
    "if is_notebook() and conf.plot_activations:\n",
    "    model = model_builder()\n",
    "    model = model.to(conf.device)\n",
    "    activations, labels = get_acts_and_labels(model, target_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "if is_notebook() and conf.plot_activations:\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(activations)\n",
    "    activations_pca = pca.transform(activations)\n",
    "\n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot first label\n",
    "    scatter1 = ax1.scatter(activations_pca[:, 0], activations_pca[:, 1], c=labels[:, 0].to('cpu'), cmap=\"viridis\")\n",
    "    ax1.set_title('Label 0')\n",
    "\n",
    "    # Plot second label\n",
    "    scatter2 = ax2.scatter(activations_pca[:, 0], activations_pca[:, 1], c=labels[:, 1].to('cpu'), cmap=\"viridis\")\n",
    "    ax2.set_title('Label 1')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{exp_dir}/activations_pretrain.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook() and conf.plot_activations:\n",
    "    group_labels = labels[:, 0] * 2 + labels[:, 1]\n",
    "    plt.scatter(activations_pca[:, 0], activations_pca[:, 1], c=group_labels.to('cpu'), cmap=\"viridis\")\n",
    "    plt.title(\"Group labels\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "if is_notebook() and conf.plot_activations:\n",
    "    component_range = [2**i for i in range(1, 9)]\n",
    "    component_range = [i for i in component_range if i <= feature_dim]\n",
    "    n_components_accs = []\n",
    "    for n_components in tqdm(component_range):\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(activations)\n",
    "        activations_pca = pca.transform(activations)\n",
    "        # fit probe \n",
    "        lr = LogisticRegression(max_iter=1000)\n",
    "        lr.fit(activations_pca, labels[:, 0].to('cpu').numpy())\n",
    "        acc = lr.score(activations_pca, labels[:, 0].to('cpu').numpy())\n",
    "        n_components_accs.append(acc)\n",
    "    plt.plot(component_range, n_components_accs, label=\"accuracy\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit linear probe \n",
    "if is_notebook() and conf.plot_activations:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression(max_iter=10000)\n",
    "    lr.fit(activations.to('cpu').numpy(), labels[:, 0].to('cpu').numpy())\n",
    "    # get accuracy \n",
    "    acc = lr.score(activations.to('cpu').numpy(), labels[:, 0].to('cpu').numpy())\n",
    "    print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook() and conf.plot_activations:\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    # Second 3D plot for group labels\n",
    "    ax3 = fig.add_subplot(121, projection='3d')\n",
    "    scatter3 = ax3.scatter(activations_pca[:, 0], activations_pca[:, 1], activations_pca[:,2], \n",
    "                        c=group_labels.to('cpu'), cmap=\"viridis\")\n",
    "    ax3.view_init(25, 210, 0)\n",
    "    ax3.set_title('Group labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_notebook() and conf.plot_activations:\n",
    "    fig = plot_activations(net.backbone, target_test_loader)\n",
    "    fig.savefig(f\"{exp_dir}/activations_pretrain.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_src_losses(logits, y, binary):\n",
    "    logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "    if binary:\n",
    "        losses = [F.binary_cross_entropy_with_logits(logit.squeeze(), y.to(torch.float32)) for logit in logits_chunked]\n",
    "    else:\n",
    "        losses = [F.cross_entropy(logit.squeeze(), y.to(torch.long)) for logit in logits_chunked]\n",
    "    return losses\n",
    "\n",
    "def compute_corrects(logits: torch.Tensor, head: int, y: torch.Tensor, binary: bool):\n",
    "    if binary:\n",
    "        return ((logits[:, head] > 0) == y.flatten()).sum().item()\n",
    "    else:\n",
    "        logits = logits.view(logits.size(0), conf.heads, -1)\n",
    "        return (logits[:, head].argmax(dim=-1) == y).sum().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "target_iter = iter(target_train_loader)\n",
    "if conf.freeze_heads:\n",
    "    # freeze second head \n",
    "    net.freeze_head(1)\n",
    "for epoch in range(conf.epochs):\n",
    "    for batch_idx, (x, y, gl) in tqdm(enumerate(source_train_loader), desc=\"Source train\", total=len(source_train_loader)):\n",
    "        x, y, gl = to_device(x, y, gl, conf.device)\n",
    "        if conf.freeze_heads and epoch == conf.head_1_epochs:\n",
    "            net.unfreeze_head(1)\n",
    "            net.freeze_head(0)\n",
    "        logits = net(x)\n",
    "        losses = compute_src_losses(logits, y, conf.binary)\n",
    "        xent = sum(losses)\n",
    "        # target loss \n",
    "        try: \n",
    "            target_x, target_y, target_gl = next(target_iter)\n",
    "        except StopIteration:\n",
    "            target_iter = iter(target_train_loader)\n",
    "            target_x, target_y, target_gl = next(target_iter)\n",
    "        target_x, target_y, target_gl = to_device(target_x, target_y, target_gl, conf.device)\n",
    "        target_logits = net(target_x)\n",
    "        repulsion_loss = loss_fn(target_logits)\n",
    "        if conf.freeze_heads and epoch < conf.head_1_epochs:\n",
    "            repulsion_loss = torch.tensor(0.0, device=conf.device)\n",
    "        # full loss \n",
    "        full_loss = conf.source_weight * xent + conf.aux_weight * repulsion_loss\n",
    "        opt.zero_grad()\n",
    "        full_loss.backward()\n",
    "        opt.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        metrics[f\"xent\"].append(xent.item())\n",
    "        metrics[f\"repulsion_loss\"].append(repulsion_loss.item())\n",
    "    # Compute loss on target validation set (used for model selection)\n",
    "    # and aggregate metrics over the entire test set (should not really be using)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        net.eval()\n",
    "        # compute repulsion loss on target validation set (used for model selection)\n",
    "        repulsion_losses_val = []\n",
    "        weighted_repulsion_losses_val = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, gl in tqdm(target_val_loader, desc=\"Target val\"):\n",
    "                x, y, gl = to_device(x, y, gl, conf.device)\n",
    "                logits_val = net(x)\n",
    "                repulsion_loss_val = loss_fn(logits_val)\n",
    "                repulsion_losses_val.append(repulsion_loss_val.item())\n",
    "                weighted_repulsion_losses_val.append(conf.aux_weight * repulsion_loss_val.item())\n",
    "        metrics[f\"target_val_repulsion_loss\"].append(np.mean(repulsion_losses_val))\n",
    "        metrics[f\"target_val_weighted_repulsion_loss\"].append(np.mean(weighted_repulsion_losses_val))\n",
    "        # compute xent on source validation set\n",
    "        xent_val = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, gl in tqdm(source_val_loader, desc=\"Source val\"):\n",
    "                x, y, gl = to_device(x, y, gl, conf.device)\n",
    "                logits_val = net(x)\n",
    "                losses_val = compute_src_losses(logits_val, y, conf.binary)\n",
    "                xent_val.append(sum(losses_val).item())\n",
    "        metrics[f\"source_val_xent\"].append(np.mean(xent_val))\n",
    "        metrics[f\"val_loss\"].append(np.mean(repulsion_losses_val) + np.mean(xent_val))\n",
    "        metrics[f\"val_weighted_loss\"].append(np.mean(weighted_repulsion_losses_val) + np.mean(xent_val))\n",
    "\n",
    "        # compute accuracy over target test set (used to evaluate actual performance)\n",
    "        total_correct = torch.zeros(conf.heads)\n",
    "        total_correct_alt = torch.zeros(conf.heads)\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y, test_gl in target_test_loader:\n",
    "                test_x, test_y, test_gl = to_device(test_x, test_y, test_gl, conf.device)\n",
    "                test_logits = net(test_x)\n",
    "                assert test_logits.shape == (batch_size(test_x), conf.heads * (1 if conf.binary else classes))\n",
    "                total_samples += test_y.size(0)\n",
    "                \n",
    "                for i in range(conf.heads):\n",
    "                    total_correct[i] += compute_corrects(test_logits, i, test_y, conf.binary)\n",
    "                    total_correct_alt[i] += compute_corrects(test_logits, i, test_gl[:, alt_index], conf.binary)\n",
    "        \n",
    "        for i in range(conf.heads):\n",
    "            metrics[f\"epoch_acc_{i}\"].append((total_correct[i] / total_samples).item())\n",
    "            metrics[f\"epoch_acc_{i}_alt\"].append((total_correct_alt[i] / total_samples).item())\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Test Accuracies:\")\n",
    "        # print validation losses\n",
    "        print(f\"Target val repulsion loss: {metrics[f'target_val_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Target val weighted repulsion loss: {metrics[f'target_val_weighted_repulsion_loss'][-1]:.4f}\")\n",
    "        print(f\"Source val xent: {metrics[f'source_val_xent'][-1]:.4f}\")\n",
    "        print(f\"val loss: {metrics[f'val_loss'][-1]:.4f}\")\n",
    "        print(f\"val weighted loss: {metrics[f'val_weighted_loss'][-1]:.4f}\")\n",
    "        for i in range(conf.heads):\n",
    "            print(f\"Head {i}: {metrics[f'epoch_acc_{i}'][-1]:.4f}, Alt: {metrics[f'epoch_acc_{i}_alt'][-1]:.4f}\")\n",
    "        \n",
    "        # plot activations \n",
    "        if conf.plot_activations:   \n",
    "            fig = plot_activations(net.backbone, target_test_loader)\n",
    "            fig.savefig(f\"{exp_dir}/activations_{epoch}.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        net.train()\n",
    "\n",
    "metrics = dict(metrics)\n",
    "# save metrics \n",
    "import json \n",
    "with open(f\"{exp_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"xent\"], label=\"xent\", color=\"red\")\n",
    "plt.plot(metrics[\"repulsion_loss\"], label=\"repulsion_loss\", color=\"blue\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print loss\n",
    "# plt.plot(metrics[\"xent\"], label=\"xent\", color=\"pink\")\n",
    "# plt.plot(metrics[\"repulsion_loss\"], label=\"repulsion_loss\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"source_val_xent\"], label=\"source_val_xent\", color=\"red\")\n",
    "plt.plot(metrics[\"target_val_weighted_repulsion_loss\"], label=\"target_val_repulsion_loss\", color=\"blue\")\n",
    "plt.plot(metrics[\"val_weighted_loss\"], label=\"val_loss\", color=\"green\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "if not is_notebook():\n",
    "    plt.close()\n",
    "plt.savefig(f\"{exp_dir}/val_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics\n",
    "# plot acc_0 and acc_1 and acc_0_alt and acc_1_alt\n",
    "plt.plot(metrics[\"epoch_acc_0\"], label=\"acc_0\", color=\"blue\")\n",
    "plt.plot(metrics[\"epoch_acc_1\"], label=\"acc_1\", color=\"green\")\n",
    "plt.plot(metrics[\"epoch_acc_0_alt\"], label=\"acc_0_alt\", color=\"lightblue\")\n",
    "plt.plot(metrics[\"epoch_acc_1_alt\"], label=\"acc_1_alt\", color=\"lightgreen\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f\"{exp_dir}/acc.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index of minimum val_weighted_loss, target_val_weighted_repulsion_loss \n",
    "min_val_weighted_loss_idx = np.argmin(metrics[\"val_weighted_loss\"])\n",
    "min_target_val_weighted_repulsion_loss_idx = np.argmin(metrics[\"target_val_weighted_repulsion_loss\"])\n",
    "# get maximum acc (max of max(acc_0, acc_1))\n",
    "accs = np.maximum(np.array(metrics[\"epoch_acc_0\"]), np.array(metrics[\"epoch_acc_1\"]))\n",
    "max_acc_idx = np.argmax(accs)\n",
    "print(f\"max_acc_idx: {max_acc_idx}\")\n",
    "print(f\"min_val_weighted_loss_idx: {min_val_weighted_loss_idx}\")\n",
    "print(f\"min_target_val_weighted_repulsion_loss_idx: {min_target_val_weighted_repulsion_loss_idx}\")\n",
    "# get accs for min val_weighted_loss and min target_val_weighted_repulsion_loss \n",
    "val_weighted_loss_acc = accs[min_val_weighted_loss_idx]\n",
    "target_val_weighted_repulsion_loss_acc = accs[min_target_val_weighted_repulsion_loss_idx]\n",
    "max_acc = accs[max_acc_idx]\n",
    "\n",
    "# plot max_acc, val_weighted_loss_acc, target_val_weighted_repulsion_loss_acc as a bar chart \n",
    "plt.bar([\"max\", \"weighted_loss\", \"weighted_repulsion_loss\"], [max_acc, val_weighted_loss_acc, target_val_weighted_repulsion_loss_acc])\n",
    "# show y ticks at every 0.1 \n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.show()\n",
    "plt.savefig(f\"{exp_dir}/max_acc_model_selection.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-gen-KG5DY0Zz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
