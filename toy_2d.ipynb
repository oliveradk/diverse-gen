{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "import matplotlib \n",
    "if not is_notebook():\n",
    "    matplotlib.use('Agg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from typing import Optional, List\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.conf import ConfLoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from toy_data.grid import generate_data, plot_data, sample_minibatch, savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get and log accuracy of model on both label types, in gif only show first, \n",
    "# later, compute curves showing mean accuracy of each loss type acros the mix rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "@dataclass\n",
    "class Config():\n",
    "    seed: int = 45\n",
    "    loss_type: LossType = LossType.DIVDIS\n",
    "    train_size: int = 500 \n",
    "    target_size: int = 500\n",
    "    batch_size: int = 32 \n",
    "    target_batch_size: int = 100\n",
    "    train_iter: int = 10_000\n",
    "    heads: int = 2\n",
    "    source_weight: float = 1.0\n",
    "    aux_weight: float = 1.0\n",
    "    mix_rate: Optional[float] = 0.1\n",
    "    l_01_mix_rate: Optional[float] = None # TODO: geneneralize\n",
    "    l_10_mix_rate: Optional[float] = None\n",
    "    gaussian: bool = True\n",
    "    std: float = 0.01\n",
    "    all_unlabeled: bool = False\n",
    "    mix_rate_lower_bound: Optional[float] = 0.5\n",
    "    inbalance_ratio: Optional[bool] = False\n",
    "    lr: float = 1e-3\n",
    "    make_gifs: bool = True\n",
    "    # dateime exp dir \n",
    "    exp_name: str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def post_init(conf: Config):\n",
    "    if conf.l_01_mix_rate is not None and conf.l_10_mix_rate is None:\n",
    "        conf.l_10_mix_rate = 0.0\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_01_mix_rate\n",
    "        assert conf.mix_rate == conf.l_01_mix_rate\n",
    "    elif conf.l_01_mix_rate is None and conf.l_10_mix_rate is not None:\n",
    "        conf.l_01_mix_rate = 0.0\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_10_mix_rate\n",
    "        assert conf.mix_rate == conf.l_10_mix_rate\n",
    "    elif conf.l_01_mix_rate is not None and conf.l_10_mix_rate is not None:\n",
    "        if conf.mix_rate is None:\n",
    "            conf.mix_rate = conf.l_01_mix_rate + conf.l_10_mix_rate\n",
    "        assert conf.mix_rate == conf.l_01_mix_rate + conf.l_10_mix_rate\n",
    "    else: # both are none \n",
    "        assert conf.mix_rate is not None\n",
    "        conf.l_01_mix_rate = conf.mix_rate / 2\n",
    "        conf.l_10_mix_rate = conf.mix_rate / 2\n",
    "    \n",
    "    if conf.mix_rate_lower_bound is None:\n",
    "        conf.mix_rate_lower_bound = conf.mix_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config \n",
    "conf = Config()\n",
    "#get config overrides if runnign from command line\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), OmegaConf.from_cli(sys.argv[1:]))\n",
    "    conf = Config(**conf_dict)\n",
    "post_init(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(conf.seed)\n",
    "np.random.seed(conf.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_data= generate_data(5000, mix_rate=conf.mix_rate, gaussian=conf.gaussian, std=conf.std)\n",
    "plot_data(ex_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = f\"output/toy_2d/{conf.exp_name}\"\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "temp_fig_dir = f\"{exp_dir}/figures/temp\"\n",
    "os.makedirs(temp_fig_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(f\"{exp_dir}/config.yaml\", \"w\") as f:\n",
    "    OmegaConf.save(conf, f)\n",
    "\n",
    "fig_save_times = sorted(\n",
    "    [1, 2, 3, 4, 8, 16, 32, 64, 120, 128] + [50 * n for n in range(conf.train_iter // 50)]\n",
    ") + [conf.train_iter-1]\n",
    "\n",
    "training_data = generate_data(conf.train_size, train=True, gaussian=conf.gaussian, std=conf.std)\n",
    "held_out_source_data = generate_data(conf.train_size, train=True, gaussian=conf.gaussian, std=conf.std) # used for infer only\n",
    "quad_proportions = [conf.l_01_mix_rate, (1-conf.mix_rate)/2, conf.l_10_mix_rate, (1-conf.mix_rate)/2]\n",
    "target_data = generate_data(conf.target_size, quadrant_proportions=quad_proportions, gaussian=conf.gaussian, std=conf.std)\n",
    "test_data = generate_data(conf.target_size // 2, mix_rate=0.5, gaussian=conf.gaussian, std=conf.std)\n",
    "test_data_alt = generate_data(conf.target_size // 2, mix_rate=0.5, swap_y_meaning=True, gaussian=conf.gaussian, std=conf.std)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 40), nn.ReLU(), nn.Linear(40, 40), nn.ReLU(), nn.Linear(40, conf.heads)\n",
    ")\n",
    "opt = torch.optim.Adam(net.parameters())\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=conf.heads)\n",
    "elif conf.loss_type == LossType.CONF:\n",
    "    loss_fn = ConfLoss(p=0.5)\n",
    "else:\n",
    "    loss_fn = ACELoss(\n",
    "        heads=conf.heads, \n",
    "        mode=conf.loss_type.value, \n",
    "        inbalance_ratio=conf.inbalance_ratio,\n",
    "        l_01_rate=conf.mix_rate_lower_bound / 2, \n",
    "        l_10_rate=conf.mix_rate_lower_bound / 2, \n",
    "        all_unlabeled=conf.all_unlabeled,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_grid(time=\"\", plot_target=True):\n",
    "    N = 20\n",
    "    x = np.linspace(-1, 1, N)\n",
    "    y = np.linspace(-1, 1, N)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    inpt = torch.tensor(np.stack([xv.reshape(-1), yv.reshape(-1)], axis=-1)).float()\n",
    "    with torch.no_grad():\n",
    "        preds = net(inpt).reshape(N, N, conf.heads).sigmoid().cpu()\n",
    "\n",
    "    x, y = training_data\n",
    "    if plot_target:\n",
    "        tar_x, tar_y = target_data\n",
    "        x = np.concatenate([x, tar_x])\n",
    "        y = np.concatenate([y, tar_y])\n",
    "\n",
    "    for i in range(conf.heads):\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.contourf(xv, yv, preds[:, :, i], cmap=\"RdBu\", alpha=0.75)\n",
    "        for g, c in [(0, \"#E7040F\"), (1, \"#00449E\")]:\n",
    "            tr_g = x[y.flatten() == g]\n",
    "            plt.scatter(tr_g[:, 0], tr_g[:, 1], zorder=10, s=10, c=c, edgecolors=\"k\")\n",
    "        plt.xlim(-1.0, 1.0)\n",
    "        ax = plt.gca()\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)\n",
    "        savefig(f\"{temp_fig_dir}/{time}_h{i}\", transparent=True)\n",
    "\n",
    "def plot_head_disagreement(time=\"\"):\n",
    "    N = 20\n",
    "    x = np.linspace(-1, 1, N)\n",
    "    y = np.linspace(-1, 1, N)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    inpt = torch.tensor(np.stack([xv.reshape(-1), yv.reshape(-1)], axis=-1)).float()\n",
    "    with torch.no_grad():\n",
    "        preds = net(inpt).reshape(N, N, conf.heads).sigmoid().cpu()\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.contourf(xv, yv, torch.abs(preds[:, :, 0] - preds[:, :, 1]), cmap=\"RdBu\", alpha=0.75)\n",
    "    plt.xlim(-1.0, 1.0)\n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    savefig(f\"{temp_fig_dir}/{time}_disagreement\", transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "for t in tqdm(range(conf.train_iter), desc=\"Training\"):\n",
    "    x, y = sample_minibatch(training_data, conf.batch_size)\n",
    "    logits = net(x)\n",
    "    logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "    losses = [F.binary_cross_entropy_with_logits(logit, y) for logit in logits_chunked]\n",
    "    xent = sum(losses)\n",
    "\n",
    "    target_x, target_y = sample_minibatch(target_data, conf.target_batch_size)\n",
    "    target_logits = net(target_x)\n",
    "\n",
    "    repulsion_loss_args = []\n",
    "    repulsion_loss = loss_fn(target_logits, *repulsion_loss_args)\n",
    "    full_loss = conf.source_weight * xent + conf.aux_weight * repulsion_loss\n",
    "    opt.zero_grad()\n",
    "    full_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    test_x, test_y = sample_minibatch(test_data, conf.target_batch_size)\n",
    "    with torch.no_grad():\n",
    "        test_logits = net(test_x)\n",
    "    test_x_alt, test_y_alt = sample_minibatch(test_data_alt, conf.target_batch_size)\n",
    "    with torch.no_grad():\n",
    "        test_logits_alt = net(test_x_alt)\n",
    "\n",
    "    for i in range(conf.heads):\n",
    "        corrects_i = (test_logits[:, i] > 0) == test_y.flatten()\n",
    "        acc_i = corrects_i.float().mean()\n",
    "        metrics[f\"acc_{i}\"].append(acc_i.item())\n",
    "        if t % 10 == 0:\n",
    "            print(f\"acc_{i}: {acc_i.item()}\")\n",
    "\n",
    "        corrects_i_alt = (test_logits_alt[:, i] > 0) == test_y_alt.flatten()\n",
    "        acc_i_alt = corrects_i_alt.float().mean()\n",
    "        metrics[f\"acc_{i}_alt\"].append(acc_i_alt.item())\n",
    "        if t % 10 == 0:\n",
    "            print(f\"acc_{i}_alt: {acc_i_alt.item()}\")\n",
    "\n",
    "    metrics[f\"xent\"].append(xent.item())\n",
    "    metrics[f\"repulsion_loss\"].append(repulsion_loss.item())\n",
    "\n",
    "    if conf.make_gifs and t in fig_save_times:\n",
    "        plot_pred_grid(t)\n",
    "        if conf.heads == 2:\n",
    "            plot_head_disagreement(t)\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train single ERM model (for comparison in learning curve)\n",
    "if conf.heads == 2:\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(2, 40), nn.ReLU(), nn.Linear(40, 40), nn.ReLU(), nn.Linear(40, conf.heads)\n",
    "    )\n",
    "    opt = torch.optim.Adam(net.parameters(), )\n",
    "\n",
    "    for t in tqdm(range(conf.train_iter), desc=\"Training ERM\"):\n",
    "        x, y = sample_minibatch(training_data, conf.batch_size)\n",
    "        logits = net(x)\n",
    "        logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "        losses = [F.binary_cross_entropy_with_logits(logit, y) for logit in logits_chunked]\n",
    "        full_loss = sum(losses)\n",
    "        opt.zero_grad()\n",
    "        full_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        test_x, test_y = sample_minibatch(test_data, conf.target_batch_size)\n",
    "        test_logits = net(test_x)\n",
    "        for i in range(conf.heads):\n",
    "            corrects_i = (test_logits[:, i] > 0) == test_y.flatten()\n",
    "            acc_i = corrects_i.float().mean()\n",
    "            metrics[f\"ERM_acc_{i}\"].append(acc_i.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics \n",
    "import json \n",
    "with open(f\"{exp_dir}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.make_gifs and conf.heads == 2:\n",
    "    # Draw learning curves\n",
    "    def draw_full_curve(t=None, with_erm=False):\n",
    "        fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(8, 6))\n",
    "        N = 10\n",
    "        uniform = np.ones(N) / N\n",
    "        axs[0].set_xlim(-10, conf.train_iter)\n",
    "        axs[0].set_ylim(0.45, 1.05)\n",
    "        smooth = lambda x: np.convolve(x, uniform, mode=\"valid\")\n",
    "        for i in [0, 1]:\n",
    "            axs[0].plot(smooth(metrics[f\"acc_{i}\"]), alpha=0.8, linewidth=2)\n",
    "        if with_erm:\n",
    "            axs[0].plot(smooth(metrics[\"ERM_acc_0\"]), c=\"dimgray\", alpha=0.5, linewidth=2)\n",
    "        axs[1].plot(smooth(metrics[\"xent\"]), c=\"dimgray\")\n",
    "        axs[2].plot(smooth(metrics[\"repulsion_loss\"]), c=\"dimgray\")\n",
    "        axs[0].set_ylabel(\"Accuracy\")\n",
    "        axs[1].set_ylabel(\"Cross-Entropy\")\n",
    "        axs[2].set_ylabel(\"MI\")\n",
    "        for ax in axs:\n",
    "            ax.spines[\"bottom\"].set_linewidth(1.2)\n",
    "            ax.spines[\"left\"].set_linewidth(1.2)\n",
    "            ax.xaxis.set_tick_params(width=1.2)\n",
    "            ax.yaxis.set_tick_params(width=1.2)\n",
    "            ax.spines[\"top\"].set_color(\"none\")\n",
    "            ax.spines[\"right\"].set_color(\"none\")\n",
    "        if t:\n",
    "            for ax in axs:\n",
    "                ax.axvline(x=t, c=\"k\")\n",
    "\n",
    "    if conf.heads == 1:\n",
    "        draw_full_curve()\n",
    "        savefig(f\"{temp_fig_dir}/learning_curve_full\")\n",
    "\n",
    "    draw_full_curve(with_erm=True)\n",
    "    savefig(f\"{temp_fig_dir}/learning_curve_full_with_ERM\")\n",
    "\n",
    "    for t in tqdm(fig_save_times, desc=\"Drawing learning curves\"):\n",
    "        draw_full_curve(t=t)\n",
    "        savefig(f\"{temp_fig_dir}/learning_curve_full_{t}\")\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    N = 10\n",
    "    uniform = np.ones(N) / N\n",
    "    plt.ylim(0.45, 1.05)\n",
    "    smooth = lambda x: np.convolve(x, uniform, mode=\"valid\")\n",
    "    ax = plt.gca()\n",
    "    for i in [0, 1]:\n",
    "        ax.plot(smooth(metrics[f\"acc_{i}\"]), alpha=0.8, linewidth=2)\n",
    "    ax.plot(smooth(metrics[\"ERM_acc_0\"]), c=\"dimgray\", alpha=0.5, linewidth=2)\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.spines[\"bottom\"].set_linewidth(1.2)\n",
    "    ax.spines[\"left\"].set_linewidth(1.2)\n",
    "    ax.xaxis.set_tick_params(width=1.2)\n",
    "    ax.yaxis.set_tick_params(width=1.2)\n",
    "    ax.spines[\"top\"].set_color(\"none\")\n",
    "    ax.spines[\"right\"].set_color(\"none\")\n",
    "    savefig(f\"{temp_fig_dir}/learning_curve_with_ERM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.make_gifs:\n",
    "    # Stitch figures into gifs\n",
    "    import imageio.v2 as imageio\n",
    "    os.makedirs(\"gifs\", exist_ok=True)\n",
    "    print(\"making gifs\")\n",
    "\n",
    "    filenames = [f\"{temp_fig_dir}/{t}_h0.png\" for t in fig_save_times]\n",
    "    images = [imageio.imread(filename) for filename in filenames]\n",
    "    gif_head_0_filename = f\"{exp_dir}/h0.gif\"\n",
    "    imageio.mimsave(gif_head_0_filename, images)\n",
    "\n",
    "    if conf.heads == 2:\n",
    "        filenames = [f\"{temp_fig_dir}/{t}_h1.png\" for t in fig_save_times]\n",
    "        images = [imageio.imread(filename) for filename in filenames]\n",
    "        gif_head_1_filename = f\"{exp_dir}/h1.gif\"\n",
    "        imageio.mimsave(gif_head_1_filename, images)\n",
    "\n",
    "    filenames = [f\"{temp_fig_dir}/{t}_disagreement.png\" for t in fig_save_times]\n",
    "    images = [imageio.imread(filename) for filename in filenames]\n",
    "    gif_disagreement_filename = f\"{exp_dir}/disagreement.gif\"\n",
    "    imageio.mimsave(gif_disagreement_filename, images)\n",
    "\n",
    "    filenames = [\n",
    "        f\"{temp_fig_dir}/learning_curve_full_{t}.png\" for t in fig_save_times\n",
    "    ]\n",
    "    images = [imageio.imread(filename) for filename in filenames]\n",
    "    gif_curve_filename = f\"{exp_dir}/curve.gif\"\n",
    "    imageio.mimsave(gif_curve_filename, images)\n",
    "\n",
    "    print(\"GIF creation complete! Files are in:\")\n",
    "    for fn in [gif_head_0_filename, gif_head_1_filename, gif_curve_filename]:\n",
    "        print(fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-gen-KG5DY0Zz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
