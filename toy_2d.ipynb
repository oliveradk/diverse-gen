{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from typing import Optional, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n",
    "from losses.divdis import DivDisLoss\n",
    "from losses.ace import ACELoss\n",
    "from losses.dbat import DBatLoss\n",
    "from losses.loss_types import LossType\n",
    "\n",
    "from toy_data.grid import generate_data, plot_data, sample_minibatch, savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add focal loss to ERM and ACE loss \n",
    "# for ACE, do (1-P_1*P_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "@dataclass\n",
    "class Config():\n",
    "    seed: int = 45 \n",
    "    loss_type: LossType = LossType.PROB\n",
    "    train_size: int = 500 \n",
    "    test_size: int = 5000\n",
    "    batch_size: int = 32 \n",
    "    test_batch_size: int = 100 \n",
    "    train_iter: int = 3000 \n",
    "    heads: int = 2 \n",
    "    aux_weight: int = 2.0\n",
    "    mix_rate: Optional[float] = 0.02\n",
    "    l_01_mix_rate: Optional[float] = None # TODO: geneneralize\n",
    "    l_10_mix_rate: Optional[float] = None\n",
    "    gamma: Optional[float] = 1.0\n",
    "    lr: float = 1e-3\n",
    "    train_separate: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # set mix rate\n",
    "        if self.l_01_mix_rate is not None and self.l_10_mix_rate is None:\n",
    "            self.l_10_mix_rate = 0.0\n",
    "            if self.mix_rate is None:\n",
    "                self.mix_rate = self.l_01_mix_rate\n",
    "            assert self.mix_rate == self.l_01_mix_rate\n",
    "        elif self.l_01_mix_rate is None and self.l_10_mix_rate is not None:\n",
    "            self.l_01_mix_rate = 0.0\n",
    "            if self.mix_rate is None:\n",
    "                self.mix_rate = self.l_10_mix_rate\n",
    "            assert self.mix_rate == self.l_10_mix_rate\n",
    "        elif self.l_01_mix_rate is not None and self.l_10_mix_rate is not None:\n",
    "            if self.mix_rate is None:\n",
    "                self.mix_rate = self.l_01_mix_rate + self.l_10_mix_rate\n",
    "            assert self.mix_rate == self.l_01_mix_rate + self.l_10_mix_rate\n",
    "        else: # both are none \n",
    "            assert self.mix_rate is not None\n",
    "            self.l_01_mix_rate = self.mix_rate / 2\n",
    "            self.l_10_mix_rate = self.mix_rate / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config \n",
    "conf = Config()\n",
    "#get config overrides if runnign from command line\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), OmegaConf.from_cli(sys.argv[1:]))\n",
    "    conf = Config(**conf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(conf.seed)\n",
    "np.random.seed(conf.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = generate_data(5000, mix_rate=0.1)\n",
    "plot_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_name(conf: Config):\n",
    "    mix_rate_str = f\"mix_{conf.mix_rate}\" if conf.mix_rate is not None else f\"l01_{conf.l_01_mix_rate}_l10_{conf.l_10_mix_rate}\"\n",
    "    gamma_str = f\"_gamma_{conf.gamma}\" if conf.gamma is not None else \"\"\n",
    "    return f\"{conf.loss_type}_h{conf.heads}_w{conf.aux_weight}_{mix_rate_str}{gamma_str}_tr_s{conf.train_size}_te_s{conf.test_size}_b{conf.batch_size}_b_te{conf.test_batch_size}_lr{conf.lr}_separate{conf.train_separate}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.mix_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = get_exp_name(conf)\n",
    "os.makedirs(f\"figures/temp/{exp_name}\", exist_ok=True)\n",
    "\n",
    "fig_save_times = sorted(\n",
    "    [1, 2, 3, 4, 8, 16, 32, 64, 120, 128] + [50 * n for n in range(conf.train_iter // 50)]\n",
    ")\n",
    "\n",
    "training_data = generate_data(500, train=True)\n",
    "quad_proportions = [conf.l_01_mix_rate, (1-conf.mix_rate)/2, conf.l_10_mix_rate, (1-conf.mix_rate)/2]\n",
    "test_data = generate_data(5000, quadrant_proportions=quad_proportions)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 40), nn.ReLU(), nn.Linear(40, 40), nn.ReLU(), nn.Linear(40, conf.heads)\n",
    ")\n",
    "opt = torch.optim.Adam(net.parameters())\n",
    "if conf.loss_type == LossType.DIVDIS:\n",
    "    loss_fn = DivDisLoss(heads=conf.heads)\n",
    "else:\n",
    "    loss_fn = ACELoss(\n",
    "        heads=conf.heads, \n",
    "        mode=conf.loss_type.value, \n",
    "        gamma=conf.gamma,\n",
    "        l_01_rate=conf.l_01_mix_rate, \n",
    "        l_10_rate=conf.l_10_mix_rate, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_grid(time=\"\"):\n",
    "    N = 20\n",
    "    x = np.linspace(-1, 1, N)\n",
    "    y = np.linspace(-1, 1, N)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    inpt = torch.tensor(np.stack([xv.reshape(-1), yv.reshape(-1)], axis=-1)).float()\n",
    "    with torch.no_grad():\n",
    "        preds = net(inpt).reshape(N, N, conf.heads).sigmoid().cpu()\n",
    "\n",
    "    tr_x, tr_y = training_data\n",
    "    for i in range(conf.heads):\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.contourf(xv, yv, preds[:, :, i], cmap=\"RdBu\", alpha=0.75)\n",
    "        for g, c in [(0, \"#E7040F\"), (1, \"#00449E\")]:\n",
    "            tr_g = tr_x[tr_y.flatten() == g]\n",
    "            plt.scatter(tr_g[:, 0], tr_g[:, 1], zorder=10, s=10, c=c, edgecolors=\"k\")\n",
    "        plt.xlim(-1.0, 1.0)\n",
    "        ax = plt.gca()\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)\n",
    "        savefig(f\"temp/{exp_name}/{time}_h{i}\", transparent=True)\n",
    "\n",
    "def plot_head_disagreement(time=\"\"):\n",
    "    N = 20\n",
    "    x = np.linspace(-1, 1, N)\n",
    "    y = np.linspace(-1, 1, N)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    inpt = torch.tensor(np.stack([xv.reshape(-1), yv.reshape(-1)], axis=-1)).float()\n",
    "    with torch.no_grad():\n",
    "        preds = net(inpt).reshape(N, N, conf.heads).sigmoid().cpu()\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.contourf(xv, yv, torch.abs(preds[:, :, 0] - preds[:, :, 1]), cmap=\"RdBu\", alpha=0.75)\n",
    "    plt.xlim(-1.0, 1.0)\n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    savefig(f\"temp/{exp_name}/{time}_disagreement\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "metrics = defaultdict(list)\n",
    "for t in range(conf.train_iter):\n",
    "    x, y = sample_minibatch(training_data, conf.batch_size)\n",
    "    logits = net(x)\n",
    "    logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "    losses = [F.binary_cross_entropy_with_logits(logit, y) for logit in logits_chunked]\n",
    "    xent = sum(losses)\n",
    "\n",
    "    target_x, target_y = sample_minibatch(test_data, conf.test_batch_size)\n",
    "    target_logits = net(target_x)\n",
    "    repulsion_loss = loss_fn(target_logits)\n",
    "\n",
    "    full_loss = xent + conf.aux_weight * repulsion_loss\n",
    "    opt.zero_grad()\n",
    "    full_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    for i in range(conf.heads):\n",
    "        corrects_i = (target_logits[:, i] > 0) == target_y.flatten()\n",
    "        acc_i = corrects_i.float().mean()\n",
    "        metrics[f\"acc_{i}\"].append(acc_i.item())\n",
    "    metrics[f\"xent\"].append(xent.item())\n",
    "    metrics[f\"repulsion_loss\"].append(repulsion_loss.item())\n",
    "\n",
    "    if t in fig_save_times:\n",
    "        print(f\"Generating plots for {t}/{conf.train_iter}\")\n",
    "        plot_pred_grid(t)\n",
    "        plot_head_disagreement(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Train single ERM model (for comparison in learning curve)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 40), nn.ReLU(), nn.Linear(40, 40), nn.ReLU(), nn.Linear(40, conf.heads)\n",
    ")\n",
    "opt = torch.optim.Adam(net.parameters())\n",
    "\n",
    "for t in range(conf.train_iter):\n",
    "    x, y = sample_minibatch(training_data, conf.batch_size)\n",
    "    logits = net(x)\n",
    "    logits_chunked = torch.chunk(logits, conf.heads, dim=-1)\n",
    "    losses = [F.binary_cross_entropy_with_logits(logit, y) for logit in logits_chunked]\n",
    "    full_loss = sum(losses)\n",
    "    opt.zero_grad()\n",
    "    full_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    target_x, target_y = sample_minibatch(test_data, conf.test_batch_size)\n",
    "    target_logits = net(target_x)\n",
    "    for i in range(conf.heads):\n",
    "        corrects_i = (target_logits[:, i] > 0) == target_y.flatten()\n",
    "        acc_i = corrects_i.float().mean()\n",
    "        metrics[f\"ERM_acc_{i}\"].append(acc_i.item())\n",
    "        print(acc_i.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Draw learning curves\n",
    "def draw_full_curve(t=None, with_erm=False):\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(8, 6))\n",
    "    N = 10\n",
    "    uniform = np.ones(N) / N\n",
    "    axs[0].set_xlim(-10, 1000)\n",
    "    axs[0].set_ylim(0.45, 1.05)\n",
    "    smooth = lambda x: np.convolve(x, uniform, mode=\"valid\")\n",
    "    for i in [0, 1]:\n",
    "        axs[0].plot(smooth(metrics[f\"acc_{i}\"]), alpha=0.8, linewidth=2)\n",
    "    if with_erm:\n",
    "        axs[0].plot(smooth(metrics[\"ERM_acc_0\"]), c=\"dimgray\", alpha=0.5, linewidth=2)\n",
    "    axs[1].plot(smooth(metrics[\"xent\"]), c=\"dimgray\")\n",
    "    axs[2].plot(smooth(metrics[\"repulsion_loss\"]), c=\"dimgray\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[1].set_ylabel(\"Cross-Entropy\")\n",
    "    axs[2].set_ylabel(\"MI\")\n",
    "    for ax in axs:\n",
    "        ax.spines[\"bottom\"].set_linewidth(1.2)\n",
    "        ax.spines[\"left\"].set_linewidth(1.2)\n",
    "        ax.xaxis.set_tick_params(width=1.2)\n",
    "        ax.yaxis.set_tick_params(width=1.2)\n",
    "        ax.spines[\"top\"].set_color(\"none\")\n",
    "        ax.spines[\"right\"].set_color(\"none\")\n",
    "    if t:\n",
    "        for ax in axs:\n",
    "            ax.axvline(x=t, c=\"k\")\n",
    "\n",
    "\n",
    "draw_full_curve()\n",
    "savefig(f\"temp/{exp_name}/learning_curve_full\")\n",
    "\n",
    "draw_full_curve(with_erm=True)\n",
    "savefig(f\"temp/{exp_name}/learning_curve_full_with_ERM\")\n",
    "\n",
    "for t in fig_save_times:\n",
    "    draw_full_curve(t=t)\n",
    "    savefig(f\"temp/{exp_name}/learning_curve_full_{t}\")\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "N = 10\n",
    "uniform = np.ones(N) / N\n",
    "plt.ylim(0.45, 1.05)\n",
    "smooth = lambda x: np.convolve(x, uniform, mode=\"valid\")\n",
    "ax = plt.gca()\n",
    "for i in [0, 1]:\n",
    "    ax.plot(smooth(metrics[f\"acc_{i}\"]), alpha=0.8, linewidth=2)\n",
    "ax.plot(smooth(metrics[\"ERM_acc_0\"]), c=\"dimgray\", alpha=0.5, linewidth=2)\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.spines[\"bottom\"].set_linewidth(1.2)\n",
    "ax.spines[\"left\"].set_linewidth(1.2)\n",
    "ax.xaxis.set_tick_params(width=1.2)\n",
    "ax.yaxis.set_tick_params(width=1.2)\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "savefig(f\"temp/{exp_name}/learning_curve_with_ERM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Stitch figures into gifs\n",
    "import imageio\n",
    "os.makedirs(\"gifs\", exist_ok=True)\n",
    "\n",
    "filenames = [f\"figures/temp/{exp_name}/{t}_h0.png\" for t in fig_save_times]\n",
    "images = [imageio.imread(filename) for filename in filenames]\n",
    "gif_head_0_filename = f\"gifs/{exp_name}_h0.gif\"\n",
    "imageio.mimsave(gif_head_0_filename, images)\n",
    "\n",
    "filenames = [f\"figures/temp/{exp_name}/{t}_h1.png\" for t in fig_save_times]\n",
    "images = [imageio.imread(filename) for filename in filenames]\n",
    "gif_head_1_filename = f\"gifs/{exp_name}_h1.gif\"\n",
    "imageio.mimsave(gif_head_1_filename, images)\n",
    "\n",
    "filenames = [f\"figures/temp/{exp_name}/{t}_disagreement.png\" for t in fig_save_times]\n",
    "images = [imageio.imread(filename) for filename in filenames]\n",
    "gif_disagreement_filename = f\"gifs/{exp_name}_disagreement.gif\"\n",
    "imageio.mimsave(gif_disagreement_filename, images)\n",
    "\n",
    "filenames = [\n",
    "    f\"figures/temp/{exp_name}/learning_curve_full_{t}.png\" for t in fig_save_times\n",
    "]\n",
    "images = [imageio.imread(filename) for filename in filenames]\n",
    "gif_curve_filename = f\"gifs/{exp_name}_curve.gif\"\n",
    "imageio.mimsave(gif_curve_filename, images)\n",
    "\n",
    "print(\"GIF creation complete! Files are in:\")\n",
    "for fn in [gif_head_0_filename, gif_head_1_filename, gif_curve_filename]:\n",
    "    print(fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-gen-KG5DY0Zz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
